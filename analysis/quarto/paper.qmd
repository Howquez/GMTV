---
title: Growth and inequality in public good provision --- an extended replication
author:
  - name: Hauke Roggenkamp
    email: hauke.roggenkamp@unisg.ch
    affiliations:
        - id: HSU
          name: Helmut Schmidt University
          department: Department of Economics
          address:  Holstenhofweg 85
          city: Hamburg
          state: Germany
          postal-code: 22043
        - id: HSG
          name: University of St. Gallen
          department: Institute for Behavioral Science and Technology
          address: Torstrasse 25
          city: St. Gallen
          state: Switzerland
          postal-code: 9000
    # attributes:
    #     corresponding: true
    # note: This is the first author footnote.
abstract: |
  You can find the most recent version of this paper [here](https://github.com/Howquez/coopUncertainty/blob/main/analysis/quarto/paper.pdf). The abstract follows at later point.
keywords: 
  - Replication study
  - Non-convenience sample
  - Open science
  - Dynamic public good game
  - Online experiment
  - Generalizability
date: last-modified
bibliography:  ../biblio.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal of Behavioral and Experimental Economics
      formatting: preprint
      model: 3p
      cite-style: authoryear
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 10, fig.height = 4)
```

```{r locale}
#| include: false
#| warning: false
#| eval: false

Sys.setlocale("LC_TIME","English United States")
```

```{r packages}
# install this package once, it'll then install and load all remaining packages
# install.packages("pacman")

pacman::p_load(magrittr, data.table, stringr, lubridate, glue, DescTools, haven,
               stargazer, ggplot2, patchwork, MetBrewer, Rmisc, knitr)
```

```{r constants}
SHOW_UP_FEE <- 5
EXCHANGE_RATE <- 1/20
```

```{r design}
# ggplot layout
layout <- theme(panel.background = element_rect(fill = "transparent", color = NA),
                plot.background = element_rect(fill = "transparent", color = NA),
                panel.grid = element_blank(),
                panel.grid.major.y = element_blank(),
                legend.key = element_rect(fill = "transparent"),
                axis.line = element_line(size = 0.25),
                axis.ticks = element_line(size = 0.25),
                axis.title = element_text(size = 8),
                legend.text = element_text(size = 8),
                plot.caption = element_text(size = 6,
                                            colour = "#555555"),
                legend.title = element_blank()
)

# color
colors <- met.brewer(name="Tam",n=7,type="discrete")
colors <- c("#F3B05C", "#1E4A75", "#65B5C0", "#AD5E21")
# cPrimary = "#ffd353"
# cSecondary = "#9f2d55"
# cInfo = "#34168"
cPrimary = "#F3B05C"
cSecondary = "#1E4A75"
cInfo = "#36BFA2"
```

<!-- OUR DATA -->


```{r readAllData}
# identify files of all sessions
files <- list.files(path = "../../data/replication/",
                       recursive = TRUE,
                       full.names = TRUE)

# load all of them into a list
csvs <- list()
for(i in files){
  name <- str_extract(string = i, pattern = "2021.*")
  temp <- read.csv(i, stringsAsFactors = FALSE) %>% data.table()
  csvs[[name]] <- temp
}

# bind list into single data table
full <- rbindlist(l = csvs, use.names = TRUE) %>% data.table()

# ignore observations who have not made it to the first contribution
DT <- full[participant._index_in_pages > 2 & 
             participant.time_started != "" & 
             participant.label != "" &
             !(is.na(dPGG.1.player.contribution))]

# tidy up
rm(list = c("files", "csvs", "i", "name", "temp"))
```

```{r metaData}
# aggregate each session's meta data
meta_long <- full[participant.time_started != "" & 
                    participant.label != "",
                  .(session.code,
                    participant.label,
                    participant.time_started,
                    participant._index_in_pages,
                    dPGG.1.player.contribution)]

# reformat time stamp in new variables
meta_long[, `:=`(date = participant.time_started %>% ymd_hms() %>% date(),
                 hour = participant.time_started %>% ymd_hms() %>% hour())]

# aggregate and count 
meta <- meta_long[,
                  .(date = date %>% unique(),
                    # weekday = date %>% unique() %>% lubridate::wday(label = TRUE),
                    time = hour %>% unique() %>% paste("00", sep = ":"),
                    showups = participant.time_started %>% length(),
                    dropouts = sum(participant._index_in_pages == 2),
                    residuals = sum(is.na(dPGG.1.player.contribution) & participant._index_in_pages == 77)),
                  by = session.code]

# get number of participants and observations (groups of 4)
meta[,
     `:=`(participants = showups - dropouts - residuals,
          observations = (showups - dropouts - residuals)/4)]

# save data
fileName <- "replication2021"
save(meta, file = paste0("../../data/processed/rda/", fileName, "_meta", ".rda"))
write.csv(meta, file = paste0("../../data/processed/csv/", fileName, "_meta", ".csv"))

rm(list = c("meta_long"))
```

```{r identifyStudents}
# we had to invite students as the general population sample was exhausted, unfortunately. let's identify them:
invitedStudents <- read.table(file = "../../data/sample/invited_students.txt", col.names = c("participant.label")) %>%
  data.table()
# note that these students participated in the ultimate session
```

```{r focusOnFirstRound}
# create data table 
replicationFirstRound <- DT[,
                 .(participant.code,
                   treatment = "replication",
                   session.code,
                   groupID = paste(session.code, dPGG.1.group.id_in_subsession, sep = "_"),
                   othersContribution = dPGG.1.group.total_contribution - dPGG.1.player.contribution,
                   ownContribution = dPGG.1.player.contribution,
                   trust = Outro.1.player.PQ11,
                   comprehension = dPGG.10.player.comprehension)]

# save data
save(replicationFirstRound, file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(replicationFirstRound, file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))
```

```{r subsetData}

# identify most relevant variables
mRegex <- "participant\\.code$|session\\.code$|dPGG\\.1\\.group\\.id_in_subsession|^dPGG.1.player.id_in_group$|\\.1\\.player.belief|endowment|contribution|stock|gain$|bot_active|10.player.comprehension|10.player.donation"
mainVariables <- str_subset(string = names(DT), pattern = mRegex)

# prune all other variables
subset <- DT[, ..mainVariables]

# tidy up
rm(list = c("mRegex", "mainVariables"))
```

```{r addNewVariables}
# refactor groupID such that it eventually contains treatment-info
subset[, groupID := paste(session.code, 
                          dPGG.1.group.id_in_subsession,
                          sep = "_")]

# add share as contribution/endowment
for(round in 1:10){
  contribution <- glue("dPGG.{round}.player.contribution")
  endowment <- glue("dPGG.{round}.player.endowment")
  subset[, glue("dPGG.{round}.player.share") := subset[[contribution]]/subset[[endowment]] ]
}

# add treatment variable
subset[,
       treatment := "replication"]
```

```{r calculateAggregates}
# we need to aggregate some outcomes on a groupID level (per sessions per treatment)
# we'll do so using a loop
# this will yield a list of data tables that will be merged to a list eventually.
cluster <- c("treatment", "session.code", "groupID")
outcomes <- c("contribution", "endowment", "stock", "gain", "bot_active")

DTs <- list()
for(outcome in outcomes){
  if(outcome == "bot_active"){
    var = names(subset) %>% str_subset(pattern = glue("group\\.{outcome}$"))
  } else {
    var = names(subset) %>% str_subset(pattern = glue("player\\.{outcome}$"))
  }


  # calculate either averages or the sum per round per group
  aggregates = subset[, lapply(.SD, sum, na.rm=TRUE), 
                      by = cluster, 
                      .SDcols=var]
  
  # transform from wide to long
  meltedAggregates <- melt(aggregates, id.vars = cluster, measure.vars = var)
  DTname <- glue("{str_to_title(outcome)}")
  DTs[[DTname]] <- meltedAggregates
  rm(list = c("DTname", "meltedAggregates", "aggregates", "var", "outcome"))
}
```

```{r renameVariables}
# each data table in that list has the same column names.
# the outcomes are all named "value", for instance.
# now, we"ll infer the round number (contained in the variable name)
for(i in 1:length(outcomes)){
  DTs[[i]] <- DTs[[i]][,
                       .(treatment,
                         session.code,
                         groupID,
                         round = str_replace_all(string = variable,
                                                 pattern = "\\D", 
                                                 replacement="") %>% as.integer(),
                         value # to be renamed afterwards
                       )
  ]
  # rename "value" to outcome variable
  setnames(DTs[[i]], old = "value", new = outcomes[i])
}

# tidy up
rm("i")
```


```{r calculateGini}
# repeat everything for the gini
var = names(subset) %>% str_subset(pattern = "player\\.stock$")
gini = subset[,
              lapply(.SD, Gini, na.rm=TRUE), 
                by = cluster, 
                .SDcols=var
              ]
Gini <- melt(gini, id.vars = cluster, measure.vars = var)

DTs[["Gini"]] <- Gini[,
                    .(treatment,
                      session.code,
                      groupID,
                      round = str_replace_all(string = variable,
                                              pattern = "\\D", 
                                              replacement="") %>% as.integer(),
                      gini = value
                       )]
rm(list = c("var", "gini", "Gini"))

# note that GMTV used start of period earnnings, i.e. endowments. We use end of period earnings, i.e. stock.
# this adjustment has been considered in our processing of GMTVs data.
```


```{r mergerEverything4FinalData}
# now merge every table in the list to one final data table called "replication"
replication <- Reduce(function(...) merge(..., by=c(cluster, "round"), all = TRUE), DTs)

# tidy up
rm(list = c("cluster", "contribution", "endowment", "round", "DTs", "outcomes"))
```

```{r calculateShare}
# define the share (one of the main outcome variables) as 
# the sum of contributions devided by the sum of endowments
replication[, share := contribution/endowment]
```

```{r defineRichGroups}
# use the median to differentiate between poor and rich groups (as GMTV did)
median <- replication[round == 10,
                      median(stock)]

# find the groups that end up rich or poor
richGroups <- replication[round == 10 & stock > median,
                          unique(groupID)] 

poorGroups <- replication[round == 10 & stock < median,
                          unique(groupID)]

# mark these groups for each period
replication[groupID %in% richGroups,
            rich := TRUE]

replication[groupID %in% poorGroups,
            rich := FALSE]

# tidy up
rm(list = c("median", "poorGroups", "richGroups"))

```

```{r misc}
# flag observations where at least one participant did not understand the game
noComp <- subset[dPGG.10.player.comprehension == 0,
                 groupID] %>% unique()
replication[,
     noComprehension := 0]
replication[groupID %in% noComp,
     noComprehension := 1]


# drop observations (i.e. groups in rounds) with dropouts (bot_active == 1) and
# where round > 10
replication <- replication[bot_active == 0 & round <= 10]

# tidy up
rm("noComp")
```

```{r saveData}
save(replication, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(replication, file = paste0("../../data/processed/csv/", fileName, ".csv"))
```

```{r createCovariates}

# so far, I neglected many variables in what happened before. The following few lines
# deal with some of these variables in a similar procedure


# add variables
DT[, treatment := "replication"]
DT[, groupID := paste(session.code, dPGG.1.group.id_in_subsession, 
                    sep = "_")]

# subset
cRegex <- "participant.code|session.code|treatment|groupID|Outro.1.player|10.player.donation|10.player.stock|switching_row|inconsistent"
covariates <- str_subset(string = names(DT),
                            pattern = cRegex)
CT <- DT[, ..covariates]

# rename
names(CT) <- names(CT) %>% 
  str_replace_all(pattern =".*player\\.",
                  replacement = "") %>%
  str_to_lower()
names(CT)[names(CT) == "groupid"] <- "groupID"

# refactor
CT[, donation := donation/20] # exchange rate tokens to real world currency 1/20
CT[donation %>% is.na, donation := 0]
CT[, gender := ifelse(test = gender == "female",
                      yes  = 1,
                      no   = 0)]
CT[, inconsistent := as.logical(inconsistent)]

# reassign payoff (with stock of the last period)
CT[, payoff := stock]
CT[, stock := NULL]

# write data
replicationCovariates <- CT
save(replicationCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(replicationCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("CT", "cRegex", "covariates"))
```

```{r readTime}
# as before, we need to read a list of files (measuring the time spent per page)

# identify files
files <- list.files(path = "../../data/pageTimes/",
                       recursive = TRUE,
                       full.names = TRUE)

# loop
csvs <- list()
for(i in files){
  name <- str_extract(string = i,
                      pattern = "2021.*")
  
  temp <- read.csv(i, 
                   stringsAsFactors = FALSE) %>%
    data.table()
  
  csvs[[name]] <- temp
}

# bind files list to data.table
timeSpent <- rbindlist(l = csvs,
                       use.names = TRUE) %>%
  data.table()

# set oder
setorder(timeSpent, session_code, participant_code, epoch_time)

# tidy up
rm(list = c("files", "i", "name", "temp"))
```


```{r calcDuration}
# shift rows to calculate this duration (per page) as the distance between
# the current and the next time stamp
timeSpent[,
          lag := shift(epoch_time, fill = NA, type = "lag"),
          by = c("session_code", "participant_code")]

timeSpent[,
          duration := epoch_time - lag,
          by = c("session_code", "participant_code")]

# calculate overall time spent as the difference between the min and max time stamp
timeSpent[,
          completion := epoch_time %>% max() - epoch_time %>% min(),
          by = c("session_code", "participant_code")]

# create new data table with selected columns
duration <- timeSpent[session_code %in% meta$session.code, # participant_code %in% DT$participant.code,
                      .(
                        session_code,
                        participant_code,
                        app_name,
                        page_name,
                        page_index,
                        page_submission = epoch_time,
                        time_spent = duration,
                        completion_time = completion
                      )]

# save data
save(duration, file = paste0("../../data/processed/rda/", fileName, "_timeSpent", ".rda"))
write.csv(duration, file = paste0("../../data/processed/csv/", fileName, "_timeSpent", ".csv"))

# tidy up
rm(list = c("timeSpent", "csvs", "DT"))
```




<!-- ORIGINAL (GMTV) DATA -->

```{r readGMTV}
# having done all that, we need to prepare the original data and bind it to ours
# I'll thus, start by reading the orginal data

DT <- read_dta(file="../../data/gaechteretal/GMTV-data.dta") %>% data.table()
CT <- read_dta(file="../../data/gaechteretal/GMTV-questionnaire-data.dta") %>% data.table()
```

```{r subsetGMTV}
# because we only replicate one of their treatments, I subset accordingly
# noPunish10 <- DT[exp_num == 5 | exp_num == 8 | exp_num == 9]
noPunish10 <- DT[longgame == 0 & punish == 0 & exp_num <= 10]
```

```{r newVariablsGMTV}
# calculate the share of endowments contributed
noPunish10[,
           share := sum/gdp,
           by = .(subj_id, per)]

# calculate gini based on endowments (as GMTV did)
noPunish10[,
           gini2 := Gini(c(tokens, other1, other2, other3)),
           by = .(subj_id, per)]

# calculate gini based on stock (i.e. end of period earnings)
noPunish10[, stock0 := (tokens - putin + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock1 := (other1 - pu1 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock2 := (other2 - pu2 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock3 := (other3 - pu3 + 1.5*sum/4) %>% ceiling()]
noPunish10[, 
           gini3 := Gini(c(stock0, stock1, stock2, stock3)),
           by = .(subj_id, per)]
```

```{r createGMTVtable}
# select columns we are interested in and rename a few to match our data

GMTV <- noPunish10[order(gr_id, per),
           .(treatment = "noPunish10", 
             session.code = exp_num,
             groupID = gr_id, 
             round = per,
             contribution = sum,
             endowment = gdp,
             share,
             stock = (gdp + ceiling(sum/4*1.5)*4-sum),
             gain = (ceiling(sum/4*1.5)*4-sum),
             gini = gini3,
             bot_active = 0,
             noComprehension = NA)] %>% unique()

# tidy up
rm(list = c("noPunish10"))
```

```{r richPoorGMTV}
 # create rich indicator (just as highgdp in original data)
median <- GMTV[round == 10,
                   median(stock)]

richGroups <- GMTV[round == 10 & stock > median,
                       unique(groupID)] 

poorGroups <- GMTV[round == 10 & stock < median,
                       unique(groupID)]

GMTV[groupID %in% richGroups,
         rich := TRUE]

GMTV[groupID %in% poorGroups,
         rich := FALSE]

# tidy up
rm(list = c("median", "richGroups", "poorGroups", "fileName"))
```

```{r saveGMTV}
fileName <- "GMTV2017"
# save data
save(GMTV, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(GMTV, file = paste0("../../data/processed/rda/", fileName, ".csv"))
```


```{r calcSwitchingPoint}
# GMTV gathered some risk data that I'll process in a separate DT and merge it
# with other covariates later on

GMTVRisk <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
               .(participant.code = subj_id,
                 e10, e20, e30, e40, e50, e60, e70,
                 inconsistent = ifelse(test = e60 < e70 | e50 < e60 | e40 < e50 | e30 < e40 | e20 < e30 | e10 < e20,
                                       yes  = TRUE,
                                       no   = FALSE)
                 )]

GMTVRisk[,
         switching_row := ifelse(test = inconsistent == TRUE,
                                yes  = NA,
                                no   = e10 + e20 + e30 + e40 + e50 + e60 + e70 + 1 + 2) # because GMTV used a 7-point-likert scale
         ]
```

```{r subsetCovariates}
# subset covariate data and rename+refactor some variables
temp <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
                         .(treatment = "noPunish10",
                           session.code = exp_num,
                           groupID = gr_id,
                           participant.code = subj_id,
                           gender,
                           age = Sys.Date() %>% lubridate::year() - age,
                           pq01 = q1,
                           pq02 = q2,
                           pq03 = q3,
                           pq04 = q4,
                           pq05 = q5,
                           pq06 = q6,
                           pq07 = q7,
                           pq08 = q8,
                           pq09 = q9,
                           pq10 = q10,
                           pq11 = q11,
                           pq12 = q12,
                           pq13 = q13,
                           pq14 = q14,
                           donation = don
                           )]

temp[donation %>% is.na, donation := 0]

```

```{r mergeAndWrite}

GMTVCovariates <- data.table::merge.data.table(x = temp,
                                               y = GMTVRisk[, .(participant.code, inconsistent, switching_row)],
                                               by = c("participant.code"))

# write data
save(GMTVCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(GMTVCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("temp", "GMTVRisk"))
```

```{r firstRound}
noPunish <- DT[exp_num == 1 | exp_num == 3 | # noPunish15
                 exp_num == 5 | exp_num == 8 | exp_num == 9] # noPunish10

temp <- noPunish[per == 1]
temp <- temp[,
             treatment := "noPunish10"]
temp <- temp[exp_num == 1 | exp_num == 3,
             treatment := "noPunish15"]

GMTVFirstRound <- temp[,
                           .(participant.code = subj_id,
                             treatment,
                             session.code = exp_num,
                             groupID = gr_id,
                             othersContribution = sumputin - putin,
                             ownContribution = putin,
                             trust = NA,
                             comprehension = NA)]

# save data
save(GMTVFirstRound, 
     file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(GMTVFirstRound, 
          file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))

rm(list = c("temp", "fileName"))
```

<!-- MERGE & TUNE DATA -->

```{r rbindOursAndGMTV, eval = TRUE}
main <- rbindlist(list(replication, GMTV), 
                  use.names = TRUE)

R1   <- rbindlist(list(replicationFirstRound, GMTVFirstRound), 
                  use.names = TRUE)

covs <- rbindlist(list(replicationCovariates, GMTVCovariates), 
                  use.names = TRUE,
                  fill = TRUE)

rm(list = c("CT", "DT", "GMTV", "GMTVCovariates", "GMTVFirstRound", 
            "noPunish", "replication", "replicationCovariates",
            "replicationFirstRound", "subset"))
```

```{r treatmentAsFactor}
main[, treatment := as.factor(treatment)]
R1[, treatment := as.factor(treatment)]
covs[, treatment := as.factor(treatment)]
```

```{r studentSample}

# Two of these sessions were special: The first (`r meta[1, session.code]`) as well as the last one (`r meta[, session.code %>% tail(n=1)]`). The first session suffered technical problems such that the risk elicitation task was omitted. The last session (almost exclusively) relied on a student sample as our non-student sample was exhausted after the first three sessions. As a consequence, the last session was conducted with 59 students while all others were conducted without any students. I'll therefore create a boolean `student` variable. Becaue all of the original experiments were conducted with students, `student` also equals 1 in the "noPunish10" treatments

main[, student := FALSE]
main[session.code == "d6jrsxnr" | treatment == "noPunish10",
     student := TRUE]

covs[, student := FALSE]
covs[session.code == "d6jrsxnr" | treatment == "noPunish10",
     student := TRUE]

R1[, student := FALSE]
R1[session.code == "d6jrsxnr" | treatment == "noPunish10",
   student := TRUE]
```

```{r earnings}

# This chunk requires raw data
files <- list.files(path = "../../data/replication/",
                       recursive = TRUE,
                       full.names = TRUE)
csvs <- list()

for(i in files){
  name <- str_extract(string = i,
                      pattern = "2021.*")
  
  temp <- read.csv(i, 
                   stringsAsFactors = FALSE) %>%
    data.table()
  
  csvs[[name]] <- temp
}

full <- rbindlist(l = csvs,
                  use.names = TRUE) %>%
  data.table()


# subset data of participants who have completed the study
tmp <- full[participant._index_in_pages >= 76 & 
              participant.time_started != "" & 
              !(is.na(dPGG.1.player.contribution)) &
              participant.label != ""]

# calculate total payoff
tmp[, finalPayoff := (dPGG.10.player.stock - dPGG.10.player.donation + HLPL.1.player.payoff) * EXCHANGE_RATE + SHOW_UP_FEE]

earningsMean <- tmp[, mean(finalPayoff, na.rm = TRUE)]
earningsSD   <- tmp[, sd(finalPayoff, na.rm = TRUE)]
```



<!-- PAPER STARTS HERE -->

<!-- 
# Key Takeaways

- We **[replicate](#gmtv-replication)** a public goods experiment with dynamic inter-dependencies and find similar results as @GMTV2017. 
    + Absolute contributions increase over time.
    + Just as in _static_ public goods experiments, the share of endowments contributed decreases over time.
    + The richest groups earn fifteen times more than the poorest groups.
    + While there clearly is growth, groups do not realize the maximal potential efficiency and earn just over `r main[treatment == "replication" & round == 10, mean(stock)/(80*1.5^10)*100] %>% round(digits=1)`% of what is possible.
- Varying the subject pool and including a voluntary climate action (VCA), we have a setup similar to @GKLS2020, which allows us to assess **[generalizability](#generalizability)**.^[Note that one can easily model the VCA as a dictator game, which relates to a literature summarized by @cartwright2022.]
- Studying a general population sample online, synchronously and over the course of multiple periods, we find that classic **[lab procedures](online-methodology)** can be applied outside of the lab too.
    + Even though we conducted the experiment online and remotely, dropouts (or "attrition") are no concern.
    + Relying on an inexperienced non-convenience sample that cannot interact with experimenters, `r trunc((R1[comprehension > 1] %>% NROW() / R1[!(is.na(comprehension))] %>% NROW())*100)`% of all participants stated that they understood the game.
    + Participants make relatively fast decisions which makes longer games feasible in the future.^[Taken together with inefficient growth and the lack of dropouts, additional rounds are fairly cheap.]

-->

# Introduction {#sec-intro}


Today's actions are tomorrow's result. There are many settings in which current decisions affect future outcomes and with it, future decision spaces. Opting for environmental friendly policies today not only reduces carbon dioxide omissions immediately but also helps us to reach the Paris climate targets tomorrow. Deferring these policies, may not necessarily prevent us from reaching these targets, but it makes it more difficult in the future. Hence, today's actions (or the omission thereof) not only affect intermediate outcomes but also the number of paths one can choose to reach certain goals.

Standard public good games---although often intended to inform climate policies^[See @GKLS2020 [p. 1] for numerous references.]---miss these temporal interdependencies simply because participants have the same set of actions in each period. Accordingly, participants' actions in a given period do not affect their number of actions in subsequent periods. A game implemented by Gächter, Mengel, Tsakas & Vostroknutov [-@GMTV2017] (hereafter, GMTV) as well as Stefan Große (2011, unpublished) incorporated interdependencies into a _dynamic_ public good game. Here, participants' actions in a given period affect their number of actions in subsequent periods. Because there is surprisingly little experimental research on these interdependencies^[One exemption are @Moser2019, who built on GMTV's design do investigate leadership.], I replicated one of their treatments.

@GKLS2020 find that standard public good games do not generalize to real-world climate action. Because the dynamic setting has a more realistic property, one wonders whether it is better suited to inform public policy. To answer this question, I ran the experiment with different samples^[GMTV ran their experiments with students in Nottingham, England.] online, that is, with participants who make their decisions in an environment whcih is more natural than the lab. In addition, I observed the participants' behavior in voluntary climate actions (VCA). This yielded a setting similar to @GKLS2020's which allows me to analyze how behavior in the abstract game translates into real-world action across samples.

@AGM2018 conducted public good games in the lab and on MTurk to draw lessons from online experimentation. This study extends this literature [see also @GoodmanPaolacci2017] by focusing on a completely inexperienced sample that has not been exposed to interactive experiments before. This required me to design a robust (and thus, more complex) software to minimize attrition. In exchange, the online setting allowed me to also collect paradata to assess the fluency and feasibility of the experiment. This article reports on both topics: the robust design as well as the feasibility.

Taken together, this study makes three contributions. First, it replicates parts of GMTV's original experiment and highlights the importance of pure replications. Second, it shows that logistically complex online experiments are feasible for samples other than students or clickworkers. Third, it shows that findings from abstract games do not generalize well and even worse with the representative sample. After reporting the methods, this paper is organized along these findings.


# Methodology {#sec-methods}

In the terminology of @Hamermesh2007, I ran both a _pure_ as well as a _scientific replication_^[@Parsons2022 use the term of a _conceptual replication_ which means the same.] of one treatment of GMTV's dynamic public good game. The pure replication re-analyzes the original data. [Appendix A](#A:-Pure-Replication) documents [errors I identified]() in the original paper. The scientific replication, where I utilize a different sample drawn from a different population in a different situation, is described in the following sections.

## Experimental Design {#sec-design}

As in the NOPUNISH 10 Period treatment of GMTV, I ran sessions with groups of four ($i \in I=\{1,2,3,4\}$), an initial endowment of $N_i^1 = 20$ tokens^[A token was worth `r EXCHANGE_RATE` Euros.], $T=10$ periods, a private account with a return of $1$ and a group account with a return of $1.5$ ($\Rightarrow$ MPCR$\equiv \frac{1.5}{4}$). With $i$'s contribution in period $t$ being $c_i^t$, the model looks as follows:

$$
N_i^{t+1}=N_i^t - c_i^t + \frac{1.5}{4}\sum_{j=1}^4 c_j^t
$$

What makes the game _dynamic_? Instead of receiving fresh endowments every period, participants received one endowment only at the beginning of the first period. A participant's endowment in the second period is the wealth he or she accumulated in the first period. A participant's endowment in the third period is the wealth he or she accumulated in the first two periods. And so on. Hence, a decision in one period has consequences on future endowments and, ultimately, growth paths. For this reason, the game is described as a _dynamic_ public good game

## Voluntary Climate Action (VCA)

Like GMTV we employ a real giving task after the abstract game. In contrast to GMTV and like @GKLS2020, we employed a VCA, where they could donate any amount their earnings to offset carbon dioxide (that is, retirine emission permits from the EU ETS).^[Importantly, @GKLS2020 made the VCA decision with a fresh endowment _before_ they played the abstract game. I deviate from their procedure to match GMTV's procedure.]
To ensure that each participant had the same basic level of information about the impact of their decision, I provided some basic information about the mechanism. The information also highlighted that the mitigation came into operation on a European level.
Finally, I informed the participants that the documentation of individual and aggregate contributions were to be posted immediately after the experiments online. To avoid privacy or social image concerns, participants learned their unique and random IDs, which they needed to identify their individual contributions.
The document certified that their contributions have been used to offset [1.82 tons](https://www.compensators.org/compensatelist/?searchterm=stefan+traub) of carbon dioxide emissions.


## Sample {#sec-sample}

I recruited the participants from the so called _[HamburgPanel](https://www.wiso.uni-hamburg.de/forschung/forschungslabor/umfragelabor/aktuelle-umfragen/hamburgpanel.html)_ using HROOT [@hroot]. The panel is provided by the University of Hamburg's Research Laboratory, which used a randomized last digits approach to build the panel while drawing from the population of citizens of Hamburg, Germany. Because the sample was exhausted at one point, I also recruited students from the University of Hamburg.

At the time I conducted the experiment, the more representative sample was not familiar with interactive experiments. In fact, I ran the first interactive group experiment with this sample. The students, in contrast, were used to this kind of experiments. Recruiting them, I excluded those who have participated in a public goods game before. As a consequence, nonnaiveté is unlikely to affect the validity of the experiment [@GoodmanPaolacci2017 p. 204].

Throughout this paper, I will compare results of my experiment with the results of GMTV's NOPUNISH 10 Period treatments. I am thus, referring to three different samples utilized at two points in time: the University of Nottingham's students (in late 2012), Hamburg's citizens and the University Hamburg's students (both in July 2021). @tbl-sample-properties shows how they compare with respect to a few properties.

```{r sampleDifferences1, results = "asis", warning = FALSE}
#| label: tbl-sample-properties
#| tbl-cap: Sample Properties

covs[, sample := "Nottingham Students"]
covs[treatment == "replication" & student == TRUE, sample := "Hamburg Students"]
covs[treatment == "replication" & student == FALSE, sample := "Hamburg Citizens"]
covs[, sample := as.factor(sample)]
# covs[, sample := ordered(sample, levels = c("Nottingham Students", "Hamburg Students", "Hamburg Citizens"))]


m1 <- lm(formula = gender ~ sample, data = covs)
m2 <- lm(formula = age ~ sample, data = covs)
m3 <- lm(formula = switching_row ~ sample, data = covs)
m4 <- lm(formula = donation ~ sample, data = covs)
m5 <- lm(formula = pq11 ~ sample, data = covs)
m6 <- lm(formula = pq12 ~ sample, data = covs)
m7 <- lm(formula = pq13 ~ sample, data = covs)
m8 <- lm(formula = pq14 ~ sample, data = covs)

stargazer(m1, m2, m5, m6, m7, m8,


          column.labels = c("female", "age",
                            "trust", "meritocracy", "government", "equality") %>% str_to_title(),
          model.numbers = FALSE,
          dep.var.labels.include = FALSE,
          header=FALSE,
          intercept.bottom = FALSE,
          covariate.labels = c("Hamburg Citizens", "Hamburg Students", "Nottingham Students"),

          type = "latex", digits = 2, omit.stat = c("adj.rsq", "f"), df = FALSE
          )
```

```{r}
# covs[, sample := ordered(sample, levels = c("Nottingham Students", "Hamburg Students", "Hamburg Citizens"))]
```

Describe sample properties here.



## Software {#sec-software}

The experiment was logistically complex for several reasons. First, the sample was inexperienced. Second, the experiment was interactive and synchronous. Third, the underlying game was dynamic and interdependent. This makes dropouts not only more likely but also more expensive, which is why attrition was a major concern implementing the experiment.

I chose oTree [@oTree] to implement the experiment because it is open-source, well documented and very flexible. Its [Bootstrap](https://getbootstrap.com/) (a powerful frontend toolkit) integration allowed me to make the graphical user interface interactive, appealing and easy to navigate. The [Highcharts library](https://www.highcharts.com/) made it easy to visualize results and to communicate dynamics. Insofar, oTree served a good tool to enhance the participants' user experience and thus, to make dropouts less likely. Furthermore, the oTree code snippets made it possible to handle dropouts.

Which features were required to handle dropouts? First, participants had to be matched to form a group _after_ comprehension questions were answered successfully. Importantly, participants were grouped by the order they answered these questions to reduce waiting times. While waiting for other players to form the group, the participants saw a wait-page informing them that they are waiting for other participants to arrive and that they do not have to wait for longer than 10 minutes. The screen also informed them that they would receive a _patience bonus_ of one Euro after the expiration of that time (or what was left of it). Second, participants only had 10 minutes to make the first contribution and 4 minutes for the remaining contributions. After this time expired, participants were replaced by bots that made random contributions. In this case, the remaining group members were informed about the replacement. Both features were implemented to limit wait times and boredom for other participants. @sec-feasibility shows that the first feature became effective in some cases, wheres the second feature did not.


## Procedure {#sec-procedure}

Participants entered the experiment at appointed times remotely from home. They first saw a welcome screen. After agreeing to the privacy policy, they could proceed to the instructions individually. Having read these instructions, each participant has also seen a demo-screen explaining the user interface. Before proceeding, they had to answer six comprehension questions correctly. Subsequently, they saw a waiting screen until they could be matched with three other participants, who have answered the comprehension questions correctly. Once matched, they were exposed to the decision screen over ten periods. At the end of the last period, participants saw results of all periods. Subsequently, they made their VCA decision, before I elicited risk preferences [@HoltLaury2002] and finished with GMTV's questionnaire.

While I tried to stick to GMTV's protocol as close as possible, I deviated in a few aspects. First, the instructions were German and also covered topics inherent to the online setting (dropouts and bots, for instance). Second, I used another software (oTree instead of zTree). Third, GMTV gave participants the opportunity to donate to _Doctors without Borders_ whereas we offered carbon dioxide offsets. Fourth, the graphical user interface looked different.

```{r totalTimeSpent}
tmp <- duration[page_index == 1 | page_index == 76, 
                .(page_index, participant_code, page_submission)]
tmp[order(participant_code, page_index), 
    total := page_submission - data.table::shift(x = page_submission,
                                                 n = 1,
                                                 type = "lag"),
    by = participant_code]

total_completion_time <- tmp[!is.na(total), mean(total/ 60) %>% round()]
```

The experiment lasted around `r total_completion_time` minutes on average. The earnings  averaged `r earningsMean %>% round(digits = 2)` Euros (sd = `r earningsSD  %>% round(digits = 2)`).^[This values include earnings from the incentivized risk elicitation task that is not part of the analysis.] 


## Pre-registration {#sec-registration}

Parts of the analyses were pre-registered in the American Economic Association's RCT Registry [@preregistration]. In addition, I pre-registered the exact analyses I planned to run when I designed designed the experiment on [GitHub](https://github.com/Howquez/coopUncertainty/blob/July21Replication/analysis/reports/rmd).^[https://github.com/Howquez/coopUncertainty/tree/July21Replication/analysis/reports/rmd to run the code, you need to executed the .Rmd files in this repository in the order that is indicated by its file names.]. GitHub is a website and cloud-based service where developers---and researchers alike---can store and manage their code. The service is based on Git^[Git is a specific open-source version control system developed in 2005.] and designed for version control. Importantly, changes are timestamped and can be tracked. Moreover, one can create branches, that is, duplicates of code -- either to work collaborative or to archive a certain state. Accordingly, I archived my analyses scripts and created a [branch](https://github.com/Howquez/coopUncertainty/tree/July21Replication) a few days _before_ we collected data.

The analysis that makes the most sense now that I have collected the data is a little different though: Because the more representative subject pool was exhausted earlier than expected, I recruited students unwantedly and changed parts of the scripts.^[The source code of this document contains the analysis and can be found [here](https://github.com/Howquez/coopUncertainty/blob/main/analysis/quarto/paper.qmd): https://github.com/Howquez/coopUncertainty/blob/main/analysis/quarto/paper.qmd] However, because the originally planned analysis code is archived and reproducible, these changes are transparent and easy to assess.

# Results {#sec-results}

## Pre-registered GMTV Replication {#sec-replication}

### Contribution Behavior {#sec-contributions}

```{r prepFirstSummary}
replication <- R1[treatment == "replication", ownContribution]
GMTV    <- R1[treatment == "noPunish10", ownContribution]


rows <- sapply(X = list(replication, GMTV), FUN = NROW) %>% max()
temp <- data.frame(replication = c(replication, rep(NA, rows - NROW(replication))),
                   GMTV = c(GMTV, rep(NA, rows - NROW(GMTV))))

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)
```

First, I ask whether the samples differ with respect to their initial contributions to the public good. Is our replication sample more pro-social than the original sample? @fig-first-round reveals that it is not. The distributions of both samples look fairly similar. Both samples contributed 10 tokens, that is, 50% of their endowments on average (median and mean).^[The two-sided rank sum test (comparing differences between samples) yields a p-Value of `r rs1` for the mean contribution in first round of the game.] Moreover, both samples' initial contributions resemble initial contributions participants usually make in the standard game with partner matching.^[See Figure 3B in @fehrgaechter2000 (p.989), for instance.] However, in the dynamic game presented here, we are particularly interested in the subsequent periods because differences add up exponentially. Do the two groups remain similar over the course of time?


```{r firstRoundViz}
#| fig-cap: Individual contributions to the dynamic public good in the first period
#| label: fig-first-round

ggplot(data = R1[treatment != "noPunish15"],
       mapping = aes(x = ownContribution, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 20),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 0.1),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Replication Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = R1[treatment == "replication", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 2) +
  geom_vline(xintercept = R1[treatment == "noPunish10", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 1) +
    labs(title = "", 
       y = "", x = "Initially Contributed Tokens",
       caption = "White lines indicate means (dashed line = replication sample).") +
  layout +
  theme(legend.position = "top")
```


```{r firstRoundSummary}
#| results: asis
#| warning: false
#| fig.cap: "test"
#| eval: false

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

stargazer(temp,
          summary.stat = c("mean", "median","sd", "max", "min", "n"),
          type = type, 
          flip = TRUE, 
          header=FALSE)
```


In particular, do the two samples' contributions follow the same path over the 10 periods they played? The answer is _no_. @fig-share-of-contributions illustrates that the samples make similar contributions at the beginning and the end of the game but behave differently in between. More precisely, the left panel--depicting the average contributions in absolute terms--shows that the original sample contributed substantially more than the replication sample _in all but the first and last period_. For this reason, the original sample's behavior differs from the replication sample's behavior in two aspects: it contributes more and exhibits a considerable drop in the last period (whereas the replication sample's contributions flatten). 

Note that increasing contributions over time imply increasing endowments over time. Hence, absolute contributions do not us much about the willingness to cooperate. For this reason, the right panel in @fig-share-of-contributions shows the average _share of endowments contributed_ over time. Both samples exhibit a similar pattern: their share of endowments contributed declined and did not stabilize. However, both samples also differ with respect to one aspect: the replication sample's share of contributions declines faster.


```{r plotContributions}

SUM <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "contribution"]

SUM[,
    sum := round(contribution)]

SUM[,
    contribution := contribution/4]

upperLimit <- SUM$contribution %>% max() %>% round() + 5

p1 <- ggplot(data = SUM, 
             aes(x = round, y = contribution, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="top") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Average Amount of Tokens contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Replication Sample")) + 
  guides(fill = "none") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

rm(list = c("SUM"))
```


```{r plotShareOfContributions}
#| fig-cap: "The average amount of tokens contributed over time in treatments."
#| label: fig-share-of-contributions

SHARE <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "share"]

# SHARE <- main[,
#             .(share = sum(contribution)/sum(endowment)),
#             by = c("round", "treatment")]

upperLimit <- 0.75

p2 <- ggplot(data = SHARE, 
             aes(x = round, y = share, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Share of Current Endowment contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Replication Sample")) +
  guides(fill = "none")

p1 + p2 + plot_layout(guides = "collect") & 
  theme(legend.position = "top")

# rm(list = c("p1", "p2"))
```


Again, both samples' behavior resembles the contributions participants usually make in the standard game with partner matching: contributions equal approximately half of endowments in the very first period and decrease to around 10% of endowments by the last period.^[The right panel is thus, comparable to the visualizations _and results_ in the standard game. See, for instance, Figure 1B in @fehrgaechter2000 (p.986).] In the dynamic game presented here, however, different paths lead to different levels of wealth -- even if they share the same start- and end-points. I am thus, more interested in the contributions' implications for wealth generation and growth. 

### Wealth Creation {#sec-wealth}

```{r summaryWealth}
#| results: asis
#| warning: false
#| fig.cap: "test"

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, stock]
GMTV    <- main[treatment == "noPunish10" & round == 10, stock]

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)

```

```{r wealthVariables}
originalWealth    <- main[treatment == "noPunish10" & round == 10, mean(stock)]
replicationWealth <- main[treatment == "replication" & round == 10, mean(stock)]
```

How do the different contribution-paths translate into wealth?^[To measure wealth and growth, I define a variable called _stock_ which sums the endowments of all participants in a given group at the end of the round (that is, after the contributions have been made, multiplied and redistributed).] Given that the original sample contributed more in most of the periods, one would expect the respective groups to be considerably more wealthy. @fig-stock-distribution indicates just that. The grey lines show that an average group in the original sample accumulated about `r round(originalWealth)` tokens. In contrast, an average group in the replication sample accumulated about `r round(replicationWealth)` tokens. This difference is insignificant at conventional levels^[The two-sided rank sum test (comparing differences between samples) yields a p-Value of `r rs1` for the mean stock in last round of the game.] though.

```{r stockDistributionViz}
#| fig-cap: Groups' income at the end of the game
#| label: fig-stock-distribution

ggplot(data = main,
       mapping = aes(x = stock, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Replication Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = replicationWealth,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = originalWealth,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Stock",
       caption = "Grey lines indicate means (dashed line = replication sample).") +
  layout +
  theme(legend.position = "top")
```

```{r overallGinis}
rG <- main[round == 10 & treatment == "replication",
           .(stock = sum(stock)),
           by = groupID][, Gini(stock)] %>% round(digits = 2)

oG <- main[round == 10 & treatment == "noPunish10",
           .(stock = sum(stock)),
           by = groupID][, Gini(stock)] %>% round(digits = 2)

rSD <- main[round == 10 & treatment == "replication",
            .(stock = sum(stock)),
            by = groupID][, sd(stock)] %>% round(digits = 2)

oSD <- main[round == 10 & treatment == "noPunish10",
            .(stock = sum(stock)),
            by = groupID][, sd(stock)] %>% round(digits = 2)
```

Although there clearly is growth, groups do not realize the maximal potential efficiency: under full cooperation, a group can accumulate at least `r (80*1.5^10) %>% trunc()` tokens or EUR `r (80*1.5^10 /20) %>% trunc()`. This is depicted in the left panel of @fig-growth-heterogeneity, where one can see the average wealth over time by sample. The panel illustrates for both samples that growth was continuous and surprisingly linear, given the exponential character of the game's design. To sum up, the contribution behavior differed between samples. In contrast, neither the eventual wealth nor the corresponding growth paths differed. Differences in contribution behavior did, thus, not translate to significantly different wealth outcomes.

Why? Perhaps because the heterogeneity within samples and across groups has been too large to _detect_ a significant difference. The right panel of @fig-growth-heterogeneity depicts heterogeneity: In the replication sample, the richest group earned `r a <- main[treatment == "replication" & round == 10, max(stock)]; a` tokens (which is about `r round(a/80, digits = 2)*100`% of the initial endowment) whereas the poorest group ends up with `r b <- main[treatment == "replication" & round == 10, min(stock)]; b` tokens (`r round(b/80, digits = 2)*100`%). More broadly, the replication sample is characterized by inequality between groups ($SD_{Replication} =$ `r rSD`). The same holds true for the original sample ($SD_{Original} =$ `r oSD`). Hence, the heterogeneity across groups does not differ between samples, which is remarkable because the replication sample was drawn from a more heterogeneous (non-convenience sample). Does it differ within groups?

<!-- doch lieber Gini? statt SD reporten? -->

```{r plotStock .column-page}

# data
STOCK <- main[,
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "treatment"),
              .SDcols = "stock"]

STOCKr <- main[rich == TRUE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

STOCKp <- main[rich == FALSE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]


# annotation
maxPath <- data.table(x = 1:10)
maxPath[, y := 80*1.5^x]
maxPath[, treatment := "replication"]
maxPath[, groupID := 42]

# plot
p1 <- ggplot(data = STOCK, 
             aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 4700), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Replication Sample")) +
  geom_point(mapping = aes(x = 10, y = 4613), col = "#b5b5b5") +
  annotate("text", x = 9, y = 4630, size = 3,
           label = "max", col = "#b5b5b5") +
  geom_line(data = maxPath, mapping = aes(x = x, y =y), col = "#b5b5b5", lty = 2) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

```{r growthHeterogeneityViz}
#| fig-cap: Average wealth over time across samples.
#| label: fig-growth-heterogeneity

# plot
p2 <- ggplot(data = main,
       mapping = aes(x = round, y = stock, color = treatment, by = groupID)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 2000),
                       expand = c(0, NA)) +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Replication Sample")) +
  labs(y = "", x = "Period") +
  layout +
  guides(col = "none")


# assembling
p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "top")
```

### Inequality {#sec-inequality}

```{r summaryGini}
#| results: asis
#| warning: false
#| fig.cap: "test"

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, gini]
GMTV    <- main[treatment == "noPunish10" & round == 10, gini]

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)

```

```{r giniVariables}
originalGini    <- main[treatment == "noPunish10" & round == 10, mean(gini)] %>% round(digits = 2)
replicationGini <- main[treatment == "replication" & round == 10, mean(gini)] %>% round(digits = 2)
```

Given the different samples and the possibility of endogenous growth--which essentially is the main feature of the game--I ask whether and how the inequality grows _within_ groups. @fig-gini-distribution illustrates that inequality did grow: at the end of the game, the original and the replication groups exhibit an average Gini coefficient of `r originalGini` and `r replicationGini`, respectively.^[The two-sided rank sum test (comparing differences between samples) yields a p-Value of `r rs1` for the mean Gini coefficient in last round of the game.] Because every participant started with the same initial endowment (in _Period 0_, so to speak), every group started equally--with a Gini coefficient equaling zero.

@fig-ginit-time-series shows that this initial state of equality ended with the first period already: both samples exhibit a stark incline in inequality before the second period started. From then on, the respective Gini coefficients grew slowly but continuously -- for both samples.

```{r giniDistributionViz}
#| fig-cap: Groups' Gini coefficients (within groups) at the end of the game
#| label: fig-gini-distribution

ggplot(data = main,
       mapping = aes(x = gini, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Replication Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = replicationGini,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = originalGini,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Gini Coefficient",
       caption = "Grey lines indicate means (dashed line = replication sample).") +
  layout +
  theme(legend.position = "top")
```

```{r giniOverTime}
#| fig-cap: Average Gini coefficient (within groups) over time across samples
#| label: fig-ginit-time-series

tmp <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "gini"]

GINI <- rbind(tmp, list(0, "replication", 0), list(0, "noPunish10", 0))

ggplot(data = GINI, 
             aes(x = round, y = gini, fill = treatment, color = treatment, lty = treatment)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 0:10) +
  scale_y_continuous(limits = c(0, 0.25), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Replication Sample")) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

**Result 1.** _The `NOPUNISH 10` treatment of GMTV can be replicated because the replication data resemble the original data with respect to initial and final contributions, wealth and growth as well as inequality._

This is remarkable given the different sample and language, the different software and user interface as well as the online setting during the COVID19 pandemic. The result suggests that, by and large, the sum of these factors did not affect people’s preferences towards cooperation.

## Online Feasibility {#sec-feasibility}

```{r reviewInformation}
#| eval: false

# to see how many people clicked on the popups to review instructions: 
full[dPGG.1.player.review_instructions !=0, .N]
full[dPGG.2.player.review_instructions !=0, .N]
#...

# or for contact information
full[dPGG.1.player.review_contact !=0, .N]
full[dPGG.2.player.review_contact !=0, .N]
# ...

```


How did the participants, who have never participated in an online group experiment before, cope with the situation? Moreover, did participants understand the unfamiliar setting they found themselves in? While the answer to the former question requires more thought, the answer to the latter simply is _yes_: `r 67` out of `r 44 + 67 + 5` answered with _"yes"_ when I asked them. Another `r 44` answered with _"rather yes"_ while nobody indicated that he or she did not understand the situation at all. There is some behavioral data supporting this finding: The user interface offered a popup to review instructions or contact information. I tracked both and find that none of the participants ever opened these popups even though they were clearly visible in the decision screen's header and introduced in the instructions. To  further analyze how participants coped with the situation, I consider three additional metrics: selection into the experiment, attrition as well as the time spent on each page.

I first comment on the selection into the experiment: It was difficult to recruit the sample. The panel counted 1.209 non-students of which we were able to recruit `r main[student == FALSE, .N]` participants who finished the experiment---even though we varied the weekdays and timing of the sessions (which were conducted during a nation-wide lockdown with home office regime). For this reason, we also recruited students in the last session which explains the relatively large number of showups in @tbl-meta. Although I intended to refrain from the recruitment of students initially, this particular sub sample enabled me to investigate the generalizability of my results as I will discuss in  @sec-generalizability. Alternatively, I also could have contacted a market research institute to recruit additional participants within a week. I refrained from doing so to contrast students and the general population sample, however.

```{r showMeta}
#| label: tbl-meta
#| tbl-cap: The Experimental Sessions' Meta Data

meta %>% kable(col.names = names(meta) %>% 
                 str_replace_all(pattern = "\\.",
                                 replacement = " ") %>% 
                 str_to_title()) #, caption = 'A table of the first 10 rows of the mtcars data.')
```


```{r plotTimeData}
N <- duration[, participant_code %>% unique() %>% length()]
plotDT <- duration[app_name == "dPGG" & page_name == "dPGG_Decision",
                   .(time_spent = time_spent %>% sum()),
                   by = c("session_code", "participant_code", "page_index", "page_name")]

plotDT[, round := seq(from = 1, to = 10), by = c("participant_code")]

upperLimit <- plotDT[, time_spent %>% mean(), by = c("round")] %>% max()

tmp <- summarySE(data = plotDT,
                 measurevar = "time_spent",
                 groupvars=c("round"),
                 na.rm = FALSE,
                 conf.interval = 0.95,
                 .drop = TRUE) %>% 
  data.table()

```

Turning to the time spent on each page, I focus on the decision times in the dynamic public goods game as @Anderhub2001 did. How many seconds did the participants need to make a decision in each period of the game? Not too many. @fig-time-spent illustrates an intuitive pattern: The first decision took about `r tmp[round == 1, round(time_spent)]` seconds. The second decision--where participants first learned about the other group members' previous decisions--took longer (about `r tmp[round == 2, round(time_spent)]` seconds). Subsequently, decision times first declined and stabilized at `r tmp[round > 3, round(mean(time_spent))]` seconds. Importantly, decision times were so short that crosstalk, that is, communication through private channels--a common concern^[See, for instance, the discussion section in @AGM2018 [p. 119].] in online experiments--was unlikely, especially because it would require the identification of other group members.^[There were only `r plotDT[page_index == 12 & time_spent > 60, .N]` participants (from all four sessions) who needed more than 60 seconds to make the second decision.]

```{r plotTime}
#| fig-cap: Average Time Spent for each Contribution per Period
#| label: fig-time-spent

ggplot(data = tmp,
       mapping = aes(x = round, y = time_spent)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE, color = cInfo, lty=2) +
  geom_errorbar(aes(ymin=time_spent-ci, ymax=time_spent+ci), width=.25, alpha = 0.5, color = cInfo) +
  geom_point(color = cInfo) +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit + 10), expand = c(0, 0)) +
  labs(y = "Time Passed in Seconds", caption = "Bars indicate 95% confidence intervals.") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))
```

Considering attrition, I find that it did not affect the interactive experiment at all. To elaborate, I differentiate between dropouts and residuals:  Participants who could not be matched to other group members are called residuals. Participants who intentionally left the experiment are called dropouts. Residuals did not participate in the experiment _by design_. Dropouts did not participate in the experiment _by choice_. Out of `r meta[, sum(showups)]` people who showed up, I count `r meta[, sum(residuals)]` residuals and `r meta[, sum(dropouts)]` dropouts. All of the residuals waited to be matched to a group unsuccessfully before they got paid one Euro for their patience. In contrast, all of the dropouts left while reading the instructions and before being matched to other group members. Moreover, they got no payment at all. Hence, attrition was no concern considering the dynamic public goods game or the expenses.

**Result 2.** _Given the decision times and the fluent procedure, attrition was as negligible as it is in physical laboratories---where (a) not every invited person shows up and (b) a number of participants divisible by the group size is required as well._


## Generalizability {#sec-generalizability}

<!--
> "There are two possible articles you can write: (a) the article you planned to write when you designed your study or (b) the article that makes the most sense now that you have seen the results. They are rarely the same, and the correct answer is (b)." [@bemwriting, p. 171]
-->

```{r}
R1covs <- covs[, .(participant.code,
                   donation = donation * 20 ,
                   donationShare = (donation * 20) / payoff * 100,
                   payoff)]
temp <- R1[R1covs, on = .(participant.code = participant.code)][treatment == "replication"]
temp[, contributionShare := ownContribution/20 * 100]
```

@GKLS2020 asked how much can we learn about voluntary climate action from the behavior in public goods games. Using a similar strategy, I answer the question for _dynamic_ public goods games: _Not much_. Overall, there seems to be no association between choices in the voluntary climate action and the first period in the dynamic public goods game. @fig-scatter-generalizability shows a scatter plot of realized choices, with the percentage of endowment spent by each participant in the first period of the game on the x-axis and that spent in the VCA on the y-axis. In addition, the figure contains a fitted line of a linear model whose slope is indistinguishable from zero. No matter how much the participants contributed in the first round, they spent, on average, about `r temp[, round(mean(donationShare))]`% of their income on the VCA.


```{r}
#| fig-cap: Scatter plot of average contributions in the dPGG and real giving task.
#| label: fig-scatter-generalizability
#| fig.height: 6

p1 <- ggplot(data = temp[, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cInfo, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cInfo) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "Percentage of endowment contributed in period 1") +
  layout

p2 <- ggplot(data = temp[student == FALSE, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cInfo, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cInfo) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "General pop.: Percentage of endowment contributed in period 1") +
  layout

p3 <- ggplot(data = temp[student == TRUE, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cInfo, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cInfo) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "Students: Percentage of endowment contributed in period 1") +
  layout

p1 / (p2 + p3)
```

Does this result hold true if one zooms in and inspects the two samples separately? Yes. Even though the general population sample is a little more consistent than the student sample, both are contributing more in the abstract game than in the VCA. @fig-kernel-generalizability shows distributions of contributions across both choices for both samples. The left panels illustrate the behavior of the general population sample. The right panels illustrate the behavior of the general population sample. The top panels show the behavior in the VCA. The bottom panels show the behavior in first period of the game. A visual inspection shows that (a) both samples behaved similarly in the first period of the game but (b) different (p =
`r summary(lm(formula = donationShare ~ student, data = temp))$coefficients[2,4] %>% round(digits = 2)`) in the VCA. Furthermore, (c) the behavior in the first period of the experiment predicts the general population sample's behavior in the VCA worse than student sample's behavior (p =
`r summary(lm(formula = contributionShare ~ donationShare + student + donationShare*student, data = temp))$coefficients[4,4] %>% round(digits = 2)`). Finally, (4) contributions in the abstract public good game are higher than contributions to the real public good of climate change mitigation.



```{r vizGeneralizability}
#| fig-cap: Kernel distributions of contributions across tasks and subject pools.
#| label: fig-kernel-generalizability
#| fig.height: 6

# viz wrapper
plotShares <- function(stud = FALSE,
                       var  = "donationShare",
                       col  = cPrimary,
                       xlab = "",
                       ylab = ""){
  
  dt <- temp[student == stud, .(x = get(var))]
  
  model <- lm(x ~ 1, dt)
  ci1 <- confint(model, level=0.95)[1]
  ci2 <- confint(model, level=0.95)[2]
  
  ggplot(data = dt,
         mapping = aes(x = x)) +
    annotate("rect", 
             xmin = ci1, xmax = ci2, 
             ymin  =0, ymax = Inf, 
             alpha = 0.33) +
    geom_density(alpha = 0.75,
                 fill = col) +
    scale_x_continuous(limits = c(0, 100),
                       expand = c(0, NA)) +
    scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
    geom_vline(xintercept = dt[,mean(x, na.rm = TRUE)],
               lty = 2) +
    labs(x = xlab,
         y = ylab) +
  layout
}

p1 <- plotShares(stud = FALSE, col = cPrimary,   var = "donationShare", ylab = "Density in VCA")
p2 <- plotShares(stud = FALSE, col = cPrimary,   var = "contributionShare", 
                 xlab = "Share of general pop. contributions", ylab = "Density in Experiment (round 1)")
p3 <- plotShares(stud = TRUE,  col = cSecondary, var = "donationShare")
p4 <- plotShares(stud = TRUE,  col = cSecondary, var = "contributionShare", xlab = "Share of students' contributions")

patchwork <- (p1 / p2) | (p3 / p4)
patchwork + plot_annotation(caption = "Dashed vertical lines indicate the respective means.\nShaded areas indicate 95% confidence intervals.")



rm(list = c("R1covs", "p1", "p2", "p3", "p4", "patchwork"))
```

**Result 3.** _Overall, contribution behavior is uncorrelated with the willingness to contribute to real public goods. This holds true for more representative samples and---to a lower degree---for student samples._



# Conclusion {#sec-conclusion}

The goal of the experiment was to replicate specific experiments of GMTV in an online setting using a general population sample. The results suggest that it is important to replicate experiments---both purely and scientifically [@Hamermesh2007 p. 716]---before drawing conclusions about generalizability.

The three most important findings are as follows: First, the contribution behavior in my experiment is statistically similar to the behavior reported in the original study. Consequently, the outcomes growth and inequality are similar as well. Second, the online experiment proceeded fluently such that dropouts were no concern. Third, contribution behavior in my abstract setting is, if anything, only weakly linked to behavior in the real world. 

The significance of the first result is that similar procedures led to replicable findings under different circumstances across two different samples. The second result is of methodological importance: It highlights that even logistically complex experiments can be conducted online with---not only with clickworkers but also with a true general population sample. The third result questions whether recruiting from more representative samples is worth the efforts because it decreases transferability of results to the real world---at least in this specific case.

{{< pagebreak >}}


# A: Pure Replication {.appendix}

This section comments on two errors as well as a misconception I found in the original data.^[The data can be found in the supplementary materials they provide in their [online appendix](https://www.sciencedirect.com/science/article/pii/S0047272717300361#s0115).] Before I proceed to explain this in more detail I would like to say that the results of the original paper still hold after the error is fixed and that the authors responded kindly and quickly, showing an interest in solving the issue. In fact, some explanations in this section stem from input provided by the authors.

### Error 1: The Gini coefficient 

The Gini coefficient is wrongly computed in some periods for some group members. The authors found that this happened whenever two group members had exactly the same endowment because the program failed to rank these group members for further calculations.

```{r readGMTVagain}
GMTV <- read_dta(file="../../data/gaechteretal/GMTV-data.dta") %>% data.table()
noPunish10 <- GMTV[exp_num == 5 | exp_num == 8 | exp_num == 9]
# noPunish10 <- GMTV[longgame == 0 & punish == 0 & exp_num <= 10]
```

@tbl-gini-error illustrates this problem. It shows group 101 in period 5 and documents that the Gini coefficient differs among group members. According to the authors, the Gini coefficient should equal `GINI=``r GINI <- GMTV[exp_num == 1 & gr_id == 101 & per == 5, tokens %>% Gini() %>% round(digits = 3)]; GINI` for all subjects in the group. Instead, participant `112` and `113` who have an equal endowment deviate from that value. Importantly, the `DescTools::Gini()` function in the statistical software `R` does not yield this error, which is why I use that function for my calculations using both my as well as the original data.

```{r}
#| label: tbl-gini-error
#| tbl-cap: Subset of Data illustrating the Gini Coefficient's Error

GMTV[exp_num == 1 & gr_id == 101 & per == 5,
     .(exp_num,
       gr_id,
       per,
       subj_id,
       tokens,
       other1,
       other2,
       other3,
       gini = round(gini, digits = 3),
       GINI = GINI
       )] %>% 
  knitr::kable()
```

### Error 2: The share of endowments contributed

The original data provides a wrong measure of the share of endowments contributed (`mean`) because it relies on a lagged endowment (`gdp`). More precisely, the authors used the following STATA code for their calculations:

```
*tsset subj_id per
*gen mean=sum/l.gdp
```

@tbl-mean-error reports participant 111 in group 101 in experiment 1 over three periods. Both the `gdp` (that is, the sum of the group’s endowments at the beginning of the period) as well as the `sum` (that is, the sum of the group’s contributions) are group-level variables. 

```{r}
#| label: tbl-mean-error
#| tbl-cap: Subset of Data illustrating the Means's Error

GMTV[, MEAN := sum/gdp]

GMTV[exp_num == 1 & gr_id == 101 & (per == 4 | per == 5 | per == 6) & subj_id == 111,
     .(exp_num,
       gr_id,
       per,
       subj_id,
       gdp,
       sum,
       mean = round(mean, digits = 3),
       MEAN = round(MEAN, digits = 3)
       )] %>% 
  knitr::kable()
```

Calculating the share as `MEAN=sum/gdp` solves the problem and yields $\frac{18}{126}=0.143$ in period 5. I thus, used this proposed definition for all my calculations using both my as well as the original data.

### The misconception: Timing

The authors wrote a note stating that the Gini coefficient as well as the wealth in the paper always refer to the situation at the start of a period and that they clarify this because the paper (last paragraph at the bottom of page 5), says that wealth is defined as the endowment at the beginning of the following period. Furthermore, they write that this error came about as they switched between these two definitions during the course of revising the paper.

I argue that it makes more sense to calculate the variables as they state in the paper. More precisely, I think that the wealth at the _beginning_ of a period is less interesting than the wealth at the _end_ of a period for two reasons: First, there is no need for such a variable because it already exists (the endowment). Second, this definition yields a value that is determined by the design of the game but misses an important outcome at the end of the game. To illustrate this, note that the wealth would be defined as four times the initial endowment in period 1. Also note that the very last value would equal the wealth at the beginning of the last period and says nothing about the outcome of that period. Because the contributions often drop in the last period, this outcome is of particular interest (yet, not represented in the data).
Moreover, this definition of wealth yields more informative values to calculate the Gini coefficient for the same reasons: We know that the Gini coefficient is zero _before_ the participants made any decision by design. We do no know the inequality at the very end of the game---and the current definition does not tell us.

For these reasons, I define wealth and inequality measures as the outcomes of a period for all of my calculations using both my as well as the original data.^[Accordingly, the definition of `GINI` I provide in @tbl-gini-error is not the definition I used to calculate the current period's Gini coefficient but the previous period's Gini coefficient.]

{{< pagebreak >}}

<!--
# B: TITLE (exploratory) {.appendix}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec suscipit eleifend felis et faucibus. Curabitur nisl risus, tincidunt ut hendrerit sed, sollicitudin eget tortor. Curabitur tortor elit, finibus a commodo eget, auctor vel dui. Ut convallis, felis in commodo bibendum, ligula neque hendrerit eros, eu eleifend libero ligula pretium dui. Nullam blandit a tellus vitae suscipit. Donec ac justo vehicula elit tincidunt mollis nec ac lectus. Nunc dignissim sem eu felis consequat, id euismod ante imperdiet. Mauris quis ex ante.

Nulla elementum laoreet libero nec consequat. Donec blandit, tellus id feugiat viverra, lectus tellus sollicitudin ipsum, ut venenatis nunc mauris ut arcu. Donec mi eros, ullamcorper aliquet varius non, cursus vel lorem. Integer feugiat dui sapien, ac porta lacus porta sit amet. Aliquam interdum dictum tempor. Quisque bibendum feugiat scelerisque. Sed eu purus eu est ullamcorper bibendum.

Donec efficitur magna malesuada tortor feugiat tristique. Sed porttitor, quam at vulputate placerat, elit magna posuere est, a eleifend lacus magna ac nunc. Quisque sit amet arcu mattis, malesuada mauris et, aliquam libero. Fusce viverra sed dolor a ullamcorper. Aliquam in ligula neque. In maximus nisl id ante vehicula, et euismod ante feugiat. Suspendisse et dui lectus. Ut euismod ultrices mi eu lacinia.

```{r firstRoundVizAppendix}

d1 <- ggplot(data = R1[treatment == "replication"],
       mapping = aes(x = ownContribution, fill = student, lty = student)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 20),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 0.1),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("General Population Sample", "Student Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = R1[treatment == "replication", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 2) +
  geom_vline(xintercept = R1[treatment == "noPunish10", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 1) +
    labs(title = "", 
       y = "", x = "Initially Contributed Tokens") +
  layout +
  theme(legend.position = "top")
```


```{r stockDistributionVizAppendix}

populationWealth <- main[treatment == "replication" & student == FALSE & round == 10,
                         mean(stock)]
studentWealth    <- main[treatment == "replication" & student == TRUE & round == 10,
                         mean(stock)]

d2 <- ggplot(data = main[treatment == "replication"],
       mapping = aes(x = stock, fill = student, lty = student)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("General Population Sample", "Student Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = studentWealth,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = populationWealth,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Stock") +
  layout +
  theme(legend.position = "top")
```


```{r giniDistributionVizAppendix}

populationGini    <- main[treatment == "replication" & student == FALSE & round == 10,
                        mean(gini)] %>% round(digits = 2)
studentGini <- main[treatment == "replication" & student == TRUE & round == 10,
                        mean(gini)] %>% round(digits = 2)

d3 <- ggplot(data = main[treatment == "replication"],
       mapping = aes(x = gini, fill = student, lty = student)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("General Population Sample", "Student Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = studentGini,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = populationGini,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Gini Coefficient") +
  layout +
  theme(legend.position = "top")
```

```{r assembleAppendixPlots}
#| fig-cap: Kernel Density Plots
#| label: fig-multiple-distributions

patchwork <- d1 + d2 + d3 + plot_layout(guides = "collect") &  theme(legend.position = "top") 
patchwork + plot_annotation(caption = "Vertical lines indicate means (dashed line = student sample).")
```

```{r plotContributionsAppendix}
#| include: false

SUM <- main[treatment == "replication",
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "student"),
            .SDcols = "contribution"]

SUM[,
    sum := round(contribution)]

SUM[,
    contribution := contribution/4]

upperLimit <- SUM$contribution %>% max() %>% round() + 5

p1 <- ggplot(data = SUM, 
             aes(x = round, y = contribution, fill = student, color = student, lty = student)) +
  layout +
  theme(legend.position="top") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Average Amount of Tokens contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) + 
  guides(fill = "none") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

rm(list = c("SUM"))
```


```{r plotShareOfContributionsAppendix}
#| fig.cap: "The average amount of tokens contributed over time in treatments."
#| include: false

SHARE <- main[treatment == "replication",
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "student"),
            .SDcols = "share"]

# SHARE <- main[,
#             .(share = sum(contribution)/sum(endowment)),
#             by = c("round", "treatment")]

upperLimit <- 0.75

p2 <- ggplot(data = SHARE, 
             aes(x = round, y = share, fill = student, color = student, lty = student)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Share of Current Endowment contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  guides(fill = "none")

p1 + p2 + plot_layout(guides = "collect") & 
  theme(legend.position = "top")

# rm(list = c("p1", "p2"))
```


```{r plotStockAppendix}
#| include: false

# data
STOCK <- main[treatment == "replication",
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "student"),
              .SDcols = "stock"]


# annotation
maxPath <- data.table(x = 1:10)
maxPath[, y := 80*1.5^x]
maxPath[, student := "replication"]
maxPath[, groupID := 42]

# plot
p1 <- ggplot(data = STOCK, 
             aes(x = round, y = stock, fill = student, color = student, lty = student)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 4700), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  geom_point(mapping = aes(x = 10, y = 4613), col = "#b5b5b5") +
  annotate("text", x = 9, y = 4630, size = 3,
           label = "max", col = "#b5b5b5") +
  geom_line(data = maxPath, mapping = aes(x = x, y = y), col = "#b5b5b5", lty = 2) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

```{r growthHeterogeneityVizAppendix}
#| fig.cap: Average wealth over time across samples.
#| include: false

# plot
p2 <- ggplot(data = main[treatment == "replication"],
       mapping = aes(x = round, y = stock, color = student, by = groupID)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 2000),
                       expand = c(0, NA)) +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  labs(y = "", x = "Period") +
  layout +
  guides(col = "none")


# assembling
p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "top")
```



```{r}
#| fig.cap: Average Gini coefficient (within groups) over time across samples
#| include: false
 
tmp <- main[treatment == "replication",
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "student"),
            .SDcols = "gini"]

GINI <- rbind(tmp, list(0, TRUE, 0), list(0, FALSE, 0))

ggplot(data = GINI, 
             aes(x = round, y = gini, fill = student, color = student, lty = student)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 0:10) +
  scale_y_continuous(limits = c(0, 0.25), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```



# C: Growth _and_ Inequality (exploratory) {.appendix}

In contrast to GMTV, I did not ask how rich and poor groups differ. Instead, I was wondering, whether equal groups are wealthier. More precisely, does the Gini coefficient correlate with growth and wealth creation? To answer that question, @fig-stock-by-gini applies a median split showing equal and unequal groups' wealth over time.

```{r plotStockByGini}
#| fig-cap: Average wealth over time across treatments.
#| label: fig-stock-by-gini

# create equality indicator (just as highgdp in original data)
median <- main[round == 10,
               median(gini)]

equalGroups <- main[round == 10 & gini > median,
                    unique(groupID)] 

unequalGroups <- main[round == 10 & gini < median,
                      unique(groupID)]

main[groupID %in% equalGroups,
     equal := TRUE]

main[groupID %in% unequalGroups,
     equal := FALSE]

STOCKe <- main[equal == TRUE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

STOCKu <- main[equal == FALSE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

upperLimit <- 700


p1 <- ggplot(data = STOCKe, 
             aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  # geom_vline(xintercept = 10, alpha = 0.66) +
  geom_line() +
  geom_point() +
  guides(lty = "none", fill = "none") +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Wealth (Equality)", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Replication Sample"))

p2 <- ggplot(data = STOCKu, 
             aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  # geom_vline(xintercept = 10, alpha = 0.66) +
  geom_line() +
  geom_point() +
  guides(lty = "none", fill = "none") +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Wealth (Inequality)", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Replication Sample"))

p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "bottom")

rm(list = c("STOCKe", "STOCKu", "p1", "p2"))
```
-->