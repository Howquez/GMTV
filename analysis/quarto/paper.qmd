---
title: Revisiting 'Growth and Inequality in Public Good Provision' ---Reproducing and Generalizing Through Inconvenient Online Experimentation
author:
  - name: Hauke Roggenkamp
    email: hauke.roggenkamp@unisg.ch
    affiliations:
        - id: HSU
          name: Helmut Schmidt University
          department: Department of Economics
          address:  Holstenhofweg 85
          city: Hamburg
          state: Germany
          postal-code: 22043
        - id: HSG
          name: University of St. Gallen
          department: Institute for Behavioral Science and Technology
          address: Torstrasse 25
          city: St. Gallen
          state: Switzerland
          postal-code: 9000
    # attributes:
    #     corresponding: true
    # note: This is the first author footnote.
abstract: |
  I revisit the dynamic public good game by GÃ¤chter et al. (2017) developed to study cooperation under dynamic interdependencies. Collecting data from both a convenient (students) and an inconvenient (general population) sample, I not only reproduce some of the author's original observations but also test their novel game's generalizability. Appending a charitable dictator game, I find no correlations between behavior in the charitable context and the dynamic game. This applies to students and the general population sample alike. Because the study of inexperiend general population samples raises methodological challenges, such as fatigue and dropouts, this research approaches them. Doing so, I and provide simple solutions to run relieable interactive experiments online.
  
  You can find the most recent version of this paper [here](https://github.com/Howquez/coopUncertainty/blob/main/analysis/quarto/paper.pdf).
keywords: 
  - Replication study
  - Non-convenience sample
  - Open science
  - Dynamic public good game
  - Online experiment
  - Generalizability
date: last-modified
bibliography:  ../biblio.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal of Behavioral and Experimental Economics
      formatting: preprint
      model: 3p
      cite-style: authoryear
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE,
                      fig.width = 10, fig.height = 4)
```

```{r locale}
#| include: false
#| warning: false
#| eval: false

Sys.setlocale("LC_TIME","English United States")
```

```{r packages}
# install this package once, it'll then install and load all remaining packages
# install.packages("pacman")

pacman::p_load(magrittr, data.table, stringr, lubridate, glue, DescTools, haven, censReg,
               stargazer, vtable, ggplot2, patchwork, MetBrewer, Rmisc, knitr)
```

```{r constants}
SHOW_UP_FEE <- 5
EXCHANGE_RATE <- 1/20
```

```{r design}
# ggplot layout
layout <- theme(panel.background = element_rect(fill = "transparent", color = NA),
                plot.background = element_rect(fill = "transparent", color = NA),
                panel.grid = element_blank(),
                panel.grid.major.y = element_blank(),
                legend.key = element_rect(fill = "transparent"),
                axis.line = element_line(size = 0.25),
                axis.ticks = element_line(size = 0.25),
                axis.title = element_text(size = 8),
                legend.text = element_text(size = 8),
                plot.caption = element_text(size = 6,
                                            colour = "#555555"),
                legend.title = element_blank()
)

# color
colors <- met.brewer(name="Tam",n=7,type="discrete")
colors <- c("#F3B05C", "#1E4A75", "#65B5C0", "#AD5E21")
# cPrimary = "#ffd353"
# cSecondary = "#9f2d55"
# cInfo = "#34168"
cPrimary = "#F3B05C"
cSecondary = "#1E4A75"
cInfo = "#36BFA2"
```

<!-- OUR DATA -->


```{r readAllData}
# identify files of all sessions
files <- list.files(path = "../../data/replication/",
                       recursive = TRUE,
                       full.names = TRUE)

# load all of them into a list
csvs <- list()
for(i in files){
  name <- str_extract(string = i, pattern = "2021.*")
  temp <- read.csv(i, stringsAsFactors = FALSE) %>% data.table()
  csvs[[name]] <- temp
}

# bind list into single data table
full <- rbindlist(l = csvs, use.names = TRUE) %>% data.table()

# ignore observations who have not made it to the first contribution
DT <- full[participant._index_in_pages > 2 & 
             participant.time_started != "" & 
             participant.label != "" &
             !(is.na(dPGG.1.player.contribution))]

# tidy up
rm(list = c("files", "csvs", "i", "name", "temp"))
```

```{r metaData}
# aggregate each session's meta data
meta_long <- full[participant.time_started != "" & 
                    participant.label != "",
                  .(session.code,
                    participant.label,
                    participant.time_started,
                    participant._index_in_pages,
                    dPGG.1.player.contribution)]

# reformat time stamp in new variables
meta_long[, `:=`(date = participant.time_started %>% ymd_hms() %>% date(),
                 hour = participant.time_started %>% ymd_hms() %>% hour())]

# aggregate and count 
meta <- meta_long[,
                  .(date = date %>% unique(),
                    # weekday = date %>% unique() %>% lubridate::wday(label = TRUE),
                    time = hour %>% unique() %>% paste("00", sep = ":"),
                    showups = participant.time_started %>% length(),
                    dropouts = sum(participant._index_in_pages == 2),
                    residuals = sum(is.na(dPGG.1.player.contribution) & participant._index_in_pages == 77)),
                  by = session.code]

# get number of participants and observations (groups of 4)
meta[,
     `:=`(participants = showups - dropouts - residuals,
          observations = (showups - dropouts - residuals)/4)]

# save data
fileName <- "replication2021"
save(meta, file = paste0("../../data/processed/rda/", fileName, "_meta", ".rda"))
write.csv(meta, file = paste0("../../data/processed/csv/", fileName, "_meta", ".csv"))

rm(list = c("meta_long"))
```

```{r identifyStudents}
# we had to invite students as the general population sample was exhausted, unfortunately. let's identify them:
invitedStudents <- read.table(file = "../../data/sample/invited_students.txt", col.names = c("participant.label")) %>%
  data.table()
# note that these students participated in the ultimate session
```

```{r focusOnFirstRound}
# create data table 
replicationFirstRound <- DT[,
                 .(participant.code,
                   treatment = "replication",
                   session.code,
                   groupID = paste(session.code, dPGG.1.group.id_in_subsession, sep = "_"),
                   othersContribution = dPGG.1.group.total_contribution - dPGG.1.player.contribution,
                   ownContribution = dPGG.1.player.contribution,
                   trust = Outro.1.player.PQ11,
                   government = Outro.1.player.PQ13,
                   comprehension = dPGG.10.player.comprehension)]

# save data
save(replicationFirstRound, file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(replicationFirstRound, file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))
```

```{r subsetData}

# identify most relevant variables
mRegex <- "participant\\.code$|session\\.code$|dPGG\\.1\\.group\\.id_in_subsession|^dPGG.1.player.id_in_group$|\\.1\\.player.belief|endowment|contribution|stock|gain$|bot_active|10.player.comprehension|10.player.donation"
mainVariables <- str_subset(string = names(DT), pattern = mRegex)

# prune all other variables
subset <- DT[, ..mainVariables]

# tidy up
rm(list = c("mRegex", "mainVariables"))
```

```{r addNewVariables}
# refactor groupID such that it eventually contains treatment-info
subset[, groupID := paste(session.code, 
                          dPGG.1.group.id_in_subsession,
                          sep = "_")]

# add share as contribution/endowment
for(round in 1:10){
  contribution <- glue("dPGG.{round}.player.contribution")
  endowment <- glue("dPGG.{round}.player.endowment")
  subset[, glue("dPGG.{round}.player.share") := subset[[contribution]]/subset[[endowment]] ]
}

# add treatment variable
subset[,
       treatment := "replication"]
```

```{r calculateAggregates}
# we need to aggregate some outcomes on a groupID level (per sessions per treatment)
# we'll do so using a loop
# this will yield a list of data tables that will be merged to a list eventually.
cluster <- c("treatment", "session.code", "groupID")
outcomes <- c("contribution", "endowment", "stock", "gain", "bot_active")

DTs <- list()
for(outcome in outcomes){
  if(outcome == "bot_active"){
    var = names(subset) %>% str_subset(pattern = glue("group\\.{outcome}$"))
  } else {
    var = names(subset) %>% str_subset(pattern = glue("player\\.{outcome}$"))
  }


  # calculate either averages or the sum per round per group
  aggregates = subset[, lapply(.SD, sum, na.rm=TRUE), 
                      by = cluster, 
                      .SDcols=var]
  
  # transform from wide to long
  meltedAggregates <- melt(aggregates, id.vars = cluster, measure.vars = var)
  DTname <- glue("{str_to_title(outcome)}")
  DTs[[DTname]] <- meltedAggregates
  rm(list = c("DTname", "meltedAggregates", "aggregates", "var", "outcome"))
}
```

```{r renameVariables}
# each data table in that list has the same column names.
# the outcomes are all named "value", for instance.
# now, we"ll infer the round number (contained in the variable name)
for(i in 1:length(outcomes)){
  DTs[[i]] <- DTs[[i]][,
                       .(treatment,
                         session.code,
                         groupID,
                         round = str_replace_all(string = variable,
                                                 pattern = "\\D", 
                                                 replacement="") %>% as.integer(),
                         value # to be renamed afterwards
                       )
  ]
  # rename "value" to outcome variable
  setnames(DTs[[i]], old = "value", new = outcomes[i])
}

# tidy up
rm("i")
```


```{r calculateGini}
# repeat everything for the gini
var = names(subset) %>% str_subset(pattern = "player\\.stock$")
gini = subset[,
              lapply(.SD, Gini, na.rm=TRUE), 
                by = cluster, 
                .SDcols=var
              ]
Gini <- melt(gini, id.vars = cluster, measure.vars = var)

DTs[["Gini"]] <- Gini[,
                    .(treatment,
                      session.code,
                      groupID,
                      round = str_replace_all(string = variable,
                                              pattern = "\\D", 
                                              replacement="") %>% as.integer(),
                      gini = value
                       )]
rm(list = c("var", "gini", "Gini"))

# note that GMTV used start of period earnnings, i.e. endowments. We use end of period earnings, i.e. stock.
# this adjustment has been considered in our processing of GMTVs data.
```


```{r mergerEverything4FinalData}
# now merge every table in the list to one final data table called "replication"
replication <- Reduce(function(...) merge(..., by=c(cluster, "round"), all = TRUE), DTs)

# tidy up
rm(list = c("cluster", "contribution", "endowment", "round", "DTs", "outcomes"))
```

```{r calculateShare}
# define the share (one of the main outcome variables) as 
# the sum of contributions devided by the sum of endowments
replication[, share := contribution/endowment]
```

```{r defineRichGroups}
# use the median to differentiate between poor and rich groups (as GMTV did)
median <- replication[round == 10,
                      median(stock)]

# find the groups that end up rich or poor
richGroups <- replication[round == 10 & stock > median,
                          unique(groupID)] 

poorGroups <- replication[round == 10 & stock < median,
                          unique(groupID)]

# mark these groups for each period
replication[groupID %in% richGroups,
            rich := TRUE]

replication[groupID %in% poorGroups,
            rich := FALSE]

# tidy up
rm(list = c("median", "poorGroups", "richGroups"))

```

```{r misc}
# flag observations where at least one participant did not understand the game
noComp <- subset[dPGG.10.player.comprehension == 0,
                 groupID] %>% unique()
replication[,
     noComprehension := 0]
replication[groupID %in% noComp,
     noComprehension := 1]


# drop observations (i.e. groups in rounds) with dropouts (bot_active == 1) and
# where round > 10
replication <- replication[bot_active == 0 & round <= 10]

# tidy up
rm("noComp")
```

```{r saveData}
save(replication, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(replication, file = paste0("../../data/processed/csv/", fileName, ".csv"))
```

```{r createCovariates}

# so far, I neglected many variables in what happened before. The following few lines
# deal with some of these variables in a similar procedure


# add variables
DT[, treatment := "replication"]
DT[, groupID := paste(session.code, dPGG.1.group.id_in_subsession, 
                    sep = "_")]

# subset
cRegex <- "participant.code|session.code|treatment|groupID|Outro.1.player|10.player.donation|10.player.stock|switching_row|inconsistent"
covariates <- str_subset(string = names(DT),
                            pattern = cRegex)
CT <- DT[, ..covariates]

# rename
names(CT) <- names(CT) %>% 
  str_replace_all(pattern =".*player\\.",
                  replacement = "") %>%
  str_to_lower()
names(CT)[names(CT) == "groupid"] <- "groupID"

# refactor
CT[, donation := donation/20] # exchange rate tokens to real world currency 1/20
CT[donation %>% is.na, donation := 0]
CT[, gender := ifelse(test = gender == "female",
                      yes  = 1,
                      no   = 0)]
CT[, inconsistent := as.logical(inconsistent)]

# reassign payoff (with stock of the last period)
CT[, payoff := stock]
CT[, stock := NULL]

# write data
replicationCovariates <- CT
save(replicationCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(replicationCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("CT", "cRegex", "covariates"))
```

```{r readTime}
# as before, we need to read a list of files (measuring the time spent per page)

# identify files
files <- list.files(path = "../../data/pageTimes/",
                       recursive = TRUE,
                       full.names = TRUE)

# loop
csvs <- list()
for(i in files){
  name <- str_extract(string = i,
                      pattern = "2021.*")
  
  temp <- read.csv(i, 
                   stringsAsFactors = FALSE) %>%
    data.table()
  
  csvs[[name]] <- temp
}

# bind files list to data.table
timeSpent <- rbindlist(l = csvs,
                       use.names = TRUE) %>%
  data.table()

# set oder
setorder(timeSpent, session_code, participant_code, epoch_time)

# tidy up
rm(list = c("files", "i", "name", "temp"))
```


```{r calcDuration}
# shift rows to calculate this duration (per page) as the distance between
# the current and the next time stamp
timeSpent[,
          lag := shift(epoch_time, fill = NA, type = "lag"),
          by = c("session_code", "participant_code")]

timeSpent[,
          duration := epoch_time - lag,
          by = c("session_code", "participant_code")]

# calculate overall time spent as the difference between the min and max time stamp
timeSpent[,
          completion := epoch_time %>% max() - epoch_time %>% min(),
          by = c("session_code", "participant_code")]

# create new data table with selected columns
duration <- timeSpent[session_code %in% meta$session.code, # participant_code %in% DT$participant.code,
                      .(
                        session_code,
                        participant_code,
                        app_name,
                        page_name,
                        page_index,
                        page_submission = epoch_time,
                        time_spent = duration,
                        completion_time = completion
                      )]

# save data
save(duration, file = paste0("../../data/processed/rda/", fileName, "_timeSpent", ".rda"))
write.csv(duration, file = paste0("../../data/processed/csv/", fileName, "_timeSpent", ".csv"))

# tidy up
rm(list = c("timeSpent", "csvs", "DT"))
```




<!-- ORIGINAL (GMTV) DATA -->

```{r readGMTV}
# having done all that, we need to prepare the original data and bind it to ours
# I'll thus, start by reading the orginal data

DT <- read_dta(file="../../data/gaechteretal/GMTV-data.dta") %>% data.table()
CT <- read_dta(file="../../data/gaechteretal/GMTV-questionnaire-data.dta") %>% data.table()
```

```{r subsetGMTV}
# because we only replicate one of their treatments, I subset accordingly
# noPunish10 <- DT[exp_num == 5 | exp_num == 8 | exp_num == 9]
noPunish10 <- DT[longgame == 0 & punish == 0 & exp_num <= 10]
```

```{r newVariablsGMTV}
# calculate the share of endowments contributed
noPunish10[,
           share := sum/gdp,
           by = .(subj_id, per)]

# calculate gini based on endowments (as GMTV did)
noPunish10[,
           gini2 := Gini(c(tokens, other1, other2, other3)),
           by = .(subj_id, per)]

# calculate gini based on stock (i.e. end of period earnings)
noPunish10[, stock0 := (tokens - putin + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock1 := (other1 - pu1 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock2 := (other2 - pu2 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock3 := (other3 - pu3 + 1.5*sum/4) %>% ceiling()]
noPunish10[, 
           gini3 := Gini(c(stock0, stock1, stock2, stock3)),
           by = .(subj_id, per)]
```

```{r createGMTVtable}
# select columns we are interested in and rename a few to match our data

GMTV <- noPunish10[order(gr_id, per),
           .(treatment = "noPunish10", 
             session.code = exp_num,
             groupID = gr_id, 
             round = per,
             contribution = sum,
             endowment = gdp,
             share,
             stock = (gdp + ceiling(sum/4*1.5)*4-sum),
             gain = (ceiling(sum/4*1.5)*4-sum),
             gini = gini3,
             bot_active = 0,
             noComprehension = NA)] %>% unique()

# tidy up
rm(list = c("noPunish10"))
```

```{r richPoorGMTV}
 # create rich indicator (just as highgdp in original data)
median <- GMTV[round == 10,
                   median(stock)]

richGroups <- GMTV[round == 10 & stock > median,
                       unique(groupID)] 

poorGroups <- GMTV[round == 10 & stock < median,
                       unique(groupID)]

GMTV[groupID %in% richGroups,
         rich := TRUE]

GMTV[groupID %in% poorGroups,
         rich := FALSE]

# tidy up
rm(list = c("median", "richGroups", "poorGroups", "fileName"))
```

```{r saveGMTV}
fileName <- "GMTV2017"
# save data
save(GMTV, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(GMTV, file = paste0("../../data/processed/rda/", fileName, ".csv"))
```


```{r calcSwitchingPoint}
# GMTV gathered some risk data that I'll process in a separate DT and merge it
# with other covariates later on

GMTVRisk <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
               .(participant.code = subj_id,
                 e10, e20, e30, e40, e50, e60, e70,
                 inconsistent = ifelse(test = e60 < e70 | e50 < e60 | e40 < e50 | e30 < e40 | e20 < e30 | e10 < e20,
                                       yes  = TRUE,
                                       no   = FALSE)
                 )]

GMTVRisk[,
         switching_row := ifelse(test = inconsistent == TRUE,
                                yes  = NA,
                                no   = e10 + e20 + e30 + e40 + e50 + e60 + e70 + 1 + 2) # because GMTV used a 7-point-likert scale
         ]
```

```{r subsetCovariates}
# subset covariate data and rename+refactor some variables
temp <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
                         .(treatment = "noPunish10",
                           session.code = exp_num,
                           groupID = gr_id,
                           participant.code = subj_id,
                           gender,
                           age = Sys.Date() %>% lubridate::year() - age,
                           pq01 = q1,
                           pq02 = q2,
                           pq03 = q3,
                           pq04 = q4,
                           pq05 = q5,
                           pq06 = q6,
                           pq07 = q7,
                           pq08 = q8,
                           pq09 = q9,
                           pq10 = q10,
                           pq11 = q11,
                           pq12 = q12,
                           pq13 = q13,
                           pq14 = q14,
                           donation = don
                           )]

temp[donation %>% is.na, donation := 0]

```

```{r mergeAndWrite}

GMTVCovariates <- data.table::merge.data.table(x = temp,
                                               y = GMTVRisk[, .(participant.code, inconsistent, switching_row)],
                                               by = c("participant.code"))

# write data
save(GMTVCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(GMTVCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("temp", "GMTVRisk"))
```

```{r firstRound}
noPunish <- DT[exp_num == 1 | exp_num == 3 | # noPunish15
                 exp_num == 5 | exp_num == 8 | exp_num == 9] # noPunish10

temp <- noPunish[per == 1]
temp <- temp[,
             treatment := "noPunish10"]
temp <- temp[exp_num == 1 | exp_num == 3,
             treatment := "noPunish15"]

GMTVFirstRound <- temp[,
                           .(participant.code = subj_id,
                             treatment,
                             session.code = exp_num,
                             groupID = gr_id,
                             othersContribution = sumputin - putin,
                             ownContribution = putin,
                             trust = NA,
                             government = NA,
                             comprehension = NA)]

# save data
save(GMTVFirstRound, 
     file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(GMTVFirstRound, 
          file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))

rm(list = c("temp", "fileName"))
```

<!-- MERGE & TUNE DATA -->

```{r rbindOursAndGMTV, eval = TRUE}
main <- rbindlist(list(replication, GMTV), 
                  use.names = TRUE)

R1   <- rbindlist(list(replicationFirstRound, GMTVFirstRound), 
                  use.names = TRUE)

covs <- rbindlist(list(replicationCovariates, GMTVCovariates), 
                  use.names = TRUE,
                  fill = TRUE)

rm(list = c("CT", "DT", "GMTV", "GMTVCovariates", "GMTVFirstRound", 
            "noPunish", "replication", "replicationCovariates",
            "replicationFirstRound", "subset"))
```

```{r treatmentAsFactor}
main[, treatment := as.factor(treatment)]
R1[, treatment := as.factor(treatment)]
covs[, treatment := as.factor(treatment)]
```

```{r studentSample}

# Two of these sessions were special: The first (`r meta[1, session.code]`) as well as the last one (`r meta[, session.code %>% tail(n=1)]`). The first session suffered technical problems such that the risk elicitation task was omitted. The last session (almost exclusively) relied on a student sample as our non-student sample was exhausted after the first three sessions. As a consequence, the last session was conducted with 59 students while all others were conducted without any students. I'll therefore create a boolean `student` variable. Becaue all of the original experiments were conducted with students, `student` also equals 1 in the "noPunish10" treatments

main[, student := FALSE]
main[session.code == "d6jrsxnr" | treatment == "noPunish10",
     student := TRUE]

covs[, student := FALSE]
covs[session.code == "d6jrsxnr" | treatment == "noPunish10",
     student := TRUE]

R1[, student := FALSE]
R1[session.code == "d6jrsxnr" | treatment == "noPunish10",
   student := TRUE]
```

```{r earnings}

# This chunk requires raw data
files <- list.files(path = "../../data/replication/",
                       recursive = TRUE,
                       full.names = TRUE)
csvs <- list()

for(i in files){
  name <- str_extract(string = i,
                      pattern = "2021.*")
  
  temp <- read.csv(i, 
                   stringsAsFactors = FALSE) %>%
    data.table()
  
  csvs[[name]] <- temp
}

full <- rbindlist(l = csvs,
                  use.names = TRUE) %>%
  data.table()


# subset data of participants who have completed the study
tmp <- full[participant._index_in_pages >= 76 & 
              participant.time_started != "" & 
              !(is.na(dPGG.1.player.contribution)) &
              participant.label != ""]

# calculate total payoff
tmp[, finalPayoff := (dPGG.10.player.stock - dPGG.10.player.donation + HLPL.1.player.payoff) * EXCHANGE_RATE + SHOW_UP_FEE]

earningsMean <- tmp[, mean(finalPayoff, na.rm = TRUE)]
earningsSD   <- tmp[, sd(finalPayoff, na.rm = TRUE)]
```



<!-- PAPER STARTS HERE -->

<!-- 
# Key Takeaways

- We **[replicate](#gmtv-replication)** a public goods experiment with dynamic inter-dependencies and find similar results as @GMTV2017. 
    + Absolute contributions increase over time.
    + Just as in _static_ public goods experiments, the share of endowments contributed decreases over time.
    + The richest groups earn fifteen times more than the poorest groups.
    + While there clearly is growth, groups do not realize the maximal potential efficiency and earn just over `r main[treatment == "replication" & round == 10, mean(stock)/(80*1.5^10)*100] %>% round(digits=1)`% of what is possible.
- Varying the subject pool and including a voluntary climate action (VCA), we have a setup similar to @GKLS2020, which allows us to assess **[generalizability](#generalizability)**.^[Note that one can easily model the VCA as a dictator game, which relates to a literature summarized by @cartwright2022.]
- Studying a general population sample online, synchronously and over the course of multiple periods, we find that classic **[lab procedures](online-methodology)** can be applied outside of the lab too.
    + Even though we conducted the experiment online and remotely, dropouts (or "attrition") are no concern.
    + Relying on an inexperienced non-convenience sample that cannot interact with experimenters, `r trunc((R1[comprehension > 1] %>% NROW() / R1[!(is.na(comprehension))] %>% NROW())*100)`% of all participants stated that they understood the game.
    + Participants make relatively fast decisions which makes longer games feasible in the future.^[Taken together with inefficient growth and the lack of dropouts, additional rounds are fairly cheap.]

-->

# Introduction {#sec-intro}

Today's actions are tomorrow's result. There are many settings in which current decisions affect future outcomes and with it, future decision spaces. Opting for environmental friendly policies today not only reduces carbon dioxide omissions immediately but also helps us to reach the Paris climate targets tomorrow. Deferring these policies, may not necessarily prevent us from reaching these targets, but it requires more effort in the future compared to a path that includes immediate action. Aiming at certain goals, today's actions (or the omission thereof) not only affect intermediate outcomes but also the number of paths one can choose from that lead to that specific goal.

Public good (or public bad) games---although often intended to inform climate policies (e.g. @MilinskiEtAl2006, @TavoniEtAl2011, @Hauser2014, @BrickEtAl2015, @GomezEtAl2018, @CalzolariEtAl2018, @CookEtAl2019)---miss these temporal interdependencies simply because participants have the same set of actions in each period. Accordingly, participants' actions in a given period do not affect their number of actions in subsequent periods. To see the lack of realism, consider carbon dioxide emissions, where the current stock will last for well over a millennium [@Inman2008; @CalzolariEtAl2018]. Playing with fresh endowments in each period is as if one could just undo carbon dioxide emissions at no cost.

A game designed by GÃ¤chter, Mengel, Tsakas & Vostroknutov [-@GMTV2017 hereafter, GMTV] as well as Stefan GroÃe (2011, unpublished) shows that it is fairly simple to remedy that flaw. They incorporated interdependencies into a _dynamic_ public good game by defining endowments as the income of previous periods. To put it differently, participants' actions in a given period affect their number of actions in subsequent periods: the more (less) they earn now, the more (less) they can contribute in the next period. Importantly, this modification qualitatively yields the same rational predictions as the static game (i.e. free-riding and the under-provision of the public good). It is thus, equally well suited to study dilemma situations.

Because there is surprisingly little experimental research on interdependencies^[One exemption are @Moser2019, who built on GMTV's design to investigate leadership.], I reproduced one of GMTV's treatments to compare dynamics across (in)experienced samples to investigate its generalizability.
@GKLS2020 find that static public good games do not generalize well to real-world climate action. They also find that generalizability depends on structural resemblance of the public goods game with the context of climate change mitigation: Greater resemblance improves generalizability. Because GMTV's dynamic setting has a more realistic propertyânamely, interdependenciesâone would expect it to be better suited to inform public policy. To test this intuition, I not only ran the experiment with different samples but also
<!--, that is, with participants who make their decisions in an environment which is more natural than the lab.-->
observed the participants' behavior in voluntary climate actions (VCA).^[The VCA is a (charitable) dictator games where each participant is a dictator dividing her budget between herself and some organization linked to the reduction of CO2 emissions. @EckelGrossman1996 were the first to implement a charitable dictator game observing contributions of 30% of the endowments. Like @GKLS2020, @CarpenterEtAl2008 report that students make lower contributions to charity than community members.] This yielded a setting similar to @GKLS2020's which allows me to analyze how behavior in the abstract game translates into real-world action across samples. This research shows that the dynamic setting _does not_ add any advances with respect to generalizability of results.

@AGM2018 conducted static public good games in the lab and on MTurk to draw lessons from online experimentation. This study extends this literature [see also @GoodmanPaolacci2017  @AmirEtAl2012] by focusing on an _inconvenient_ sample (i.e. one that is completely inexperienced sample that has not been exposed to interactive experiments before) playing a computationally more complex game. This required me to design a robust (and thus, more complex) software to minimize attrition. I collected paradata to assess the desired fluency and feasibility of the experiment. This research reports on the robust design that made the dynamic game feasible.

Taken together, this study makes three contributions. First, it reproduces parts of GMTV's original experiment and highlights the importance of pure replications. Second, it shows that logistically complex online experiments are feasible for samples other than students or clickworkers. Third, this paper supports critics arguing that findings from abstract games do not generalize well---not even with a more representative sample. 

After commenting on transparent research practices and reporting the methods in @sec-transparency and @sec-methods, this paper is organized along these findings: The confirmatory reproduction is reported in @sec-replication, the online feasibility in @sec-feasibility and the generalizability in @sec-generalizability. @sec-conclusion concludes.

# Transparency {#sec-transparency}

Well before credibility crises comprised a variety disciplines, @bemwriting [p. 2] indirectly conceded discrepancies between fuzzy research processes and the polished article that results from it:

> _"There are two possible articles you can write: (a) the article you planned to write when you designed your study or (b) the article that makes the most sense now that you have seen the results. They are rarely the same, and the correct answer is (b)."_ 

Now, 20 years later, we accustom ourselves with mechanisms designed to unravel the fuzzy back-and-forth between exploratory and confirmatory research. Prominently, pre-registrations were established to tie the researchers' hands when doing confirmatory research--reducing observable discrepancies between Daryl Bem's above-mentioned (a) and (b). Today, researchers can not only choose where to pre-register but also decide about the level of detail they provide. More severely, nothing stops researchers from pre-registering multiple hypotheses opposing each other (see e.g. @SimmonsEtAl2021 and @PhamEtAl2021 for a discussion).

@WaldronAllen2022 report a wide variety of depth of pre-registrations in the field of Neuropsychopharmacology and document that the _'pre-registered'_ label can be obtained with minimal effort. In absence of transparency and scrutiny pre-registrations run into danger to miss the target of making science more credible. Even more so, they may have adverse effects as the _'pre-registered'_ label becomes cheap (too) to acquire.

Discussing the costs and benefits of pre-registrations, @Olken2015 stresses the uncertainty of research processes and promotes moderate approaches. As @WaldronAllen2022, he argues that pre-registrations not necessarily eliminate  flexibility. These 'moderate' or 'flexible' approaches require transparency to reduce concerns of _'data-mining'_ [@Olken2015 p. 61] by showing (a) the analyses I planned to run, (b) the analysis that made most sense after collecting the data and the transition from (a) to (b). Because this is neither incentivized nor easy to verify, we need tools that make transparency convenient to establish and to scrutinize.
I propose version control (such as Git) as one such tool and illustrate its application on this paper:  GitHub is a website and cloud-based service where developers---and researchers alike---can store and manage their code. The service is based on Git^[Git is a specific open-source version control system developed in 2005.] and designed for version control. Importantly, changes are time-stamped and can be tracked. Moreover, one can create branches, that is, duplicates of code -- either to work collaborative or to archive a certain state. 

To sum up, pre-registrations alone are necessary but not sufficient to restore the credibility in experimental sciences. Pre-registrations must come with additional emphasis on transparency covering the whole research process as well as with a higher level of scrutiny that looks beyond mere labels.

In fact, the experiment reported in this article qualifies for the _'pre-registered'_ label at a first glance:
The analysis was pre-registered in the American Economic Association's RCT Registry [@preregistration]. Further, I pre-registered the exact analyses I planned to run when I designed the experiment on [GitHub](https://github.com/Howquez/coopUncertainty/blob/July21Replication/analysis/reports/rmd).^[https://github.com/Howquez/coopUncertainty/tree/July21Replication/analysis/reports/rmd to run the code, you need to executed the .Rmd files in this repository in the order that is indicated by its file names.] 

At a second glance, the analysis that made the most sense after having collected the data and that is reported here is a little different though: Because the more representative subject pool was exhausted earlier than expected, I recruited students which opened up a new research direction: assessing generalizability. Hence, this article combines both confirmatory as well as exploratory research. 

Accordingly, I  changed many of the analysis scripts.^[The source code of this document contains the analysis and can be found [here](https://github.com/Howquez/coopUncertainty/blob/main/analysis/quarto/paper.qmd): https://github.com/Howquez/coopUncertainty/blob/main/analysis/quarto/paper.qmd] However, because the originally planned analysis code is archived and reproducible, these changes are transparent. Importantly, I followed a literate programming approach [@Knuth_1984; @AkhtarYe_2023]. Hence, all documents needed to analyze the data and to write the report stem from the same source. This establishes consistence between the commands one tells the statistical software to do and the explanation one tells human beings one told the statistical software to do. As such, (a), (b) and the transition from (a) to (b) are not only transparent but also comprehensible.

<!--
I archived detailed analyses scripts and created a [branch](https://github.com/Howquez/coopUncertainty/tree/July21Replication) a few days _before_ we collected data and pre-registered the analysis via the American Economic Association's RCT Registry [@preregistration].
-->

# Methodology {#sec-methods}

In the terminology of @Hamermesh2007, I ran both a _pure_ as well as a _scientific_ reproduction^[@Parsons2022 use the term of a _conceptual replication_ which means the same.] of one treatment of GMTV's dynamic public good game. The pure reproduction re-analyzes the original data. [Appendix A](#A:-Pure-Replication) documents [errors I identified]() in the original paper. The scientific reproduction, where I utilize a different sample drawn from a different population in a different situation, is described in the following sections.

## Experimental Design {#sec-design}

The design builds on the workhorse model @Zelmer2003 [p.301] describes in her meta-analysis, where 

> _"subjects are divided into groups and play the same game for a finite number of periods. Each period, every subject is endowed with an income [...] The subject must then divide this income between a contribution to a private account [...] that yields a constant return to themselves only and a contribution to a public account [...] where consumption benefits accrue to all group members. At the end of each period, subjects typically learn the aggregate contribution to the public good by all members of their group and their earnings for the period."_

Considering the design, the only differences between such a _static_ game (with partner matching) and GMTV's design are _dynamics_: Instead of receiving fresh endowments every period, participants receive one endowment only at the beginning of the first period. A participant's endowment in the second period is the wealth she accumulated in the first period. A participant's endowment in the third period is the wealth she accumulated in the first two periods. And so on. Hence, a decision in one period has consequences on future endowments and, ultimately, growth paths. For this reason, the game is described as a _dynamic_ public good game. 

 As in the NOPUNISH 10 Period treatment of GMTV, I ran sessions with groups of four ($i \in I=\{1,2,3,4\}$), an initial endowment of $N_i^1 = 20$ tokens^[A token was worth `r EXCHANGE_RATE` Euros.], $T=10$ periods, a private account with a return of $1$ and a group account with a return of $1.5$ ($\Rightarrow$ MPCR$\equiv \frac{1.5}{4}$). With $i$'s contribution in period $t$ being $c_i^t$, the model looks as follows:

$$
N_i^{t+1}=N_i^t - c_i^t + \frac{1.5}{4}\sum_{j=1}^4 c_j^t
$$


## Voluntary Climate Action (VCA)

Like GMTV we employ a real giving task after the abstract game. In contrast to GMTV and like @GKLS2020, we employed a VCA, where participants could donate any amount their earnings to offset carbon dioxide (that is, retire emission permits from the EU ETS).^[Importantly, @GKLS2020 made the VCA decision with a fresh endowment _before_ they played the abstract game. I deviate from their procedure to match GMTV's procedure.]
To ensure that each participant had the same basic level of information about the impact of their decision, I provided some basic information about the mechanism. The information also highlighted that the mitigation came into operation on a European level.
Finally, I informed the participants that the documentation of individual and aggregate contributions were to be posted immediately after the concluding the sessions online. To avoid privacy or social image concerns, participants learned their unique and random IDs, which they needed to identify their individual contributions.
The document certified that their contributions have been used to offset [1.82 tons](https://www.compensators.org/compensatelist/?searchterm=stefan+traub) of carbon dioxide emissions.


## Recruitment and Sample Characteristics {#sec-sample}

I recruited the participants from the so called _[HamburgPanel](https://www.wiso.uni-hamburg.de/forschung/forschungslabor/umfragelabor/aktuelle-umfragen/hamburgpanel.html)_ using HROOT [@hroot]. The panel is provided by the University of Hamburg's Research Laboratory, which used a randomized last digits approach to build the panel while drawing from the population of citizens of Hamburg, Germany. Because the sample was exhausted at one point, I also recruited students from the University of Hamburg.

At the time I conducted the experiment, the more representative sample was not familiar with interactive experiments. In fact, I ran the first interactive group experiment with this sample. The students, in contrast, were used to single-player experiments. Recruiting them, I excluded those who have participated in interactive experiments such as public goods games before, to keep the two distinct subject pools comparable in terms of their experience. As a consequence, nonnaivetÃ© is unlikely to affect the validity of the experiment [@GoodmanPaolacci2017 p. 204].

Throughout this paper, I will compare results of my experiment with the results of GMTV's NOPUNISH 10 Period treatments. I am thus, referring to three different samples utilized at two points in time: the University of Nottingham's students (in late 2012), Hamburg's citizens and the University Hamburg's students (both in July 2021). @tbl-sample-properties gives an overview.

```{r sampleDifferences1, results = "asis", warning = FALSE, eval=FALSE}

covs[, sample := "Nottingham Students"]
covs[treatment == "replication" & student == TRUE, sample := "Hamburg Students"]
covs[treatment == "replication" & student == FALSE, sample := "Hamburg Citizens"]
covs[, sample := as.factor(sample)]
# covs[, sample := ordered(sample, levels = c("Nottingham Students", "Hamburg Students", "Hamburg Citizens"))]


m1 <- lm(formula = gender ~ sample, data = covs)
m2 <- lm(formula = age ~ sample, data = covs)
m3 <- lm(formula = switching_row ~ sample, data = covs)
m4 <- lm(formula = donation ~ sample, data = covs)
m5 <- lm(formula = pq11 ~ sample, data = covs)
m6 <- lm(formula = pq12 ~ sample, data = covs)
m7 <- lm(formula = pq13 ~ sample, data = covs)
m8 <- lm(formula = pq14 ~ sample, data = covs)

stargazer(m1, m2, m5, m6, m7, m8,


          column.labels = c("female", "age",
                            "trust", "meritocracy", "government", "equality") %>% str_to_title(),
          model.numbers = FALSE,
          dep.var.labels.include = FALSE,
          header=FALSE,
          intercept.bottom = FALSE,
          covariate.labels = c("Hamburg Citizens", "Hamburg Students", "Nottingham Students"),

          type = "latex", digits = 2, omit.stat = c("adj.rsq", "f"), df = FALSE
          )
```

```{r sampleProperties}
#| results: 'asis'
#| label: tbl-sample-properties
#| tbl-cap: Sample Properties

covs[, sample := "Nottingham Students"]
covs[treatment == "replication" & student == TRUE, sample := "Hamburg Students"]
covs[treatment == "replication" & student == FALSE, sample := "Hamburg Citizens"]
covs[, sample := as.factor(sample)]

vtable::sumtable(data = covs, 
                 vars = c('age', 'gender', 'switching_row'),
                 group = 'sample',
                 group.test = TRUE,
                 digits = 2,
                 labels = c('Age', 'Female', 'Switching Row'),
                 # note = "Switching Row describes choices in a multiple price list to elicit risk preferences. The higher the value, the higher the certainty equivalent. Government Responsible is self-reported based on a question adapted from GMTV ('The government should take responsibility that people are better provided for.')",
                 out = 'latex')
```


```{r}
# covs[, sample := ordered(sample, levels = c("Nottingham Students", "Hamburg Students", "Hamburg Citizens"))]
```

Overall, I recruited `r covs[treatment == 'replication', .N]` participants for the experiment. The three samples differ significantly with respect to their age. The non-student sample is more diverse compared to the two student samples. 





## Software {#sec-software}

The experiment was logistically complex for several reasons. First, the sample was inexperienced. Second, the experiment was interactive and synchronous. Third, the underlying game was dynamic and interdependent. This makes dropouts not only more likely but also more expensive, which is why attrition was a major concern implementing the experiment.

I chose oTree [@oTree] to implement the experiment because it is open-source, well documented and very flexible. Its [Bootstrap](https://getbootstrap.com/) (a powerful frontend toolkit) integration allowed me to make the graphical user interface interactive, appealing and easy to navigate. The [Highcharts library](https://www.highcharts.com/) made it easy to visualize results and to communicate dynamics. Insofar, oTree served a good tool to enhance the participants' user experience and thus, to make dropouts less likely.

Which features were required to handle dropouts? First, participants had to be matched to form a group _after_ comprehension questions were answered successfully. Importantly, participants were grouped by the order they answered these questions to reduce waiting times. While waiting for other players to form the group, the participants saw a wait-page informing them that they are waiting for other participants to arrive and that they do not have to wait for longer than 10 minutes. The screen also informed them that they would receive a _patience bonus_ of one Euro after the expiration of that time (or what was left of it). Second, participants only had 10 minutes to make the first contribution and 4 minutes for the remaining contributions. After this time expired, participants were replaced by bots that made random contributions. In this case, the remaining group members were informed about the replacement. Both features were implemented to limit wait times and boredom for other participants. @sec-feasibility shows that the first feature became effective in some cases, wheres the second feature did not.


## Procedure {#sec-procedure}

Participants entered the experiment at appointed times remotely from home. They first saw a welcome screen. After agreeing to the privacy policy, they could proceed to the instructions individually. Having read these instructions, each participant has also seen a demo-screen explaining the user interface. Before proceeding, they had to answer six comprehension questions correctly to avoid confusion in later stages [@FerraroVossler2010]. Subsequently, they saw a waiting screen until they could be matched with three other participants, who have answered the comprehension questions correctly. Once matched, they were exposed to the decision screen over ten periods. At the end of the last period, participants saw results of all periods. Subsequently, they made their VCA decision, before I elicited risk preferences [@HoltLaury2002] and finished with GMTV's questionnaire.

While I stuck to GMTV's protocol as close as possible, I deviated in a few aspects. First, the instructions were German and also covered topics inherent to the online setting (dropouts and bots, for instance). Second, I used another software (oTree instead of zTree) which also affected the graphical user interface participants were exposed to. Third, GMTV gave participants the opportunity to donate to _Doctors without Borders_ whereas we offered carbon dioxide offsets.

```{r totalTimeSpent}
tmp <- duration[page_index == 1 | page_index == 76, 
                .(page_index, participant_code, page_submission)]
tmp[order(participant_code, page_index), 
    total := page_submission - data.table::shift(x = page_submission,
                                                 n = 1,
                                                 type = "lag"),
    by = participant_code]

total_completion_time <- tmp[!is.na(total), mean(total/ 60) %>% round()]
```

The experiment lasted around `r total_completion_time` minutes on average. The earnings  averaged `r earningsMean %>% round(digits = 2)` Euros (sd = `r earningsSD  %>% round(digits = 2)`).^[This values include earnings from the incentivized risk elicitation task that is not part of the analysis.] 

# Results {#sec-results}

## Pre-registered GMTV Reproduction {#sec-replication}

```{r}
N_r <- R1[treatment == "replication", .N]
N_o <- R1[treatment == "noPunish10", .N]
```

Throughout this section, I pool data from both the citizens as well as the students from Hamburg and compare it with the students GMTV recruited in Nottingham. I refer to the two samples as the _reproduction sample_ ($N_r$ = `r N_r`) and the _original sample_ ($N_o$ = `r N_o`), respectively.

### Contribution Behavior {#sec-contributions}

```{r prepFirstSummary}
replication <- R1[treatment == "replication", ownContribution]
GMTV    <- R1[treatment == "noPunish10", ownContribution]


rows <- sapply(X = list(replication, GMTV), FUN = NROW) %>% max()
temp <- data.frame(replication = c(replication, rep(NA, rows - NROW(replication))),
                   GMTV = c(GMTV, rep(NA, rows - NROW(GMTV))))

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)
```

First, I ask whether the samples differ with respect to their initial contributions to the public good. Is the reproduction sample (consisting of both students and non-students) more pro-social than the original sample? A two-sided rank sum test reveals that it is not (p=`r rs1`). Both samples contributed 10 tokens, that is, 50% of their endowments on average (median and mean). Moreover, both samples' initial contributions resemble initial contributions participants usually make in the static game with partner matching.^[See Figure 3B in @fehrgaechter2000 [p.989], for instance.] However, in the dynamic game presented here, we are particularly interested in the subsequent periods because differences add up exponentially. Do the two groups remain similar over the course of time?


```{r firstRoundViz}
#| eval: false
#| fig-cap: Individual contributions to the dynamic public good in the first period
#| label: fig-first-round

ggplot(data = R1[treatment != "noPunish15"],
       mapping = aes(x = ownContribution, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 20),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 0.1),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Reproduction Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = R1[treatment == "replication", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 2) +
  geom_vline(xintercept = R1[treatment == "noPunish10", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 1) +
    labs(title = "", 
       y = "", x = "Initially Contributed Tokens",
       caption = "White lines indicate means (dashed line = reproduction sample).") +
  layout +
  theme(legend.position = "top")
```


```{r firstRoundSummary}
#| results: asis
#| warning: false
#| fig.cap: "test"
#| eval: false

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

stargazer(temp,
          summary.stat = c("mean", "median","sd", "max", "min", "n"),
          type = type, 
          flip = TRUE, 
          header=FALSE)
```

```{r}
for(r in 1:10){
  a <- main[round == r & treatment == 'replication', share]
  b <- main[round == r & treatment == 'noPunish10', share]
  
  wilcox.test(a, 
              b,
              exact = FALSE)$p.value %>% 
    round(digits = 4) %>% 
    formatC(format = "f", 
            digits = 4)# %>%
    # print()
  }


r <- 5
a <- main[round == r & treatment == 'replication', contribution]
b <- main[round == r & treatment == 'noPunish10', contribution]

rs2 <- wilcox.test(a, 
            b,
            exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)
```

In particular, do the two samples' contributions follow the same path over the 10 periods they played? The answer is _no_. @fig-share-of-contributions illustrates that the samples make similar contributions at the beginning and the end of the game but behave differently in between. More precisely, the left panel--depicting the average contributions in absolute terms--shows that the original sample contributed more than the reproduction sample _in all but the first and last period_.^[In period five, this difference is significant: a two-sided rank sum test yields p=`r rs2`. ] For this reason, the original sample's behavior differs from the reproduction sample's behavior in two aspects: it contributes more and exhibits a considerable drop in the last period (whereas the reproduction sample's contributions flatten). 

Note that increasing contributions over time imply increasing endowments over time. Hence, absolute contributions do not tell much about the willingness to cooperate. For this reason, the right panel in @fig-share-of-contributions shows the average _share of endowments contributed_ over time. Both samples exhibit a similar pattern: their share of endowments contributed declined and did not stabilize. However, both samples also differ with respect to one aspect: the reproduction sample's share of contributions declines faster. This is mirrored by a two-sided rank sum test which is only significant at a five percent level in periods three and four.


```{r plotContributions}

SUM <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "contribution"]

SUM[,
    sum := round(contribution)]

SUM[,
    contribution := contribution/4]

upperLimit <- SUM$contribution %>% max() %>% round() + 5

p1 <- ggplot(data = SUM, 
             aes(x = round, y = contribution, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="top") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Average Amount of Tokens contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) + 
  guides(fill = "none") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

rm(list = c("SUM"))
```


```{r plotShareOfContributions}
#| fig-cap: "The average amount of tokens contributed over time in treatments."
#| label: fig-share-of-contributions

SHARE <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "share"]

# SHARE <- main[,
#             .(share = sum(contribution)/sum(endowment)),
#             by = c("round", "treatment")]

upperLimit <- 0.75

p2 <- ggplot(data = SHARE, 
             aes(x = round, y = share, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Share of Current Endowment contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  guides(fill = "none")

p1 + p2 + plot_layout(guides = "collect") & 
  theme(legend.position = "top")

# rm(list = c("p1", "p2"))
```


Again, both samples' behavior resembles the contributions participants usually make in the static game with partner matching: contributions equal approximately half of endowments in the very first period and decrease to around ten percent of endowments by the last period.^[The right panel is thus, comparable to the visualizations _and results_ in the static game. See, for instance, Figure 1B in @fehrgaechter2000 [p.986].] In the dynamic game presented here, however, different paths lead to different levels of wealth -- even if they share the same start- and end-points. I am thus, more interested in the contributions' implications for wealth generation and growth. 

### Wealth Creation {#sec-wealth}

```{r summaryWealth}
#| results: asis
#| warning: false
#| fig.cap: "test"

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, stock]
GMTV    <- main[treatment == "noPunish10" & round == 10, stock]

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)

```

```{r wealthVariables}
originalWealth    <- main[treatment == "noPunish10" & round == 10, mean(stock)]
replicationWealth <- main[treatment == "replication" & round == 10, mean(stock)]
```

How do the different contribution-paths translate into wealth?^[To measure wealth and growth, I define a variable called _stock_ which sums the endowments of all participants in a given group at the end of the round (that is, after the contributions have been made, multiplied and redistributed).] Given that the original sample contributed more, one would expect the respective groups to be  more wealthy. A mere mean comparison indicates just that: An average group in the original sample accumulated about `r round(originalWealth)` tokens. In contrast, an average group in the reproduction sample accumulated about `r round(replicationWealth)` tokens. This difference is insignificant at conventional levels though: A two-sided rank sum test (comparing differences between samples) yields a p-Value of `r rs1` for the mean stock in last period of the game.

```{r stockDistributionViz}
#| eval: false
#| fig-cap: Groups' income at the end of the game
#| label: fig-stock-distribution

ggplot(data = main,
       mapping = aes(x = stock, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Reproduction Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = replicationWealth,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = originalWealth,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Stock",
       caption = "Grey lines indicate means (dashed line = reproduction sample).") +
  layout +
  theme(legend.position = "top")
```

```{r overallGinis}
rG <- main[round == 10 & treatment == "replication",
           .(stock = sum(stock)),
           by = groupID][, Gini(stock)] %>% round(digits = 2)

oG <- main[round == 10 & treatment == "noPunish10",
           .(stock = sum(stock)),
           by = groupID][, Gini(stock)] %>% round(digits = 2)

rSD <- main[round == 10 & treatment == "replication",
            .(stock = sum(stock)),
            by = groupID][, sd(stock)] %>% round(digits = 2)

oSD <- main[round == 10 & treatment == "noPunish10",
            .(stock = sum(stock)),
            by = groupID][, sd(stock)] %>% round(digits = 2)
```

Although there clearly is growth, groups do not realize the maximal potential efficiency: under full cooperation, a group can accumulate at least `r (80*1.5^10) %>% trunc()` tokens or EUR `r (80*1.5^10 /20) %>% trunc()`. This is depicted in the left panel of @fig-growth-heterogeneity, where one can see the average wealth over time by sample. The panel illustrates for both samples that growth was continuous and surprisingly linear, given the exponential character of the game's design. Despite somewhat differing contribution behavior between samples, neither the eventual wealth nor the corresponding growth paths differed. Differences in contribution behavior did, thus, not translate to significantly different wealth outcomes.

Why? Perhaps because the heterogeneity within samples and across groups has been too large to _detect_ a significant difference. The right panel of @fig-growth-heterogeneity depicts heterogeneity: In the reproduction sample, the richest group earned `r a <- main[treatment == "replication" & round == 10, max(stock)]; a` tokens (which is about `r round(a/80, digits = 2)*100`% of the initial endowment) whereas the poorest group ends up with `r b <- main[treatment == "replication" & round == 10, min(stock)]; b` tokens (`r round(b/80, digits = 2)*100`%). More broadly, the reproduction sample is characterized by inequality between groups ($SD_{Replication} =$ `r rSD`). The same holds true for the original sample ($SD_{Original} =$ `r oSD`). Hence, the heterogeneity across groups does not differ between samples, which is remarkable because the reproduction sample was drawn from a more heterogeneous (non-convenience sample). Does it differ within groups?

<!-- doch lieber Gini? statt SD reporten? -->

```{r plotStock .column-page}

# data
STOCK <- main[,
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "treatment"),
              .SDcols = "stock"]

STOCKr <- main[rich == TRUE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

STOCKp <- main[rich == FALSE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]


# annotation
maxPath <- data.table(x = 1:10)
maxPath[, y := 80*1.5^x]
maxPath[, treatment := "replication"]
maxPath[, groupID := 42]

# plot
p1 <- ggplot(data = STOCK, 
             aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 4700), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  geom_point(mapping = aes(x = 10, y = 4613), col = "#b5b5b5") +
  annotate("text", x = 9, y = 4630, size = 3,
           label = "max", col = "#b5b5b5") +
  geom_line(data = maxPath, mapping = aes(x = x, y =y), col = "#b5b5b5", lty = 2) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

```{r growthHeterogeneityViz}
#| fig-cap: Average wealth over time across samples.
#| label: fig-growth-heterogeneity

# plot
p2 <- ggplot(data = main,
       mapping = aes(x = round, y = stock, color = treatment, by = groupID)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 2000),
                       expand = c(0, NA)) +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  labs(y = "", x = "Period") +
  layout +
  guides(col = "none")


# assembling
p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "top")
```

### Inequality {#sec-inequality}

```{r summaryGini}
#| results: asis
#| warning: false
#| fig.cap: "test"

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, gini]
GMTV    <- main[treatment == "noPunish10" & round == 10, gini]

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)

```

```{r giniVariables}
originalGini    <- main[treatment == "noPunish10" & round == 10, mean(gini)] %>% round(digits = 2)
replicationGini <- main[treatment == "replication" & round == 10, mean(gini)] %>% round(digits = 2)
```

Given the different samples and the possibility of endogenous growth--which essentially is the main feature of the game--I ask whether and how the inequality grows _within_ groups. @fig-gini-time-series illustrates that inequality did grow: at the end of the game, the original and the replication groups exhibit an average Gini coefficient of `r originalGini` and `r replicationGini`, respectively.^[The two-sided rank sum test (comparing differences between samples) yields a p-Value of `r rs1` for the mean Gini coefficient in last round of the game.] Because every participant started with the same initial endowment (in _period 0_, so to speak), every group started equally--with a Gini coefficient equaling zero.

@fig-gini-time-series also shows that this initial state of equality ended with the first period already: both samples exhibit a stark incline in inequality before the second period started. From then on, the respective Gini coefficients grew slowly but continuously -- for both samples.^[In each and every period, the two-sided rank sum test comparing gini coefficients between both sample yields p-values way over ten percent.]

```{r giniDistributionViz}
#| eval: false
#| fig-cap: Groups' Gini coefficients (within groups) at the end of the game
#| label: fig-gini-distribution

ggplot(data = main,
       mapping = aes(x = gini, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Reproduction Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = replicationGini,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = originalGini,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Gini Coefficient",
       caption = "Grey lines indicate means (dashed line = reproduction sample).") +
  layout +
  theme(legend.position = "top")
```

```{r giniOverTime}
#| fig-cap: Average Gini coefficient (within groups) over time across samples
#| label: fig-gini-time-series

tmp <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "gini"]

GINI <- rbind(tmp, list(0, "replication", 0), list(0, "noPunish10", 0))

ggplot(data = GINI, 
             aes(x = round, y = gini, fill = treatment, color = treatment, lty = treatment)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 0:10) +
  scale_y_continuous(limits = c(0, 0.25), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

**Result 1.** _The `NOPUNISH 10` treatment of GMTV can be replicated because the replication data resemble the original data with respect to initial and final contributions, wealth and growth as well as inequality._

This is remarkable given the different sample and language, the different software and user interface as well as the online setting during the COVID19 pandemic. The result suggests that, by and large, the sum of these factors did not affect peopleâs preferences towards cooperation.

## Online Feasibility {#sec-feasibility}

```{r}
N <- duration[, participant_code %>% unique() %>% length()]
```

Throughout this section, I do not consider GMTV's data and pool data I gathered from the citizens as well as the students from Hamburg (N = `r N`).

```{r reviewInformation}
#| eval: false

# to see how many people clicked on the popups to review instructions: 
full[dPGG.1.player.review_instructions !=0, .N]
full[dPGG.2.player.review_instructions !=0, .N]
#...

# or for contact information
full[dPGG.1.player.review_contact !=0, .N]
full[dPGG.2.player.review_contact !=0, .N]
# ...

```


How did the participants, who have never participated in an online group experiment before, cope with the situation? Moreover, did participants understand the unfamiliar setting they found themselves in? While the answer to the former question requires more thought, the answer to the latter simply is _yes_: `r 67` out of `r 44 + 67 + 5` answered with _"yes"_ when I asked them. Another `r 44` answered with _"rather yes"_ while nobody indicated that he or she did not understand the situation at all. There is some behavioral data supporting this finding: The user interface offered a popup to review instructions or contact information. I tracked both and find that none of the participants ever opened these popups even though they were clearly visible in the decision screen's header and introduced in the instructions. To further analyze how participants coped with the situation, I consider three additional metrics: selection into the experiment, attrition as well as the time spent on each page.

I first comment on the selection into the experiment: It was difficult to recruit the sample. The panel counted 1.209 non-students of which I were able to recruit `r main[student == FALSE, .N]` participants who finished the experiment---even though I varied the weekdays and timing of the sessions (which were conducted during a nation-wide lockdown with home office regime). For this reason, I also recruited students in the last session which explains the relatively large number of showups in @tbl-meta. Although I intended to refrain from the recruitment of students initially, this particular sub sample enabled me to investigate the generalizability of my results as I will discuss in  @sec-generalizability.

```{r showMeta}
#| label: tbl-meta
#| tbl-cap: The Experimental Sessions' Meta Data

meta %>% kable(col.names = names(meta) %>% 
                 str_replace_all(pattern = "\\.",
                                 replacement = " ") %>% 
                 str_to_title()) #, caption = 'A table of the first 10 rows of the mtcars data.')
```


```{r plotTimeData}
N <- duration[, participant_code %>% unique() %>% length()]
plotDT <- duration[app_name == "dPGG" & page_name == "dPGG_Decision",
                   .(time_spent = time_spent %>% sum()),
                   by = c("session_code", "participant_code", "page_index", "page_name")]

plotDT[, round := seq(from = 1, to = 10), by = c("participant_code")]

upperLimit <- plotDT[, time_spent %>% mean(), by = c("round")] %>% max()

tmp <- summarySE(data = plotDT,
                 measurevar = "time_spent",
                 groupvars=c("round"),
                 na.rm = FALSE,
                 conf.interval = 0.95,
                 .drop = TRUE) %>% 
  data.table()

```

Turning to the time spent on each page, I focus on the decision times in the dynamic public goods game as @Anderhub2001 did. How many seconds did the participants need to make a decision in each period of the game? Not too many. @fig-time-spent illustrates an intuitive pattern: The first decision took about `r tmp[round == 1, round(time_spent)]` seconds. The second decision--where participants first learned about the other group members' previous decisions--took longer (about `r tmp[round == 2, round(time_spent)]` seconds). Subsequently, decision times first declined and stabilized at `r tmp[round > 3, round(mean(time_spent))]` seconds. Importantly, decision times were so short that crosstalk, that is, communication through private channels--a common concern^[See, for instance, the discussion section in @AGM2018 [p. 119].] in online experiments--was unlikely, especially because it would require the identification of other group members.^[There were only `r plotDT[page_index == 12 & time_spent > 60, .N]` participants (from all four sessions) who needed more than 60 seconds to make the second decision.]

```{r plotTime}
#| fig-cap: Average Time Spent for each Contribution per Period
#| label: fig-time-spent

ggplot(data = tmp,
       mapping = aes(x = round, y = time_spent)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE, color = cInfo, lty=2) +
  geom_errorbar(aes(ymin=time_spent-ci, ymax=time_spent+ci), width=.25, alpha = 0.5, color = cInfo) +
  geom_point(color = cInfo) +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit + 10), expand = c(0, 0)) +
  labs(y = "Time Passed in Seconds", caption = "Bars indicate 95% confidence intervals.") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))
```

Considering attrition, I find that it did not affect the interactive experiment at all. To elaborate, I differentiate between dropouts and residuals:  Participants who could not be matched to other group members are called residuals. Participants who intentionally left the experiment are called dropouts. Residuals did not participate in the experiment _by design_. Dropouts did not participate in the experiment _by choice_. Out of `r meta[, sum(showups)]` people who showed up, I count `r meta[, sum(residuals)]` residuals and `r meta[, sum(dropouts)]` dropouts. All of the residuals waited to be matched to a group unsuccessfully before they got paid one Euro for their patience. In contrast, all of the dropouts left while reading the instructions and before being matched to other group members. Moreover, they got no payment at all. Hence, attrition was no concern considering the dynamic public goods game or the expenses.

**Result 2.** _Given the decision times and the fluent procedure, attrition was as negligible as it is in physical laboratories---where (a) not every invited person shows up and (b) a number of participants divisible by the group size is required as well._


## Generalizability {#sec-generalizability}

```{r generalizability_prep}
R1covs <- covs[, .(participant.code,
                   donation = donation * 20 ,
                   donationShare = (donation * 20) / payoff * 100,
                   age,
                   gender,
                   switching_row,
                   payoff)]
temp <- R1[R1covs, on = .(participant.code = participant.code)][treatment == "replication"]
temp[, contributionShare := ownContribution/20 * 100]
temp[, consistency := 1 - (contributionShare - donationShare)]

N_s <- temp[student == TRUE, .N]
N_g <- temp[student == FALSE, .N]
```


As before, I do not consider GMTVs' data in this section. Instead, I will differentiate between data from the citizens and the students from Hamburg. I refer to the two samples as the _students_ ($N_s$ = `r N_s`) and the _general population sample_ ($N_g$ = `r N_g`), respectively.


@GKLS2020 asked how much can we learn about voluntary climate action from the behavior in public goods games. Using a similar strategy, I answer the question for a _dynamic_ public good game: _Not much_. Overall, there seems to be no association between choices in the voluntary climate action and the first period in the dynamic public goods game. 


```{r vca_differences}
rs1 <- wilcox.test(temp[student == FALSE, donationShare], 
                   temp[student == TRUE, donationShare],
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)
```

@fig-kernel-generalizability shows distributions of contributions across both choices for both samples. The top panels illustrate the behavior of the general population sample. The bottom panels illustrate the behavior of the student sample. The left panels show the behavior in the VCA. The right panels show the behavior in first period of the game. A visual inspection shows that (a) mean contributions are positive in both tasks for both samples.^[The same holds true for median contributions.] (b) Furthermore, average contributions are lower in the VCA. (c) In contrast to @GKLS2020's observation, we do not observe a difference between samples in the abstract game's contribution behavior. (d) However, the student's share of income contributed to the VCA is significantly lower than the general population sample's contributions (two-sided rank sum test, p=`r rs1`). Taken together, these aggregate results indicate, that the consistency between tasks is higher for the general population than it is for students. Or, to put it differently, the general population's behavior in the abstract game better predicts their behavior in real-world mitigation context.


```{r vizGeneralizability}
#| fig-cap: Kernel distributions of contributions across tasks and subject pools.
#| label: fig-kernel-generalizability
#| fig.height: 6

# viz wrapper
plotShares <- function(stud = FALSE,
                       var  = "donationShare",
                       col  = cPrimary,
                       xlab = "",
                       ylab = ""){
  
  dt <- temp[student == stud, .(x = get(var))]
  
  model <- lm(x ~ 1, dt)
  ci1 <- confint(model, level=0.95)[1]
  ci2 <- confint(model, level=0.95)[2]
  
  ggplot(data = dt,
         mapping = aes(x = x)) +
    annotate("rect", 
             xmin = ci1, xmax = ci2, 
             ymin  =0, ymax = Inf, 
             alpha = 0.33) +
    geom_density(alpha = 0.75,
                 fill = col) +
    scale_x_continuous(limits = c(0, 100),
                       expand = c(0, NA)) +
    scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
    geom_vline(xintercept = dt[,mean(x, na.rm = TRUE)],
               lty = 2) +
    labs(x = xlab,
         y = ylab) +
  layout
}

p1 <- plotShares(stud = FALSE, 
                 col = cSecondary,   
                 var = "donationShare",
                 xlab = "Share of erarnings the general pop. contributed", 
                 ylab = "Density in VCA")

p2 <- plotShares(stud = FALSE, 
                 col = cSecondary,   
                 var = "contributionShare", 
                 xlab = "Share of endowment the general pop. contributed", 
                 ylab = "Density in dPGG Experiment (period 1)")

p3 <- plotShares(stud = TRUE,  col = cPrimary, var = "donationShare",
                 xlab = "Share of income the students contributed",
                 ylab = "Density in VCA")

p4 <- plotShares(stud = TRUE,  col = cPrimary, var = "contributionShare", 
                 xlab = "Share of endowment the students contributed",
                 ylab = "Density in dPGG Experiment (period 1)")

patchwork <- (p1 / p3) | (p2 / p4)
patchwork + plot_annotation(caption = "Dashed vertical lines indicate the respective means.\nShaded areas indicate 95% confidence intervals.")



rm(list = c("R1covs", "p1", "p2", "p3", "p4", "patchwork"))
```

@tbl-tobit-generalizability shows that this is not the case. It reports tobit regression results cautioning against transferability from dPGG results to real-world mitigation behavior: The share of endowment contributed in the first period (displayed in the first row) does not predict the share of earnings donated as a VCA. The student status negatively affects VCA donations in column two but disappears if one controls for age in column three. Importantly, the interaction of student status and first-period contributions is not significant. This suggests that the general population sample's transferability is just as bad as the student sample's. We thus, find a similar result as @GKLS2020 [p.6].

```{r generalizability_tobit}
#| results: 'asis'
#| label: tbl-tobit-generalizability
#| tbl-cap: Correlations between first-round-dPGG and VCA behavior

to1 <- censReg(formula = donationShare ~ contributionShare, 
                   data = temp, left = 0, right = 100)
to2 <- censReg(formula = donationShare ~ contributionShare + student + student * contributionShare, 
                   data = temp, left = 0, right = 100)
reg <- lm(formula = donationShare ~ contributionShare + student + student * contributionShare, 
                   data = temp,)
to3 <- censReg(formula = donationShare ~ contributionShare + student + student * contributionShare + age + gender, 
                   data = temp, left = 0, right = 100)
to4 <- censReg(formula = donationShare ~ contributionShare + student + student * contributionShare + age + gender + switching_row, 
                   data = temp, left = 0, right = 100)


stargazer(to1, to2, # reg, 
          to3, # to4,
          
          model.numbers = TRUE,
          dep.var.caption  = "Tobit Regression censoring below 0 and above 100",
          dep.var.labels = "VCA Donation as Share of Earnings",
          header=FALSE,
          intercept.bottom = TRUE,
          order = c(1, 2, 6, 3, 4, 5),
          covariate.labels = c("First-period contribution in percent", "Student (1 = yes)", "First-period contr. x Student", "Age", "Female (1 = yes)", "Risk Aversion (1-12)"),
          type = "latex", digits = 2, omit.stat = c("adj.rsq", "f"), df = FALSE
          )
```

```{r}
#| fig-cap: Scatter plot of average contributions in the dPGG and real giving task.
#| label: fig-scatter-generalizability
#| fig.height: 6
#| eval: false

p1 <- ggplot(data = temp[, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cInfo, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cInfo) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "Percentage of endowment contributed in period 1") +
  layout

p2 <- ggplot(data = temp[student == FALSE, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cSecondary, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cSecondary) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "General pop.: Percentage of endowment contributed in period 1") +
  layout

p3 <- ggplot(data = temp[student == TRUE, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cPrimary, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cPrimary) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "Students: Percentage of endowment contributed in period 1") +
  layout

p1 / (p2 + p3)
```

<!--
@fig-kernel-generalizability also indicates that the general population's contribution behavior is more consistent than the students' behavior. Hence, one may conclude that their behavior in the abstract game translates comparably well to the real world. This is not the case. If anything, the _students'_ behavior in the abstract game is more predictive for their VCA. @fig-scatter-generalizability visualizes the within-subjects relationship of both contribution tasks. More precisely, it shows a scatter plot of realized choices, with the percentage of endowment spent by each participant in the first period of the game on the x-axis and that spent in the VCA on the y-axis. In addition, the figures contains a fitted line of a linear model. The upper panel illustrates overall behavior and exhibits a slope which is indistinguishable from zero. No matter how much the participants contributed in the first round, they spent, on average, about `r temp[, round(mean(donationShare))]`% of their income on the VCA. The two bottom panels differentiate between both samples with slopes that differ qualitatively. However, none of the slopes is significant. 
-->

**Result 3.** _There is no significant correlation between average contributions in the abstract public goods game and contributions to the real public good of climate change mitigation._



# Conclusion {#sec-conclusion}

The initial goal of the experiment was to reproduce specific experiments of GMTV in an online setting using a general population sample. The results suggest that it is important to reproduce experiments---both purely and scientifically [@Hamermesh2007 p. 716]---before drawing conclusions about generalizability.

The three most important findings are as follows: First, the contribution behavior in my experiment is statistically similar to the behavior reported in the original study. Consequently, the outcomes growth and inequality are reproducible. Second, the online experiment proceeded fluently such that dropouts were no concern. Third, contribution behavior in the dynamic abstract setting is not linked to behavior in the real world -- neither for students nor a more representative sample.

The significance of the first result is that similar procedures led to replicable findings under different circumstances across two different samples. The second result is of methodological importance: It highlights that even logistically complex experiments can be conducted online with---not only with clickworkers but also with a true general population sample. The third result questions whether recruiting from more representative samples is worth the efforts because it did not affect transferability of abstract results to the real world.

Limitations?

{{< pagebreak >}}


# A: Pure Replication {.appendix}

This section comments on two errors as well as a misconception I found in the original data.^[The data can be found in the supplementary materials they provide in their [online appendix](https://www.sciencedirect.com/science/article/pii/S0047272717300361#s0115).] Before I proceed to explain this in more detail I would like to say that the results of the original paper still hold after the error is fixed and that the authors responded kindly and quickly, showing an interest in solving the issue. In fact, some explanations in this section stem from input provided by the authors.

### Error 1: The Gini coefficient 

The Gini coefficient is wrongly computed in some periods for some group members. The authors found that this happened whenever two group members had exactly the same endowment because the program failed to rank these group members for further calculations.

```{r readGMTVagain}
GMTV <- read_dta(file="../../data/gaechteretal/GMTV-data.dta") %>% data.table()
noPunish10 <- GMTV[exp_num == 5 | exp_num == 8 | exp_num == 9]
# noPunish10 <- GMTV[longgame == 0 & punish == 0 & exp_num <= 10]
```

@tbl-gini-error illustrates this problem. It shows group 101 in period 5 and documents that the Gini coefficient differs among group members. According to the authors, the Gini coefficient should equal `GINI=``r GINI <- GMTV[exp_num == 1 & gr_id == 101 & per == 5, tokens %>% Gini() %>% round(digits = 3)]; GINI` for all subjects in the group. Instead, participant `112` and `113` who have an equal endowment deviate from that value. Importantly, the `DescTools::Gini()` function in the statistical software `R` does not yield this error, which is why I use that function for my calculations using both my as well as the original data.

```{r}
#| label: tbl-gini-error
#| tbl-cap: Subset of Data illustrating the Gini Coefficient's Error

GMTV[exp_num == 1 & gr_id == 101 & per == 5,
     .(exp_num,
       gr_id,
       per,
       subj_id,
       tokens,
       other1,
       other2,
       other3,
       gini = round(gini, digits = 3),
       GINI = GINI
       )] %>% 
  knitr::kable()
```

### Error 2: The share of endowments contributed

The original data provides a wrong measure of the share of endowments contributed (`mean`) because it relies on a lagged endowment (`gdp`). More precisely, the authors used the following STATA code for their calculations:

```
*tsset subj_id per
*gen mean=sum/l.gdp
```

@tbl-mean-error reports participant 111 in group 101 in experiment 1 over three periods. Both the `gdp` (that is, the sum of the groupâs endowments at the beginning of the period) as well as the `sum` (that is, the sum of the groupâs contributions) are group-level variables. 

```{r}
#| label: tbl-mean-error
#| tbl-cap: Subset of Data illustrating the Means's Error

GMTV[, MEAN := sum/gdp]

GMTV[exp_num == 1 & gr_id == 101 & (per == 4 | per == 5 | per == 6) & subj_id == 111,
     .(exp_num,
       gr_id,
       per,
       subj_id,
       gdp,
       sum,
       mean = round(mean, digits = 3),
       MEAN = round(MEAN, digits = 3)
       )] %>% 
  knitr::kable()
```

Calculating the share as `MEAN=sum/gdp` solves the problem and yields $\frac{18}{126}=0.143$ in period 5. I thus, used this proposed definition for all my calculations using both my as well as the original data.

### The misconception: Timing

The authors wrote a note stating that the Gini coefficient as well as the wealth in the paper always refer to the situation at the start of a period and that they clarify this because the paper (last paragraph at the bottom of page 5), says that wealth is defined as the endowment at the beginning of the following period. Furthermore, they write that this error came about as they switched between these two definitions during the course of revising the paper.

I argue that it makes more sense to calculate the variables as they state in the paper. More precisely, I think that the wealth at the _beginning_ of a period is less interesting than the wealth at the _end_ of a period for two reasons: First, there is no need for such a variable because it already exists (the endowment). Second, this definition yields a value that is determined by the design of the game but misses an important outcome at the end of the game. To illustrate this, note that the wealth would be defined as four times the initial endowment in period 1. Also note that the very last value would equal the wealth at the beginning of the last period and says nothing about the outcome of that period. Because the contributions often drop in the last period, this outcome is of particular interest (yet, not represented in the data).
Moreover, this definition of wealth yields more informative values to calculate the Gini coefficient for the same reasons: We know that the Gini coefficient is zero _before_ the participants made any decision by design. We do no know the inequality at the very end of the game---and the current definition does not tell us.

For these reasons, I define wealth and inequality measures as the outcomes of a period for all of my calculations using both my as well as the original data.^[Accordingly, the definition of `GINI` I provide in @tbl-gini-error is not the definition I used to calculate the current period's Gini coefficient but the previous period's Gini coefficient.]

{{< pagebreak >}}

<!--
# B: TITLE (exploratory) {.appendix}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec suscipit eleifend felis et faucibus. Curabitur nisl risus, tincidunt ut hendrerit sed, sollicitudin eget tortor. Curabitur tortor elit, finibus a commodo eget, auctor vel dui. Ut convallis, felis in commodo bibendum, ligula neque hendrerit eros, eu eleifend libero ligula pretium dui. Nullam blandit a tellus vitae suscipit. Donec ac justo vehicula elit tincidunt mollis nec ac lectus. Nunc dignissim sem eu felis consequat, id euismod ante imperdiet. Mauris quis ex ante.

Nulla elementum laoreet libero nec consequat. Donec blandit, tellus id feugiat viverra, lectus tellus sollicitudin ipsum, ut venenatis nunc mauris ut arcu. Donec mi eros, ullamcorper aliquet varius non, cursus vel lorem. Integer feugiat dui sapien, ac porta lacus porta sit amet. Aliquam interdum dictum tempor. Quisque bibendum feugiat scelerisque. Sed eu purus eu est ullamcorper bibendum.

Donec efficitur magna malesuada tortor feugiat tristique. Sed porttitor, quam at vulputate placerat, elit magna posuere est, a eleifend lacus magna ac nunc. Quisque sit amet arcu mattis, malesuada mauris et, aliquam libero. Fusce viverra sed dolor a ullamcorper. Aliquam in ligula neque. In maximus nisl id ante vehicula, et euismod ante feugiat. Suspendisse et dui lectus. Ut euismod ultrices mi eu lacinia.

```{r firstRoundVizAppendix}

d1 <- ggplot(data = R1[treatment == "replication"],
       mapping = aes(x = ownContribution, fill = student, lty = student)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 20),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 0.1),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("General Population Sample", "Student Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = R1[treatment == "replication", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 2) +
  geom_vline(xintercept = R1[treatment == "noPunish10", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 1) +
    labs(title = "", 
       y = "", x = "Initially Contributed Tokens") +
  layout +
  theme(legend.position = "top")
```


```{r stockDistributionVizAppendix}

populationWealth <- main[treatment == "replication" & student == FALSE & round == 10,
                         mean(stock)]
studentWealth    <- main[treatment == "replication" & student == TRUE & round == 10,
                         mean(stock)]

d2 <- ggplot(data = main[treatment == "replication"],
       mapping = aes(x = stock, fill = student, lty = student)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("General Population Sample", "Student Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = studentWealth,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = populationWealth,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Stock") +
  layout +
  theme(legend.position = "top")
```


```{r giniDistributionVizAppendix}

populationGini    <- main[treatment == "replication" & student == FALSE & round == 10,
                        mean(gini)] %>% round(digits = 2)
studentGini <- main[treatment == "replication" & student == TRUE & round == 10,
                        mean(gini)] %>% round(digits = 2)

d3 <- ggplot(data = main[treatment == "replication"],
       mapping = aes(x = gini, fill = student, lty = student)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("General Population Sample", "Student Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = studentGini,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = populationGini,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Gini Coefficient") +
  layout +
  theme(legend.position = "top")
```

```{r assembleAppendixPlots}
#| fig-cap: Kernel Density Plots
#| label: fig-multiple-distributions

patchwork <- d1 + d2 + d3 + plot_layout(guides = "collect") &  theme(legend.position = "top") 
patchwork + plot_annotation(caption = "Vertical lines indicate means (dashed line = student sample).")
```

```{r plotContributionsAppendix}
#| include: false

SUM <- main[treatment == "replication",
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "student"),
            .SDcols = "contribution"]

SUM[,
    sum := round(contribution)]

SUM[,
    contribution := contribution/4]

upperLimit <- SUM$contribution %>% max() %>% round() + 5

p1 <- ggplot(data = SUM, 
             aes(x = round, y = contribution, fill = student, color = student, lty = student)) +
  layout +
  theme(legend.position="top") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Average Amount of Tokens contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) + 
  guides(fill = "none") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

rm(list = c("SUM"))
```


```{r plotShareOfContributionsAppendix}
#| fig.cap: "The average amount of tokens contributed over time in treatments."
#| include: false

SHARE <- main[treatment == "replication",
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "student"),
            .SDcols = "share"]

# SHARE <- main[,
#             .(share = sum(contribution)/sum(endowment)),
#             by = c("round", "treatment")]

upperLimit <- 0.75

p2 <- ggplot(data = SHARE, 
             aes(x = round, y = share, fill = student, color = student, lty = student)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Share of Current Endowment contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  guides(fill = "none")

p1 + p2 + plot_layout(guides = "collect") & 
  theme(legend.position = "top")

# rm(list = c("p1", "p2"))
```


```{r plotStockAppendix}
#| include: false

# data
STOCK <- main[treatment == "replication",
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "student"),
              .SDcols = "stock"]


# annotation
maxPath <- data.table(x = 1:10)
maxPath[, y := 80*1.5^x]
maxPath[, student := "replication"]
maxPath[, groupID := 42]

# plot
p1 <- ggplot(data = STOCK, 
             aes(x = round, y = stock, fill = student, color = student, lty = student)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 4700), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  geom_point(mapping = aes(x = 10, y = 4613), col = "#b5b5b5") +
  annotate("text", x = 9, y = 4630, size = 3,
           label = "max", col = "#b5b5b5") +
  geom_line(data = maxPath, mapping = aes(x = x, y = y), col = "#b5b5b5", lty = 2) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

```{r growthHeterogeneityVizAppendix}
#| fig.cap: Average wealth over time across samples.
#| include: false

# plot
p2 <- ggplot(data = main[treatment == "replication"],
       mapping = aes(x = round, y = stock, color = student, by = groupID)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 2000),
                       expand = c(0, NA)) +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  labs(y = "", x = "Period") +
  layout +
  guides(col = "none")


# assembling
p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "top")
```



```{r}
#| fig.cap: Average Gini coefficient (within groups) over time across samples
#| include: false
 
tmp <- main[treatment == "replication",
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "student"),
            .SDcols = "gini"]

GINI <- rbind(tmp, list(0, TRUE, 0), list(0, FALSE, 0))

ggplot(data = GINI, 
             aes(x = round, y = gini, fill = student, color = student, lty = student)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 0:10) +
  scale_y_continuous(limits = c(0, 0.25), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("General Population Sample", "Student Sample")) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```



# C: Growth _and_ Inequality (exploratory) {.appendix}

In contrast to GMTV, I did not ask how rich and poor groups differ. Instead, I was wondering, whether equal groups are wealthier. More precisely, does the Gini coefficient correlate with growth and wealth creation? To answer that question, @fig-stock-by-gini applies a median split showing equal and unequal groups' wealth over time.

```{r plotStockByGini}
#| fig-cap: Average wealth over time across treatments.
#| label: fig-stock-by-gini

# create equality indicator (just as highgdp in original data)
median <- main[round == 10,
               median(gini)]

equalGroups <- main[round == 10 & gini > median,
                    unique(groupID)] 

unequalGroups <- main[round == 10 & gini < median,
                      unique(groupID)]

main[groupID %in% equalGroups,
     equal := TRUE]

main[groupID %in% unequalGroups,
     equal := FALSE]

STOCKe <- main[equal == TRUE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

STOCKu <- main[equal == FALSE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

upperLimit <- 700


p1 <- ggplot(data = STOCKe, 
             aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  # geom_vline(xintercept = 10, alpha = 0.66) +
  geom_line() +
  geom_point() +
  guides(lty = "none", fill = "none") +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Wealth (Equality)", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample"))

p2 <- ggplot(data = STOCKu, 
             aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  # geom_vline(xintercept = 10, alpha = 0.66) +
  geom_line() +
  geom_point() +
  guides(lty = "none", fill = "none") +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Wealth (Inequality)", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample"))

p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "bottom")

rm(list = c("STOCKe", "STOCKu", "p1", "p2"))
```
-->