---
title: "Growth and inequality in public good provision: an extended replication"
description: |
  Report and analysis protocoll.
author:
  - name: Hauke Roggenkamp 
    url: https://www.ibt.unisg.ch/team/hauke-roggenkamp/
    affiliation: Helmut Schmidt University
    affiliation_url: https://www.hsu-hh.de/en/
  - affiliation: Institute for Behavioral Science and Technology
    affiliation_url: https://www.ibt.unisg.ch/
date: "`r Sys.Date()`"
bibliography: ../biblio.bib
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    toc_float: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages}
# install this package once, it'll then install and load all remaining packages
# install.packages("pacman")

pacman::p_load(magrittr, data.table, stringr, lubridate, glue, DescTools, haven,
               stargazer, ggplot2, patchwork, MetBrewer, Rmisc)
```

```{r design}
# ggplot layout
layout <- theme(panel.background = element_rect(fill = "transparent", color = NA),
                plot.background = element_rect(fill = "transparent", color = NA),
                panel.grid = element_blank(),
                panel.grid.major.y = element_blank(),
                legend.key = element_rect(fill = "transparent"),
                axis.line = element_line(size = 0.25),
                axis.ticks = element_line(size = 0.25),
                plot.caption = element_text(colour = "#555555"),
                legend.title = element_blank()
)

# color
colors <- met.brewer(name="Tam",n=7,type="discrete")
cPrimary = "#ffd353"
cSecondary = "#9f2d55"
cInfo = "#34168"
```

<!-- OUR DATA -->


```{r readAllData}
# identify files of all sessions
files <- list.files(path = "../../data/replication/",
                       recursive = TRUE,
                       full.names = TRUE)

# load all of them into a list
csvs <- list()
for(i in files){
  name <- str_extract(string = i, pattern = "2021.*")
  temp <- read.csv(i, stringsAsFactors = FALSE) %>% data.table()
  csvs[[name]] <- temp
}

# bind list into single data table
full <- rbindlist(l = csvs, use.names = TRUE) %>% data.table()

# ignore observations who have not made it to the first contribution
DT <- full[participant._index_in_pages > 2 & 
             participant.time_started != "" & 
             participant.label != "" &
             !(is.na(dPGG.1.player.contribution))]

# tidy up
rm(list = c("files", "csvs", "i", "name", "temp"))
```

```{r metaData}
# aggregate each session's meta data
meta_long <- full[participant.time_started != "" & 
                    participant.label != "",
                  .(session.code,
                    participant.label,
                    participant.time_started,
                    dPGG.1.player.contribution)]

# reformat time stamp in new variables
meta_long[, `:=`(date = participant.time_started %>% ymd_hms() %>% date(),
                 hour = participant.time_started %>% ymd_hms() %>% hour())]

# aggregate and count 
meta <- meta_long[,
                  .(date = date %>% unique(),
                    time = hour %>% unique() %>% paste("00", sep = ":"),
                    showups = participant.time_started %>% length(),
                    dropouts = dPGG.1.player.contribution %>% is.na() %>% sum(na.rm = FALSE)),
                  by = session.code]

# get number of participants and observations (groups of 4)
meta[,
     `:=`(participants = showups - dropouts,
          observations = (showups - dropouts)/4)]

# save data
fileName <- "replication2021"
save(meta, file = paste0("../../data/processed/rda/", fileName, "_meta", ".rda"))
write.csv(meta, file = paste0("../../data/processed/csv/", fileName, "_meta", ".csv"))

rm(list = c("full", "meta_long"))
```

```{r identifyStudents}
# we had to invite students as the general population sample was exhausted, unfortunately. let's identify them:
invitedStudents <- read.table(file = "../../data/sample/invited_students.txt", col.names = c("participant.label")) %>%
  data.table()
# note that these students participated in the ultimate session
```

```{r focusOnFirstRound}
# create data table 
replicationFirstRound <- DT[,
                 .(participant.code,
                   treatment = "replication",
                   session.code,
                   groupID = paste(session.code, dPGG.1.group.id_in_subsession, sep = "_"),
                   othersContribution = dPGG.1.group.total_contribution - dPGG.1.player.contribution,
                   ownContribution = dPGG.1.player.contribution,
                   trust = Outro.1.player.PQ11,
                   comprehension = dPGG.10.player.comprehension)]

# save data
save(replicationFirstRound, file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(replicationFirstRound, file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))
```

```{r subsetData}

# identify most relevant variables
mRegex <- "participant\\.code$|session\\.code$|dPGG\\.1\\.group\\.id_in_subsession|^dPGG.1.player.id_in_group$|\\.1\\.player.belief|endowment|contribution|stock|gain$|bot_active|10.player.comprehension|10.player.donation"
mainVariables <- str_subset(string = names(DT), pattern = mRegex)

# prune all other variables
subset <- DT[, ..mainVariables]

# tidy up
rm(list = c("mRegex", "mainVariables"))
```

```{r addNewVariables}
# refactor groupID such that it eventually contains treatment-info
subset[, groupID := paste(session.code, 
                          dPGG.1.group.id_in_subsession,
                          sep = "_")]

# add share as contribution/endowment
for(round in 1:10){
  contribution <- glue("dPGG.{round}.player.contribution")
  endowment <- glue("dPGG.{round}.player.endowment")
  subset[, glue("dPGG.{round}.player.share") := subset[[contribution]]/subset[[endowment]] ]
}

# add treatment variable
subset[,
       treatment := "replication"]
```

```{r calculateAggregates}
# we need to aggregate some outcomes on a groupID level (per sessions per treatment)
# we'll do so using a loop
# this will yield a list of data tables that will be merged to a list eventually.
cluster <- c("treatment", "session.code", "groupID")
outcomes <- c("contribution", "endowment", "stock", "gain", "bot_active")

DTs <- list()
for(outcome in outcomes){
  if(outcome == "bot_active"){
    var = names(subset) %>% str_subset(pattern = glue("group\\.{outcome}$"))
  } else {
    var = names(subset) %>% str_subset(pattern = glue("player\\.{outcome}$"))
  }


  # calculate either averages or the sum per round per group
  aggregates = subset[, lapply(.SD, sum, na.rm=TRUE), 
                      by = cluster, 
                      .SDcols=var]
  
  # transform from wide to long
  meltedAggregates <- melt(aggregates, id.vars = cluster, measure.vars = var)
  DTname <- glue("{str_to_title(outcome)}")
  DTs[[DTname]] <- meltedAggregates
  rm(list = c("DTname", "meltedAggregates", "aggregates", "var", "outcome"))
}
```

```{r renameVariables}
# each data table in that list has the same column names.
# the outcomes are all named "value", for instance.
# now, we"ll infer the round number (contained in the variable name)
for(i in 1:length(outcomes)){
  DTs[[i]] <- DTs[[i]][,
                       .(treatment,
                         session.code,
                         groupID,
                         round = str_replace_all(string = variable,
                                                 pattern = "\\D", 
                                                 replacement="") %>% as.integer(),
                         value # to be renamed afterwards
                       )
  ]
  # rename "value" to outcome variable
  setnames(DTs[[i]], old = "value", new = outcomes[i])
}

# tidy up
rm("i")
```


```{r calculateGini}
# repeat everything for the gini
var = names(subset) %>% str_subset(pattern = "player\\.stock$")
gini = subset[,
              lapply(.SD, Gini, na.rm=TRUE), 
                by = cluster, 
                .SDcols=var
              ]
Gini <- melt(gini, id.vars = cluster, measure.vars = var)

DTs[["Gini"]] <- Gini[,
                    .(treatment,
                      session.code,
                      groupID,
                      round = str_replace_all(string = variable,
                                              pattern = "\\D", 
                                              replacement="") %>% as.integer(),
                      gini = value
                       )]
rm(list = c("var", "gini", "Gini"))

# note that GMTV used start of period earnnings, i.e. endowments. We use end of period earnings, i.e. stock.
# this adjustment has been considered in our processing of GMTVs data.
```


```{r mergerEverything4FinalData}
# now merge every table in the list to one final data table called "replication"
replication <- Reduce(function(...) merge(..., by=c(cluster, "round"), all = TRUE), DTs)

# tidy up
rm(list = c("cluster", "contribution", "endowment", "round", "DTs", "outcomes"))
```

```{r calculateShare}
# define the share (one of the main outcome variables) as 
# the sum of contributions devided by the sum of endowments
replication[, share := contribution/endowment]
```

```{r defineRichGroups}
# use the median to differentiate between poor and rich groups (as GMTV did)
median <- replication[round == 10,
                      median(stock)]

# find the groups that end up rich or poor
richGroups <- replication[round == 10 & stock > median,
                          unique(groupID)] 

poorGroups <- replication[round == 10 & stock < median,
                          unique(groupID)]

# mark these groups for each period
replication[groupID %in% richGroups,
            rich := TRUE]

replication[groupID %in% poorGroups,
            rich := FALSE]

# tidy up
rm(list = c("median", "poorGroups", "richGroups"))

```

```{r misc}
# flag observations where at least one participant did not understand the game
noComp <- subset[dPGG.10.player.comprehension == 0,
                 groupID] %>% unique()
replication[,
     noComprehension := 0]
replication[groupID %in% noComp,
     noComprehension := 1]


# drop observations (i.e. groups in rounds) with dropouts (bot_active == 1) and
# where round > 10
replication <- replication[bot_active == 0 & round <= 10]

# tidy up
rm("noComp")
```

```{r saveData}
save(replication, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(replication, file = paste0("../../data/processed/csv/", fileName, ".csv"))
```

```{r createCovariates}

# so far, I neglected many variables in what happened before. The following few lines
# deal with some of these variables in a similar procedure


# add variables
DT[, treatment := "replication"]
DT[, groupID := paste(session.code, dPGG.1.group.id_in_subsession, 
                    sep = "_")]

# subset
cRegex <- "participant.code|session.code|treatment|groupID|Outro.1.player|10.player.donation|10.player.stock|switching_row|inconsistent"
covariates <- str_subset(string = names(DT),
                            pattern = cRegex)
CT <- DT[, ..covariates]

# rename
names(CT) <- names(CT) %>% 
  str_replace_all(pattern =".*player\\.",
                  replacement = "") %>%
  str_to_lower()
names(CT)[names(CT) == "groupid"] <- "groupID"

# refactor
CT[, donation := donation/20] # exchange rate tokens to real world currency 1/20
CT[donation %>% is.na, donation := 0]
CT[, gender := ifelse(test = gender == "female",
                      yes  = 1,
                      no   = 0)]
CT[, inconsistent := as.logical(inconsistent)]

# reassign payoff (with stock of the last period)
CT[, payoff := stock]
CT[, stock := NULL]

# write data
replicationCovariates <- CT
save(replicationCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(replicationCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("CT", "cRegex", "covariates"))
```

```{r readTime}
# as before, we need to read a list of files (measuring the time spent per page)

# identify files
files <- list.files(path = "../../data/pageTimes/",
                       recursive = TRUE,
                       full.names = TRUE)

# loop
csvs <- list()
for(i in files){
  name <- str_extract(string = i,
                      pattern = "2021.*")
  
  temp <- read.csv(i, 
                   stringsAsFactors = FALSE) %>%
    data.table()
  
  csvs[[name]] <- temp
}

# bind files list to data.table
timeSpent <- rbindlist(l = csvs,
                       use.names = TRUE) %>%
  data.table()

# set oder
setorder(timeSpent, session_code, participant_code, epoch_time)

# tidy up
rm(list = c("files", "i", "name", "temp"))
```


```{r calcDuration}
# shift rows to calculate this duration (per page) as the distance between
# the current and the next time stamp
timeSpent[,
          lag := shift(epoch_time, fill = NA, type = "lag"),
          by = c("session_code", "participant_code")]

timeSpent[,
          duration := epoch_time - lag,
          by = c("session_code", "participant_code")]

# calculate overall time spent as the difference between the min and max time stamp
timeSpent[,
          completion := epoch_time %>% max() - epoch_time %>% min(),
          by = c("session_code", "participant_code")]

# create new data table with selected columns
duration <- timeSpent[participant_code %in% DT$participant.code,
                      .(
                        session_code,
                        participant_code,
                        app_name,
                        page_name,
                        page_index,
                        page_submission = epoch_time,
                        time_spent = duration,
                        completion_time = completion
                      )]

# save data
save(duration, file = paste0("../../data/processed/rda/", fileName, "_timeSpent", ".rda"))
write.csv(duration, file = paste0("../../data/processed/csv/", fileName, "_timeSpent", ".csv"))

# tidy up
rm(list = c("timeSpent", "csvs", "DT"))
```




<!-- ORIGINAL (GMTV) DATA -->

```{r readGMTV}
# having done all that, we need to prepare the original data and bind it to ours
# I'll thus, start by reading the orginal data

DT <- read_dta(file="../../data/gaechteretal/GMTV-data.dta") %>% data.table()
CT <- read_dta(file="../../data/gaechteretal/GMTV-questionnaire-data.dta") %>% data.table()
```

```{r subsetGMTV}
# because we only replicate one of their treatments, I subset accordingly
# noPunish10 <- DT[exp_num == 5 | exp_num == 8 | exp_num == 9]
noPunish10 <- DT[longgame == 0 & punish == 0 & exp_num <= 10]
```

```{r newVariablsGMTV}
# calculate the share of endowments contributed
noPunish10[,
           share := sum/gdp,
           by = .(subj_id, per)]

# calculate gini based on endowments (as GMTV did)
noPunish10[,
           gini2 := Gini(c(tokens, other1, other2, other3)),
           by = .(subj_id, per)]

# calculate gini based on stock (i.e. end of period earnings)
noPunish10[, stock0 := (tokens - putin + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock1 := (other1 - pu1 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock2 := (other2 - pu2 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock3 := (other3 - pu3 + 1.5*sum/4) %>% ceiling()]
noPunish10[, 
           gini3 := Gini(c(stock0, stock1, stock2, stock3)),
           by = .(subj_id, per)]
```

```{r createGMTVtable}
# select columns we are interested in and rename a few to match our data

GMTV <- noPunish10[order(gr_id, per),
           .(treatment = "noPunish10", 
             session.code = exp_num,
             groupID = gr_id, 
             round = per,
             contribution = sum,
             endowment = gdp,
             share,
             stock = (gdp + ceiling(sum/4*1.5)*4-sum),
             gain = (ceiling(sum/4*1.5)*4-sum),
             gini = gini3,
             bot_active = 0,
             noComprehension = NA)] %>% unique()

# tidy up
rm(list = c("noPunish10"))
```

```{r richPoorGMTV}
 # create rich indicator (just as highgdp in original data)
median <- GMTV[round == 10,
                   median(stock)]

richGroups <- GMTV[round == 10 & stock > median,
                       unique(groupID)] 

poorGroups <- GMTV[round == 10 & stock < median,
                       unique(groupID)]

GMTV[groupID %in% richGroups,
         rich := TRUE]

GMTV[groupID %in% poorGroups,
         rich := FALSE]

# tidy up
rm(list = c("median", "richGroups", "poorGroups", "fileName"))
```

```{r saveGMTV}
fileName <- "GMTV2017"
# save data
save(GMTV, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(GMTV, file = paste0("../../data/processed/rda/", fileName, ".csv"))
```


```{r calcSwitchingPoint}
# GMTV gathered some risk data that I'll process in a separate DT and merge it
# with other covariates later on

GMTVRisk <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
               .(participant.code = subj_id,
                 e10, e20, e30, e40, e50, e60, e70,
                 inconsistent = ifelse(test = e60 < e70 | e50 < e60 | e40 < e50 | e30 < e40 | e20 < e30 | e10 < e20,
                                       yes  = TRUE,
                                       no   = FALSE)
                 )]

GMTVRisk[,
         switching_row := ifelse(test = inconsistent == TRUE,
                                yes  = NA,
                                no   = e10 + e20 + e30 + e40 + e50 + e60 + e70 + 1 + 2) # because GMTV used a 7-point-likert scale
         ]
```

```{r subsetCovariates}
# subset covariate data and rename+refactor some variables
temp <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
                         .(treatment = "noPunish10",
                           session.code = exp_num,
                           groupID = gr_id,
                           participant.code = subj_id,
                           gender,
                           age = Sys.Date() %>% lubridate::year() - age,
                           pq01 = q1,
                           pq02 = q2,
                           pq03 = q3,
                           pq04 = q4,
                           pq05 = q5,
                           pq06 = q6,
                           pq07 = q7,
                           pq08 = q8,
                           pq09 = q9,
                           pq10 = q10,
                           pq11 = q11,
                           pq12 = q12,
                           pq13 = q13,
                           pq14 = q14,
                           donation = don
                           )]

temp[donation %>% is.na, donation := 0]

```

```{r mergeAndWrite}

GMTVCovariates <- data.table::merge.data.table(x = temp,
                                               y = GMTVRisk[, .(participant.code, inconsistent, switching_row)],
                                               by = c("participant.code"))

# write data
save(GMTVCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(GMTVCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("temp", "GMTVRisk"))
```

```{r firstRound}
noPunish <- DT[exp_num == 1 | exp_num == 3 | # noPunish15
                 exp_num == 5 | exp_num == 8 | exp_num == 9] # noPunish10

temp <- noPunish[per == 1]
temp <- temp[,
             treatment := "noPunish10"]
temp <- temp[exp_num == 1 | exp_num == 3,
             treatment := "noPunish15"]

GMTVFirstRound <- temp[,
                           .(participant.code = subj_id,
                             treatment,
                             session.code = exp_num,
                             groupID = gr_id,
                             othersContribution = sumputin - putin,
                             ownContribution = putin,
                             trust = NA,
                             comprehension = NA)]

# save data
save(GMTVFirstRound, 
     file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(GMTVFirstRound, 
          file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))

rm(list = c("temp", "fileName"))
```

<!-- MERGE & TUNE DATA -->

```{r rbindOursAndGMTV, eval = TRUE}
main <- rbindlist(list(replication, GMTV), 
                  use.names = TRUE)

R1   <- rbindlist(list(replicationFirstRound, GMTVFirstRound), 
                  use.names = TRUE)

covs <- rbindlist(list(replicationCovariates, GMTVCovariates), 
                  use.names = TRUE,
                  fill = TRUE)

rm(list = c("CT", "DT", "GMTV", "GMTVCovariates", "GMTVFirstRound", 
            "invitedStudents", "noPunish", "replication", "replicationCovariates",
            "replicationFirstRound", "subset"))
```

```{r studentSample}

# Two of these sessions were special: The first (`r meta[1, session.code]`) as well as the last one (`r meta[, session.code %>% tail(n=1)]`). The first session suffered technical problems such that the risk elicitation task was omitted. The last session (almost exclusively) relied on a student sample as our non-student sample was exhausted after the first three sessions. As a consequence, the last session was conducted with 59 students while all others were conducted without any students. I'll therefore create a boolean `student` variable. Becaue all of the original experiments were conducted with students, `student` also equals 1 in the "noPunish10" treatments

main[, student := FALSE]
main[session.code == "d6jrsxnr" | treatment == "noPunish10",
     student := TRUE]

R1[, student := FALSE]
R1[session.code == "d6jrsxnr" | treatment == "noPunish10",
   student := TRUE]
```



<!-- PAPER STARTS HERE -->

<!-- 
# Key Takeaways

- We **[replicate](#gmtv-replication)** a public goods experiment with dynamic inter-dependencies and find similar results as @GMTV2017. 
    + Absolute contributions increase over time.
    + Just as in _static_ public goods experiments, the share of endowments contributed decreases over time.
    + The richest groups earn fifteen times more than the poorest groups.
    + While there clearly is growth, groups do not realize the maximal potential efficiency and earn just over `r main[treatment == "replication" & round == 10, mean(stock)/(80*1.5^10)*100] %>% round(digits=1)`% of what is possible.
- Varying the subject pool and including a voluntary climate action (VCA), we have a setup similar to @GKLS2020, which allows us to assess **[generalizability](#generalizability)**.^[Note that one can easily model the VCA as a dictator game, which relates to a literature summarized by @cartwright2022.]
- Studying a general population sample online, synchronously and over the course of multiple periods, we find that classic **[lab procedures](online-methodology)** can be applied outside of the lab too.
    + Even though we conducted the experiment online and remotely, dropouts (or "attrition") are no concern.
    + Relying on an inexperienced non-convenience sample that cannot interact with experimenters, `r trunc((R1[comprehension > 1] %>% NROW() / R1[!(is.na(comprehension))] %>% NROW())*100)`% of all participants stated that they understood the game.
    + Participants make relatively fast decisions which makes longer games feasible in the future.^[Taken together with inefficient growth and the lack of dropouts, additional rounds are fairly cheap.]

-->

***

**Hintergrund:** Ich habe diesen [post](https://groups.google.com/g/esa-announce/c/-_2OmbkdxEk) gesehen und überlege meine Replikation entsprechend [hier](https://www.sciencedirect.com/journal/journal-of-behavioral-and-experimental-economics/about/call-for-papers#transparency-reproducibility-and-generalizability-of-behavioral-economics-experiments) einzureichen. Ist das realistisch? Wenn ja, wie soll man die Story aufziehen? Soll man denen mal schreiben und fragen, ob noch Platz ist?

***

> "There are two possible articles you can write: (a) the article you planned to write when you designed your study or (b) the article that makes the most sense now that you have seen the results. They are rarely the same, and the correct answer is (b)." [@bem2021writing, p. 171]

# Introduction

Macroeconomists understand the world as dynamic where the timing matters: Today's savings decision affect lifetime consumption which is why it makes a difference whether you consume more today or tomorrow. Yet, experimental economists often design static games without dynamic inter-dependencies: participants face the same decisions multiple times and previous decisions do not affect current incentives of endowments.

To measure how cooperation affects growth and inequality, @GMTV2017 (to whom I often refer as GMTV in what follows) extended a standard public goods game with temporal inter-dependencies.^[As well as the option to punish group members.] Their results show that participants behave strikingly similar as in the static games (contributions are non-zero; the share of current endowments contributed declines over time; and the initial contribution is around half of their endowments) and illustrate how this affects growth and inequality.

Because environmental and climate action (or the absence of it) is characterized by pro-social behavior and inter-temporal dynamics (the more we hesitate to cut CO2 emissions, the more expensive it will be to reach a certain target in the future), we are interested in applying this setting in a behavioral and environmental economics context with a general population sample.

Due to COVID-19 and corresponding restrictions at that time, we decided to [replicate](#gmtv-replication) GMTV's no punishment 10 period game to evaluate, whether it is feasible to run a multi-period, synchronous group experiment with such a sample first.

As we were not able to recruit the expected number of subjects in the general population sample, we added student participants. Because of this and because the experiment also contains voluntary climate action (VCA), we have a setup similar to @GKLS2020, which allows us to assess [generalizability](#generalizability).

Hence, our paper makes three contributions: First, we run a pre-registered^[The replication project is registered in the AEA RCT Registry and the unique identifying number is: [AEARCTR-0007902](https://doi.org/10.1257/rct.7902-2.0) [@preregistration].] replication of one of GMTV's treatment arms and arrive at similar conclusions: Absolute contributions increase over time. Just as in _static_ public goods experiments, the share of endowments contributed decreases over time. The richest groups earn fifteen times more than the poorest groups. While there clearly is growth, groups do not realize the maximal potential efficiency and earn just over `r main[treatment == "replication" & round == 10, mean(stock)/(80*1.5^10)*100] %>% round(digits=1)`% of what is possible.

Second, like @AGM2018, we study a general population sample online, synchronously and over the course of multiple periods and we find that classic [lab procedures](online-methodology) can be applied outside of the lab too. Relying on an inexperienced sample that cannot interact with experimenters, `r trunc((R1[comprehension > 1] %>% NROW() / R1[!(is.na(comprehension))] %>% NROW())*100)`% of all participants stated that they understood the game. Participants make relatively fast decisions which makes longer games feasible in the future.^[Taken together with inefficient growth and the lack of dropouts, additional rounds are fairly cheap.] In addition, dropouts (or "attrition") are no concern. 

Third, we extend the literature on generalizability and find that:

The paper is organized along these lines: Chapter x ...

# Experimental Design and Procedures

This experiment replicated Gächter et al.'s NOPUNISH 10-round treatment arm as close as possible (given the remote circumstances). A demonstration of the experiment can be found [here](https://cliccs.herokuapp.com/).^[Click [here](https://github.com/Howquez/coopUncertainty) to visit the corresponding Github repository.]

All participants were recruited in by the University of Hamburg's  WISO Research Lab using HROOT [@hroot].

# Pre-registered GMTV Replication

## First Round

```{r prepFirstSummary}
replication <- R1[treatment == "replication", ownContribution]
GMTV    <- R1[treatment == "noPunish10", ownContribution]


rows <- sapply(X = list(replication, GMTV), FUN = NROW) %>% max()
temp <- data.frame(replication = c(replication, rep(NA, rows - NROW(replication))),
                   GMTV = c(GMTV, rep(NA, rows - NROW(GMTV))))

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)
```

First, we ask whether the samples differ with respect to their initial contributions to the public good. Is our replication sample more pro-social than the original sample? As Figure 1 reveals, they are not. The distributions look fairly similar and both samples contributed 10 tokens, that is, 50% of their endowments on average (median and mean).^[The two-sided rank sum test (comparing differences between data sources) yields a p-Value of `r rs1` for the mean contribution in first round of the game.] As such, the individuals' behavior in both samples is not only comparable to each other but also to initial contributions in the standard game with partner matching.^[See Figure 3B in @fehrgaechter2000, for instance.] Because participants played a dynamic game, we are particularly interested in the subsequent periods. Do the two groups remain similar?


```{r firstRoundViz}
ggplot(data = R1[treatment != "noPunish15"],
       mapping = aes(x = ownContribution, fill = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 20),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 0.1),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Replication Sample")) +
  labs(title = "Individual contributions to the dynamic public good in the first period", 
       y = "", x = "Contributrions (Tokens)") +
  layout +
  theme(legend.position = "bottom")
```


```{r firstRoundSummary, results = "asis", warning = FALSE, fig.cap = "test", eval = FALSE}

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

stargazer(temp,
          summary.stat = c("mean", "median","sd", "max", "min", "n"),
          type = type, 
          flip = TRUE, 
          header=FALSE)
```

## Provision of the public good


We proceed by further discussing contributions over time on a group level. The left panel in Figure 1 shows the average amount of tokens groups contributed over time. Contributions are clearly non-zero and are increasing over the course of the first eight periods. While contributions flatten in the replication, the GMTV data exhibit a notable drop in the last round. 


```{r plotContributions}

SUM <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "contribution"]

SUM[,
    sum := round(contribution)]

upperLimit <- SUM$contribution %>% max() %>% round() + 10

p1 <- ggplot(data = SUM, 
             aes(x = round, y = contribution, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Average Amount of Tokens contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary)) + 
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

rm(list = c("SUM"))
```


```{r plotShareOfContributions, fig.cap = "The average amount of tokens contributed over time in treatments."}

SHARE <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "share"]

# SHARE <- main[,
#             .(share = sum(contribution)/sum(endowment)),
#             by = c("round", "treatment")]

upperLimit <- 0.75

p2 <- ggplot(data = SHARE, 
             aes(x = round, y = share, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Share of Current Endowment contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary))

p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "bottom")

rm(list = c("p1", "p2"))
```

Note that increasing contributions over time imply that groups have increasing endowments over time. Hence, increasing contributions do not necessarily imply that groups contribute increasing shares of their endowments. The right panel in Figure 1 shows the share of overall endowments contributed over time. In the original data, groups contribute around `r SHARE[round == 1 & treatment == "noPunish10", round(share*100)]`% of their endowment in round 1. This amount steadily decreases to `r SHARE[round == 10 & treatment == "noPunish10", round(share*100)]`%. The replication data exhibit a similar pattern with shares ranging from `r SHARE[round == 1 & treatment == "replication", round(share*100)]`% to `r SHARE[round == 10 & treatment == "replication", round(share*100)]`%. `r rm("SHARE")`

Taken together, our replication partly mirrors the results of GMTV. Absolute contributions tended to be higher in @GMTV2017 but end up at around the same level as in the replication due to a stark decline in contributions in the last round. In terms of shares contributed both data sources exhibit a similar pattern: they decline and do not stabilize. Even though the share of current endowments contributed in the last round is quite similar, the share declined a little faster in our data, however.

## Wealth Creation

Possibly of more interest are the implications contributions have for wealth generation and growth. To measure growth, we define a variable called _stock_ which sums the endowments of all participants in a given group at the end of the round (that is, after the contributions have been made, multiplied and redistributed). @GMTV2017 refer to that variable as "wealth" so we will do the same in what follows. Before the start of round 1, wealth will be 80 in all groups by construction. The maximal wealth that can be reached in round 10 (if each group member contributes their entire endowment in each round) is approximately `r (80*1.5^10) %>% trunc()` tokens or `r (80*1.5^10 /20) %>% trunc()` Euro per group. 

Table 3 shows some summary statistics regarding wealth at the end of the last period.  While there is clearly growth, groups do not realize the maximal potential efficiency as the replication groups reach on average a level of `r main[treatment == "replication" & round == 10, mean(stock)] %>% trunc()` tokens out of `r (80*1.5^10) %>% trunc()` maximally possible or `r main[treatment == "replication" & round == 10, mean(stock)/(80*1.5^10)*100] %>% round(digits=1)`%. As in the original data, there is large heterogeneity with the richest group reaching `r main[treatment == "replication" & round == 10, max(stock)]` tokens whereas the poorest group ends up with `r main[treatment == "replication" & round == 10, min(stock)]` tokens.

```{r summary1, results = "asis", warning = FALSE, fig.cap = "test"}

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, stock]
GMTV    <- main[treatment == "noPunish10" & round == 10, stock]


rows <- sapply(X = list(replication, GMTV), FUN = NROW) %>% max()
temp <- data.frame(replication = c(replication, rep(NA, rows - NROW(replication))),
                   GMTV = c(GMTV, rep(NA, rows - NROW(GMTV))))

stargazer(temp,
          summary.stat = c("mean", "median","sd", "max", "min", "n"),
          type = type, 
          flip = TRUE, 
          header=FALSE)

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)

```


We thus, observe growth even for the poorest group and spot heterogeneity even within data sources.

Figure 2 shows the dynamics of wealth over time. The left panel focuses on all groups, the upper right panel on those with above median wealth after round 10 (“successful” groups) and the lower left panel on those with below median wealth after round 10 (“unsuccessful” groups).

```{r plotStock, fig.cap = "Average wealth over time across treatments."}
STOCK <- main[,
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "treatment"),
              .SDcols = "stock"]

STOCKr <- main[rich == TRUE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

STOCKp <- main[rich == FALSE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

upperLimit <- STOCKr$stock %>% max() %>% round() + 20

p1 <- ggplot(data = STOCK, 
       aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
          layout +
          theme(legend.position="bottom") +
          # geom_vline(xintercept = 10, alpha = 0.66) +
          geom_line(show.legend=FALSE) +
          geom_point() +
          # annotate("segment", x = 1, xend = 10, 
          #          y = STOCK[round == 1 & treatment == "replication", stock],
          #          yend = STOCK[round == 10 & treatment == "replication", stock],
          #          colour = "black", lty = 2, alpha = 0.5) +
          # annotate("segment", x = 1, xend = 10, 
          #          y = STOCK[round == 1 & treatment == "noPunish10", stock],
          #          yend = STOCK[round == 10 & treatment == "noPunish10", stock],
          #          colour = "black", lty = 2, alpha = 0.5) +
          scale_x_continuous(name="",  breaks = 1:10) +
          scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
          labs(y = "Wealth") +
          scale_color_manual(values = colors) + 
          theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

p2 <- ggplot(data = STOCKr, 
       aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
          layout +
          theme(legend.position="bottom") +
          # geom_vline(xintercept = 10, alpha = 0.66) +
          geom_line(show.legend=FALSE) +
          geom_point() +
          scale_x_continuous(name="",  breaks = 1:10) +
          scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
          labs(y = "Wealth (Rich)") +
          scale_color_manual(values = colors)

p3 <- ggplot(data = STOCKp, 
       aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
          layout +
          theme(legend.position="bottom") +
          # geom_vline(xintercept = 10, alpha = 0.66) +
          geom_line(show.legend=FALSE) +
          geom_point() +
          scale_x_continuous(name="",  breaks = 1:10) +
          scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
          labs(y = "Wealth (Poor)") +
          scale_color_manual(values = colors)

(p1 | (p2 / p3)) + plot_layout(guides = "collect") & theme(legend.position = "bottom")

rm(list = c("STOCK", "STOCKr", "STOCKp", "p1", "p2", "p3"))
```

The Figure illustrates that (in both data sources) growth was continuous and surprisingly linear, given the exponential character of the game's design.


To sum up, our groups also tend to be poorer, and median wealth is higher in GMTV. The difference in mean ranks is not significant according to a two-sided ranksum test^[The two-sided rank sum test (comparing differences between data sources) yields a p-Value of `r rs1` for the mean wealth after the last round of the game.], however. 

To further assess the statistical significance of differences in means, we run OLS regressions where we regress wealth on a treatment dummy for _Replication_ (Table 3). These regressions show that differences in means are only significant for below median groups. This implies that the poor groups in our data are even less wealthy than GMTV's poor groups.

```{r tableStock, results = "asis", warning = FALSE}

# create subsets
main_all  <- main[round == 10]
main_poor <- main[round == 10 & rich == FALSE]
main_rich <- main[round == 10 & rich == TRUE]

# create table
if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

m1 <- lm(formula = stock ~ treatment, data = main_all)
m2 <- lm(formula = stock ~ treatment, data = main_poor)
m3 <- lm(formula = stock ~ treatment, data = main_rich)

stargazer(m1, m2, m3,

          column.labels = c("All", "Below median", "Above median"),
          model.numbers = FALSE,
          dep.var.labels = "Wealth",
          header = FALSE,
          covariate.labels = c("Replication"),

          type = type, digits = 2, omit.stat = c("adj.rsq", "f"), df = FALSE
          )
```

As such, our results look familiar to GMTV's findings, where poor groups grew ever so slightly, while only rich groups experienced growth (that became exponential only after the 10th round).

## Inequality

In this subsection, we focus on the degree of inequality created endogenously in our setting. The smallest possible value the Gini coefficient takes is zero (if all four group members own one fourth of the wealth) and the largest possible value it takes is one (if one group member holds the entire wealth). Table 4 shows some summary statistics regarding the Gini coefficient.

The round 10 Gini coefficient ranges between `r main[treatment == "replication" & round == 10, gini %>% min()] %>% round(digits = 3)` and `r main[treatment == "replication" & round == 10, gini %>% max()] %>% round(digits = 3)` in our data with a median of `r main[treatment == "replication" & round == 10, gini %>% median()] %>% round(digits = 3)`.

```{r summary2, results = "asis", warning = FALSE}

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, gini]
GMTV    <- main[treatment == "noPunish10" & round == 10, gini]


rows <- sapply(X = list(replication, GMTV), FUN = NROW) %>% max()
temp <- data.frame(replication = c(replication, rep(NA, rows - NROW(replication))),
                   GMTV = c(GMTV, rep(NA, rows - NROW(GMTV))))

stargazer(temp,
          summary.stat = c("mean", "median","sd", "max", "min", "n"),
          type = type, 
          flip = TRUE, 
          header=FALSE)

rs2 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)

```

The two-sided rank sum test yields a p-Value of `r rs2` for the mean ```gini``` during the last round of the game.

Figure 3 illustrates the dynamics of the Gini coefficient (at the end of each round) over time and shows that inequality increases slightly.

```{r plotInequality, fig.cap = "Average Gini coefficient over time across treatments."}
GINI <- main[,
             lapply(.SD, mean, na.rm = TRUE),
             by = c("round", "treatment"),
             .SDcols = "gini"]

GINIr <- main[rich == TRUE,
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "treatment"),
              .SDcols = "gini"]

GINIp <- main[rich == FALSE,
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "treatment"),
              .SDcols = "gini"]

upperLimit <- GINI$gini %>% max() %>% round(digits = 1) + 0.15

p1 <- ggplot(data = GINI, 
       aes(x = round, y = gini, fill = treatment, color = treatment, lty = treatment)) +
          layout +
          theme(legend.position="bottom") +
          # geom_vline(xintercept = 10, alpha = 0.66) +
          geom_line(show.legend=FALSE) +
          geom_point() +
          scale_x_continuous(name="",  breaks = 1:10) +
          scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
          labs(y = "Gini Coefficient") +
          scale_color_manual(values = colors) + 
          theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

p2 <- ggplot(data = GINIr, 
       aes(x = round, y = gini, fill = treatment, color = treatment, lty = treatment)) +
          layout +
          theme(legend.position="bottom") +
          # geom_vline(xintercept = 10, alpha = 0.66) +
          geom_line(show.legend=FALSE) +
          geom_point() +
          scale_x_continuous(name="",  breaks = 1:10) +
          scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
          labs(y = "Gini (Rich)") +
          scale_color_manual(values = colors)

p3 <- ggplot(data = GINIp, 
       aes(x = round, y = gini, fill = treatment, color = treatment, lty = treatment)) +
          layout +
          theme(legend.position="bottom") +
          # geom_vline(xintercept = 10, alpha = 0.66) +
          geom_line(show.legend=FALSE) +
          geom_point() +
          scale_x_continuous(name="",  breaks = 1:10) +
          scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
          labs(y = "Gini (Poor)") +
          scale_color_manual(values = colors)

(p1 | (p2 / p3)) + plot_layout(guides = "collect") & theme(legend.position = "bottom")

rm(list = c("GINI", "GINIp", "GINIr", "p1", "p2", "p3"))
```

### Inequality Differences between Data Sources

Once more, we consider whether we were able to replicate GMTV's results with our data.

The following table shows a simple OLS regression to illustrate differences in the 10th round's Gini coefficient between the replication's data and GMTV's data. Mean Gini coefficients are similar across data sources and there are no statistically significant differences in mean Gini coefficients.

```{r tableGINI, results = "asis", warning = FALSE}

# create subsets
main_all  <- main[round == 10]
main_poor <- main[round == 10 & rich == FALSE]
main_rich <- main[round == 10 & rich == TRUE]

# create table
if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

m1 <- lm(formula = gini ~ treatment, data = main_all)
m2 <- lm(formula = gini ~ treatment, data = main_poor)
m3 <- lm(formula = gini ~ treatment, data = main_rich)

stargazer(m1, m2, m3,

          column.labels = c("All", "Below median", "Above median"),
          model.numbers = FALSE,
          dep.var.labels = "Gini",
          header=FALSE,
          covariate.labels = c("Replication"),

          type = type, digits = 2, omit.stat = c("adj.rsq", "f"), df = FALSE
          )
```


# Online Methodology

## Time Spent

Participants spent approximately `r duration[, completion_time %>% unique() %>% mean()] %/% 60 + 1` minutes completing the experiment. Reading the instructions and answering the comprehension questions took the most time, that is, `r duration[page_name == "Intro_Instructions", time_spent %>% unique() %>% mean()] %/% 60 + 1` minutes. The public goods game required `r duration[app_name == "dPGG", time_spent %>% sum(), by = c("session_code", "participant_code")][,V1] %>% mean(na.rm = TRUE) %/% 60` minutes of the participants' time.

```{r plotTime, fig.cap = "Average Time Spent for each Contribution per Round."}
N <- duration[, participant_code %>% unique() %>% length()]
plotDT <- duration[app_name == "dPGG" & page_name == "dPGG_Decision",
                   .(time_spent = time_spent %>% sum()),
                   by = c("session_code", "participant_code", "page_index", "page_name")]

plotDT[, round := seq(from = 1, to = 10), by = c("participant_code")]

upperLimit <- plotDT[, time_spent %>% mean(), by = c("round")] %>% max()



ggplot(data = summarySE(data = plotDT,
                        measurevar = "time_spent",
                        groupvars=c("round"),
                        na.rm = FALSE,
                        conf.interval = 0.95,
                        .drop = TRUE),
       mapping = aes(x = round, y = time_spent)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE, color = cSecondary, lty=2) +
  geom_errorbar(aes(ymin=time_spent-ci, ymax=time_spent+ci), width=.25, alpha = 0.5, color = cSecondary) +
  geom_point(color = cSecondary) +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit + 10), expand = c(0, 0)) +
  labs(y = "Time Passed in Seconds", caption = "Bars indicate 95% confidence intervals.") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))
```

Figure 4 illustrates the time participants needed to make each of their contributions during the replication. One can see that the initial as well as most of the other contributions take about 20 seconds of time. Interestingly, the second contribution takes (on average) about 50% more time than the first one - presumably because this is the first time participants' learn their respective group members' actions. The third contribution is a little faster and all subsequent contributions stabilize at `r plotDT[round > 3, time_spent %>% mean()] %>% ceiling()` seconds.

Given that no participant dropped out after answering the comprehension questions correctly and given that participants need less than 20 seconds to make their contributions, more than 10 rounds are feasible.

# Exporatory: Generalizability

The following figure gives a first overview over the distribution of contributions across the two sorts of contributions. The density-plots in the top panel show the fraction of the initial endowment contributed in the first round of the experiment. The two plots in the bottom panel show contribution behavior in the VCA, that is, donations to climate change mitigation. The left panels depict the general population's behavior while the right panelshows the students' behavior. All contributions are relative to the maximum amount a participant could have contributed.

Means are indicated by vertical dashed lines and are positive in both settings across samples.^[While the medians are similar in the bottom panel, that is, the experiment's contributions, they are smaller in the taop panel and zero in the student sample.] Overall contributions in the VCA setting are smaller than in the experiment's first round.

```{r vizGeneralizability}
R1covs <- covs[, .(participant.code,
                   donation = donation * 20 ,
                   donationShare = (donation * 20) / payoff * 100,
                   payoff)]
temp <- R1[R1covs, on = .(participant.code = participant.code)][treatment == "replication"]
temp[, contributionShare := ownContribution/20 * 100]

# viz wrapper
plotShares <- function(stud = FALSE,
                       var  = "donationShare",
                       col  = cPrimary,
                       xlab = "",
                       ylab = ""){
  
  dt <- temp[student == stud, .(x = get(var))]
  
  ggplot(data = dt,
         mapping = aes(x = x)) +
    geom_density(alpha = 1,
                 fill = col) +
    scale_x_continuous(limits = c(0, 100),
                       expand = c(0, NA)) +
    scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
    geom_vline(xintercept = dt[,mean(x, na.rm = TRUE)],
               lty = 2) +
    labs(x = xlab,
         y = ylab) +
  layout
}

p1 <- plotShares(stud = FALSE, col = cPrimary,   var = "donationShare", ylab = "Density in VCA")
p2 <- plotShares(stud = FALSE, col = cPrimary,   var = "contributionShare", 
                 xlab = "Share of general pop. contributions", ylab = "Density in Experiment (round 1)")
p3 <- plotShares(stud = TRUE,  col = cSecondary, var = "donationShare")
p4 <- plotShares(stud = TRUE,  col = cSecondary, var = "contributionShare", xlab = "Share of students' contributions")

(p1 / p2) | (p3 / p4)



rm(list = c("R1covs", "p1", "p2", "p3", "p4"))
```

lorem ipsum.

```{r}
ggplot(data = temp[, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.33, col = cSecondary, size = 2) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "Experiment: Percentage of endowment contributed in round 1") +
  layout
```
A visual inspection of the scatter plot (where the saturation of the points is proportional to the frequency of a pair of contribution choices) does not hint at an association between the two sorts of contributions.

## To do

- explain similarities and differences to @GKLS2020 setup and how they came to be
- compare samples across tasks
- who generalizes (better)?
- who is more heterogeneous and what does that mean?
- How do the results compare to @GKLS2020

# Conclusion


