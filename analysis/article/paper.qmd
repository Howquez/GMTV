---
title: Revisiting 'Growth and Inequality in Public Good Provision' ---Reproducing and Generalizing Through Inconvenient Online Experimentation
author:
  - name: Hauke Roggenkamp
    email: Hauke.Roggenkamp@unisg.ch
    orcid: 0009-0005-5176-4718
    affiliations:
        - id: HSU
          name: Helmut Schmidt University
          department: Department of Economics
          address:  Holstenhofweg 85
          city: Hamburg
          state: Germany
          postal-code: 22043
        - id: HSG
          name: University of St. Gallen
          department: Institute for Behavioral Science and Technology
          address: Torstrasse 25
          city: St. Gallen
          state: Switzerland
          postal-code: 9000
abstract: |
  I revisit the dynamic public goods game developed by GÃ¤chter et al. (2017) to study cooperation under dynamic interdependencies. Collecting data from both a convenient (students) and an inconvenient (general population) sample, I not only reproduce some of the authors' original observations but also test their novel game's generalizability. Appending a charitable dictator game, I find no correlations between behavior in the charitable context and the dynamic game. This applies to students and the general population sample alike. Because the study of inexperienced general population samples raises methodological challenges, such as fatigue and dropouts, this research approaches them. In doing so, I provide simple solutions to run reliable interactive experiments online. Furthermore, this article showcases the use of literate programming and version control which I argue are convenient tools to make pre-registrations more credible and flexible.

keywords: 
  - Public goods
  - Dynamic games
  - Reproducibility
  - Generalizability
  - Online experiment
  - Climate change mitigation
date: last-modified
bibliography:  ../biblio.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal of Behavioral and Experimental Economics
      formatting: review
      model: 3p
      layout: onecolumn
      cite-style: authoryear
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE,
                      fig.width = 10, fig.height = 4)
```

```{r locale}
#| include: false
#| warning: false
#| eval: false

Sys.setlocale("LC_TIME","English United States")
```


```{r}
#| warning: false

if (!requireNamespace("groundhog", quietly = TRUE)) {
    install.packages("groundhog")
    library("groundhog")
}

pkgs <- c("magrittr", "data.table", "stringr", "lubridate", "glue", "haven",
          "DescTools", "censReg", "stargazer", "vtable", "ggplot2",
          "patchwork", "Rmisc", "knitr")

groundhog::groundhog.library(pkg = pkgs,
                             date = "2024-01-01")

rm(pkgs)
```

```{r constants}
SHOW_UP_FEE <- 5
EXCHANGE_RATE <- 1/20
```

```{r design}
# ggplot layout
layout <- theme(panel.background = element_rect(fill = "transparent", color = NA),
                plot.background = element_rect(fill = "transparent", color = NA),
                panel.grid = element_blank(),
                panel.grid.major.y = element_blank(),
                legend.key = element_rect(fill = "transparent"),
                axis.line = element_line(size = 0.25),
                axis.ticks = element_line(size = 0.25),
                axis.title = element_text(size = 8),
                legend.text = element_text(size = 8),
                plot.caption = element_text(size = 6,
                                            colour = "#555555"),
                legend.title = element_blank()
)

# color
cPrimary = "#F3B05C"
cSecondary = "#1E4A75"
cInfo = "#36BFA2"
```

<!-- OUR DATA -->


```{r readAllData}
# identify files of all sessions
files <- list.files(path = "../../data/replication/",
                       recursive = TRUE,
                       full.names = TRUE)

# load all of them into a list
csvs <- list()
for(i in files){
  name <- str_extract(string = i, pattern = "2021.*")
  temp <- read.csv(i, stringsAsFactors = FALSE) %>% data.table()
  csvs[[name]] <- temp
}

# bind list into single data table
full <- rbindlist(l = csvs, use.names = TRUE) %>% data.table()

# ignore observations who have not made it to the first contribution
DT <- full[participant._index_in_pages > 2 & 
             participant.time_started != "" & 
             participant.label != "" &
             !(is.na(dPGG.1.player.contribution))]

# tidy up
rm(list = c("files", "csvs", "i", "name", "temp"))
```

```{r metaData}
# aggregate each session's meta data
meta_long <- full[participant.time_started != "" & 
                    participant.label != "",
                  .(session.code,
                    participant.label,
                    participant.time_started,
                    participant._index_in_pages,
                    dPGG.1.player.contribution)]

# reformat time stamp in new variables
meta_long[, `:=`(date = participant.time_started %>% ymd_hms() %>% date(),
                 hour = participant.time_started %>% ymd_hms() %>% hour())]

# aggregate and count 
meta <- meta_long[,
                  .(date = date %>% unique(),
                    # weekday = date %>% unique() %>% lubridate::wday(label = TRUE),
                    time = hour %>% unique() %>% paste("00", sep = ":"),
                    showups = participant.time_started %>% length(),
                    dropouts = sum(participant._index_in_pages == 2),
                    residuals = sum(is.na(dPGG.1.player.contribution) & participant._index_in_pages == 77)),
                  by = session.code]

# get number of participants and observations (groups of 4)
meta[,
     `:=`(participants = showups - dropouts - residuals,
          observations = (showups - dropouts - residuals)/4)]

# save data
fileName <- "replication2021"
save(meta, file = paste0("../../data/processed/rda/", fileName, "_meta", ".rda"))
write.csv(meta, file = paste0("../../data/processed/csv/", fileName, "_meta", ".csv"))

rm(list = c("meta_long"))
```

```{r identifyStudents}
# we had to invite students as the general population sample was exhausted, unfortunately. let's identify them:
invitedStudents <- read.table(file = "../../data/sample/invited_students.txt", col.names = c("participant.label")) %>%
  data.table()
# note that these students participated in the ultimate session
```

```{r focusOnFirstRound}
# create data table 
replicationFirstRound <- DT[,
                 .(participant.code,
                   treatment = "replication",
                   session.code,
                   groupID = paste(session.code, dPGG.1.group.id_in_subsession, sep = "_"),
                   othersContribution = dPGG.1.group.total_contribution - dPGG.1.player.contribution,
                   ownContribution = dPGG.1.player.contribution,
                   trust = Outro.1.player.PQ11,
                   government = Outro.1.player.PQ13,
                   comprehension = dPGG.10.player.comprehension)]

# save data
save(replicationFirstRound, file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(replicationFirstRound, file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))
```

```{r subsetData}

# identify most relevant variables
mRegex <- "participant\\.code$|session\\.code$|dPGG\\.1\\.group\\.id_in_subsession|^dPGG.1.player.id_in_group$|\\.1\\.player.belief|endowment|contribution|stock|gain$|bot_active|10.player.comprehension|10.player.donation"
mainVariables <- str_subset(string = names(DT), pattern = mRegex)

# prune all other variables
subset <- DT[, ..mainVariables]

# tidy up
rm(list = c("mRegex", "mainVariables"))
```

```{r addNewVariables}
# refactor groupID such that it eventually contains treatment-info
subset[, groupID := paste(session.code, 
                          dPGG.1.group.id_in_subsession,
                          sep = "_")]

# add share as contribution/endowment
for(round in 1:10){
  contribution <- glue("dPGG.{round}.player.contribution")
  endowment <- glue("dPGG.{round}.player.endowment")
  subset[, glue("dPGG.{round}.player.share") := subset[[contribution]]/subset[[endowment]] ]
}

# add treatment variable
subset[,
       treatment := "replication"]
```

```{r calculateAggregates}
# we need to aggregate some outcomes on a groupID level (per sessions per treatment)
# we'll do so using a loop
# this will yield a list of data tables that will be merged to a list eventually.
cluster <- c("treatment", "session.code", "groupID")
outcomes <- c("contribution", "endowment", "stock", "gain", "bot_active")

DTs <- list()
for(outcome in outcomes){
  if(outcome == "bot_active"){
    var = names(subset) %>% str_subset(pattern = glue("group\\.{outcome}$"))
  } else {
    var = names(subset) %>% str_subset(pattern = glue("player\\.{outcome}$"))
  }


  # calculate either averages or the sum per round per group
  aggregates = subset[, lapply(.SD, sum, na.rm=TRUE), 
                      by = cluster, 
                      .SDcols=var]
  
  # transform from wide to long
  meltedAggregates <- melt(aggregates, id.vars = cluster, measure.vars = var)
  DTname <- glue("{str_to_title(outcome)}")
  DTs[[DTname]] <- meltedAggregates
  rm(list = c("DTname", "meltedAggregates", "aggregates", "var", "outcome"))
}
```

```{r renameVariables}
# each data table in that list has the same column names.
# the outcomes are all named "value", for instance.
# now, we"ll infer the round number (contained in the variable name)
for(i in 1:length(outcomes)){
  DTs[[i]] <- DTs[[i]][,
                       .(treatment,
                         session.code,
                         groupID,
                         round = str_replace_all(string = variable,
                                                 pattern = "\\D", 
                                                 replacement="") %>% as.integer(),
                         value # to be renamed afterwards
                       )
  ]
  # rename "value" to outcome variable
  setnames(DTs[[i]], old = "value", new = outcomes[i])
}

# tidy up
rm("i")
```


```{r calculateGini}
# repeat everything for the gini
var = names(subset) %>% str_subset(pattern = "player\\.stock$")
gini = subset[,
              lapply(.SD, Gini, na.rm=TRUE), 
                by = cluster, 
                .SDcols=var
              ]
Gini <- melt(gini, id.vars = cluster, measure.vars = var)

DTs[["Gini"]] <- Gini[,
                    .(treatment,
                      session.code,
                      groupID,
                      round = str_replace_all(string = variable,
                                              pattern = "\\D", 
                                              replacement="") %>% as.integer(),
                      gini = value
                       )]
rm(list = c("var", "gini", "Gini"))

# note that GMTV used start of period earnnings, i.e. endowments. We use end of period earnings, i.e. stock.
# this adjustment has been considered in our processing of GMTVs data.
```


```{r mergerEverything4FinalData}
# now merge every table in the list to one final data table called "replication"
replication <- Reduce(function(...) merge(..., by=c(cluster, "round"), all = TRUE), DTs)

# tidy up
rm(list = c("cluster", "contribution", "endowment", "round", "DTs", "outcomes"))
```

```{r calculateShare}
# define the share (one of the main outcome variables) as 
# the sum of contributions devided by the sum of endowments
replication[, share := contribution/endowment]
```

```{r defineRichGroups}
# use the median to differentiate between poor and rich groups (as GMTV did)
median <- replication[round == 10,
                      median(stock)]

# find the groups that end up rich or poor
richGroups <- replication[round == 10 & stock > median,
                          unique(groupID)] 

poorGroups <- replication[round == 10 & stock < median,
                          unique(groupID)]

# mark these groups for each period
replication[groupID %in% richGroups,
            rich := TRUE]

replication[groupID %in% poorGroups,
            rich := FALSE]

# tidy up
rm(list = c("median", "poorGroups", "richGroups"))

```

```{r misc}
# flag observations where at least one participant did not understand the game
noComp <- subset[dPGG.10.player.comprehension == 0,
                 groupID] %>% unique()
replication[,
     noComprehension := 0]
replication[groupID %in% noComp,
     noComprehension := 1]


# drop observations (i.e. groups in rounds) with dropouts (bot_active == 1) and
# where round > 10
replication <- replication[bot_active == 0 & round <= 10]

# tidy up
rm("noComp")
```

```{r saveData}
save(replication, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(replication, file = paste0("../../data/processed/csv/", fileName, ".csv"))
```

```{r createCovariates}

# so far, I neglected many variables in what happened before. The following few lines
# deal with some of these variables in a similar procedure


# add variables
DT[, treatment := "replication"]
DT[, groupID := paste(session.code, dPGG.1.group.id_in_subsession, 
                    sep = "_")]

# subset
cRegex <- "participant.code|session.code|treatment|groupID|Outro.1.player|10.player.donation|10.player.stock|switching_row|inconsistent"
covariates <- str_subset(string = names(DT),
                            pattern = cRegex)
CT <- DT[, ..covariates]

# rename
names(CT) <- names(CT) %>% 
  str_replace_all(pattern =".*player\\.",
                  replacement = "") %>%
  str_to_lower()
names(CT)[names(CT) == "groupid"] <- "groupID"

# refactor
CT[, donation := donation/20] # exchange rate tokens to real world currency 1/20
CT[donation %>% is.na, donation := 0]
CT[, gender := ifelse(test = gender == "female",
                      yes  = 1,
                      no   = 0)]
CT[, inconsistent := as.logical(inconsistent)]

# reassign payoff (with stock of the last period)
CT[, payoff := stock]
CT[, stock := NULL]

# write data
replicationCovariates <- CT
save(replicationCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(replicationCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("CT", "cRegex", "covariates"))
```

```{r readTime}
# as before, we need to read a list of files (measuring the time spent per page)

# identify files
files <- list.files(path = "../../data/pageTimes/",
                       recursive = TRUE,
                       full.names = TRUE)

# loop
csvs <- list()
for(i in files){
  name <- str_extract(string = i,
                      pattern = "2021.*")
  
  temp <- read.csv(i, 
                   stringsAsFactors = FALSE) %>%
    data.table()
  
  csvs[[name]] <- temp
}

# bind files list to data.table
timeSpent <- rbindlist(l = csvs,
                       use.names = TRUE) %>%
  data.table()

# set oder
setorder(timeSpent, session_code, participant_code, epoch_time)

# tidy up
rm(list = c("files", "i", "name", "temp"))
```


```{r calcDuration}
# shift rows to calculate this duration (per page) as the distance between
# the current and the next time stamp
timeSpent[,
          lag := shift(epoch_time, fill = NA, type = "lag"),
          by = c("session_code", "participant_code")]

timeSpent[,
          duration := epoch_time - lag,
          by = c("session_code", "participant_code")]

# calculate overall time spent as the difference between the min and max time stamp
timeSpent[,
          completion := epoch_time %>% max() - epoch_time %>% min(),
          by = c("session_code", "participant_code")]

# create new data table with selected columns
duration <- timeSpent[session_code %in% meta$session.code, # participant_code %in% DT$participant.code,
                      .(
                        session_code,
                        participant_code,
                        app_name,
                        page_name,
                        page_index,
                        page_submission = epoch_time,
                        time_spent = duration,
                        completion_time = completion
                      )]

# save data
save(duration, file = paste0("../../data/processed/rda/", fileName, "_timeSpent", ".rda"))
write.csv(duration, file = paste0("../../data/processed/csv/", fileName, "_timeSpent", ".csv"))

# tidy up
rm(list = c("timeSpent", "csvs", "DT"))
```




<!-- ORIGINAL (GMTV) DATA -->

```{r readGMTV}
# having done all that, we need to prepare the original data and bind it to ours
# I'll thus, start by reading the orginal data

DT <- read_dta(file="../../data/gaechteretal/GMTV-data.dta") %>% data.table()
CT <- read_dta(file="../../data/gaechteretal/GMTV-questionnaire-data.dta") %>% data.table()
```

```{r subsetGMTV}
# because we only replicate one of their treatments, I subset accordingly
# noPunish10 <- DT[exp_num == 5 | exp_num == 8 | exp_num == 9]
noPunish10 <- DT[longgame == 0 & punish == 0 & exp_num <= 10]
```

```{r newVariablsGMTV}
# calculate the share of endowments contributed
noPunish10[,
           share := sum/gdp,
           by = .(subj_id, per)]

# calculate gini based on endowments (as GMTV did)
noPunish10[,
           gini2 := Gini(c(tokens, other1, other2, other3)),
           by = .(subj_id, per)]

# calculate gini based on stock (i.e. end of period earnings)
noPunish10[, stock0 := (tokens - putin + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock1 := (other1 - pu1 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock2 := (other2 - pu2 + 1.5*sum/4) %>% ceiling()]
noPunish10[, stock3 := (other3 - pu3 + 1.5*sum/4) %>% ceiling()]
noPunish10[, 
           gini3 := Gini(c(stock0, stock1, stock2, stock3)),
           by = .(subj_id, per)]
```

```{r createGMTVtable}
# select columns we are interested in and rename a few to match our data

GMTV <- noPunish10[order(gr_id, per),
           .(treatment = "noPunish10", 
             session.code = exp_num,
             groupID = gr_id, 
             round = per,
             contribution = sum,
             endowment = gdp,
             share,
             stock = (gdp + ceiling(sum/4*1.5)*4-sum),
             gain = (ceiling(sum/4*1.5)*4-sum),
             gini = gini3,
             bot_active = 0,
             noComprehension = NA)] %>% unique()

# tidy up
rm(list = c("noPunish10"))
```

```{r richPoorGMTV}
 # create rich indicator (just as highgdp in original data)
median <- GMTV[round == 10,
                   median(stock)]

richGroups <- GMTV[round == 10 & stock > median,
                       unique(groupID)] 

poorGroups <- GMTV[round == 10 & stock < median,
                       unique(groupID)]

GMTV[groupID %in% richGroups,
         rich := TRUE]

GMTV[groupID %in% poorGroups,
         rich := FALSE]

# tidy up
rm(list = c("median", "richGroups", "poorGroups", "fileName"))
```

```{r saveGMTV}
fileName <- "GMTV2017"
# save data
save(GMTV, file = paste0("../../data/processed/rda/", fileName, ".rda"))
write.csv(GMTV, file = paste0("../../data/processed/csv/", fileName, ".csv"))
```


```{r calcSwitchingPoint}
# GMTV gathered some risk data that I'll process in a separate DT and merge it
# with other covariates later on

GMTVRisk <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
               .(participant.code = subj_id,
                 e10, e20, e30, e40, e50, e60, e70,
                 inconsistent = ifelse(test = e60 < e70 | e50 < e60 | e40 < e50 | e30 < e40 | e20 < e30 | e10 < e20,
                                       yes  = TRUE,
                                       no   = FALSE)
                 )]

GMTVRisk[,
         switching_row := ifelse(test = inconsistent == TRUE,
                                yes  = NA,
                                no   = e10 + e20 + e30 + e40 + e50 + e60 + e70 + 1 + 2) # because GMTV used a 7-point-likert scale
         ]
```

```{r subsetCovariates}
# subset covariate data and rename+refactor some variables
temp <- CT[exp_num == 5 | exp_num == 8 | exp_num == 9,
                         .(treatment = "noPunish10",
                           session.code = exp_num,
                           groupID = gr_id,
                           participant.code = subj_id,
                           gender,
                           age = Sys.Date() %>% lubridate::year() - age,
                           pq01 = q1,
                           pq02 = q2,
                           pq03 = q3,
                           pq04 = q4,
                           pq05 = q5,
                           pq06 = q6,
                           pq07 = q7,
                           pq08 = q8,
                           pq09 = q9,
                           pq10 = q10,
                           pq11 = q11,
                           pq12 = q12,
                           pq13 = q13,
                           pq14 = q14,
                           donation = don
                           )]

temp[donation %>% is.na, donation := 0]

```

```{r mergeAndWrite}

GMTVCovariates <- data.table::merge.data.table(x = temp,
                                               y = GMTVRisk[, .(participant.code, inconsistent, switching_row)],
                                               by = c("participant.code"))

# write data
save(GMTVCovariates, 
     file = paste0("../../data/processed/rda/", fileName, "_COVS", ".rda"))
write.csv(GMTVCovariates, 
          file = paste0("../../data/processed/csv/", fileName, "_COVS", ".csv"))

# tidy up
rm(list = c("temp", "GMTVRisk"))
```

```{r firstRound}
noPunish <- DT[exp_num == 1 | exp_num == 3 | # noPunish15
                 exp_num == 5 | exp_num == 8 | exp_num == 9] # noPunish10

temp <- noPunish[per == 1]
temp <- temp[,
             treatment := "noPunish10"]
temp <- temp[exp_num == 1 | exp_num == 3,
             treatment := "noPunish15"]

GMTVFirstRound <- temp[,
                           .(participant.code = subj_id,
                             treatment,
                             session.code = exp_num,
                             groupID = gr_id,
                             othersContribution = sumputin - putin,
                             ownContribution = putin,
                             trust = NA,
                             government = NA,
                             comprehension = NA)]

# save data
save(GMTVFirstRound, 
     file = paste0("../../data/processed/rda/", fileName, "_R1", ".rda"))
write.csv(GMTVFirstRound, 
          file = paste0("../../data/processed/csv/", fileName, "_R1", ".csv"))

rm(list = c("temp", "fileName"))
```

<!-- MERGE & TUNE DATA -->

```{r rbindOursAndGMTV, eval = TRUE}
main <- rbindlist(list(replication, GMTV), 
                  use.names = TRUE)

R1   <- rbindlist(list(replicationFirstRound, GMTVFirstRound), 
                  use.names = TRUE)

covs <- rbindlist(list(replicationCovariates, GMTVCovariates), 
                  use.names = TRUE,
                  fill = TRUE)

rm(list = c("CT", "DT", "GMTV", "GMTVCovariates", "GMTVFirstRound", 
            "noPunish", "replication", "replicationCovariates",
            "replicationFirstRound", "subset"))
```

```{r treatmentAsFactor}
main[, treatment := as.factor(treatment)]
R1[, treatment := as.factor(treatment)]
covs[, treatment := as.factor(treatment)]
```

```{r studentSample}

# Two of these sessions were special: The first (`r meta[1, session.code]`) as well as the last one (`r meta[, session.code %>% tail(n=1)]`). The first session suffered technical problems such that the risk elicitation task was omitted. The last session (almost exclusively) relied on a student sample as our non-student sample was exhausted after the first three sessions. As a consequence, the last session was conducted with 59 students while all others were conducted without any students. I'll therefore create a boolean `student` variable. Becaue all of the original experiments were conducted with students, `student` also equals 1 in the "noPunish10" treatments

# Importantly, as of today, I don't remember how to perfectly identify students in the last session. Hence, I label all participants in that session as students. This is reasonable when analyzing group level outcomes because it is likely that there was at least one student in many of the groups. It is somewhat problematic in the generalization discussion. However, using age as an additional variable to guess student status (e.g. non-student if age > 30) does not not change the results qualitatively.

main[, student := FALSE]
main[session.code == "d6jrsxnr" | treatment == "noPunish10",
     student := TRUE]

covs[, student := FALSE]
covs[session.code == "d6jrsxnr" | treatment == "noPunish10",
     student := TRUE]

R1[, student := FALSE]
R1[session.code == "d6jrsxnr" | treatment == "noPunish10",
   student := TRUE]
```

```{r earnings}

# This chunk requires raw data
files <- list.files(path = "../../data/replication/",
                       recursive = TRUE,
                       full.names = TRUE)
csvs <- list()

for(i in files){
  name <- str_extract(string = i,
                      pattern = "2021.*")
  
  temp <- read.csv(i, 
                   stringsAsFactors = FALSE) %>%
    data.table()
  
  csvs[[name]] <- temp
}

full <- rbindlist(l = csvs,
                  use.names = TRUE) %>%
  data.table()


# subset data of participants who have completed the study
tmp <- full[participant._index_in_pages >= 76 & 
              participant.time_started != "" & 
              !(is.na(dPGG.1.player.contribution)) &
              participant.label != ""]

# calculate total payoff
tmp[, finalPayoff := (dPGG.10.player.stock - dPGG.10.player.donation + HLPL.1.player.payoff) * EXCHANGE_RATE + SHOW_UP_FEE]

earningsMean <- tmp[, mean(finalPayoff, na.rm = TRUE)]
earningsSD   <- tmp[, sd(finalPayoff, na.rm = TRUE)]
```



<!-- PAPER STARTS HERE -->

<!-- 
# Key Takeaways

- We **[replicate](#gmtv-replication)** a public goods experiment with dynamic inter-dependencies and find similar results as @GMTV2017. 
    + Absolute contributions increase over time.
    + Just as in _static_ public goods experiments, the share of endowments contributed decreases over time.
    + The richest groups earn fifteen times more than the poorest groups.
    + While there clearly is growth, groups do not realize the maximal potential efficiency and earn just over `r main[treatment == "replication" & round == 10, mean(stock)/(80*1.5^10)*100] %>% round(digits=1)`% of what is possible.
- Varying the subject pool and including a voluntary climate action (VCA), we have a setup similar to @GKLS2020, which allows us to assess **[generalizability](#generalizability)**.^[Note that one can easily model the VCA as a dictator game, which relates to a literature summarized by @cartwright2022.]
- Studying a general population sample online, synchronously and over the course of multiple periods, we find that classic **[lab procedures](online-methodology)** can be applied outside of the lab too.
    + Even though we conducted the experiment online and remotely, dropouts (or "attrition") are no concern.
    + Relying on an inexperienced non-convenience sample that cannot interact with experimenters, `r trunc((R1[comprehension > 1] %>% NROW() / R1[!(is.na(comprehension))] %>% NROW())*100)`% of all participants stated that they understood the game.
    + Participants make relatively fast decisions which makes longer games feasible in the future.^[Taken together with inefficient growth and the lack of dropouts, additional rounds are fairly cheap.]

-->

# Introduction {#sec-intro}

Today's actions are tomorrow's result. There are many settings in which current decisions affect future outcomes and with it, future decision spaces. 
<!-- Think of lifetime consumptionâsmoothing and save more tomorrow -->
For example, opting for environmentally friendly policies today not only reduces carbon dioxide emissions immediately, but also helps us reach the Paris climate targets tomorrow. Deferring these policies to a later state, may not necessarily prevent us from reaching these targets, but it will require more effort in the future compared to a path which includes immediate action [@HaenselEtAl2022]. Aiming at certain goals, today's actions (or the omission thereof) not only affect intermediate outcomes but also the number of viable paths one can choose from that lead to that specific goal.

Public good (or public bad) games---although often intended to inform climate policies [e.g. @MilinskiEtAl2006; @TavoniEtAl2011; @Hauser2014; @BrickEtAl2015; @GomezEtAl2018; @CalzolariEtAl2018; @CookEtAl2019]---fail to include these temporal interdependencies simply because participants have the same set of actions in each period. Accordingly, participants' actions in a given period do not affect their action space in subsequent periods. To better capture the lack of realism, consider carbon dioxide emissions, where the current stock will last for well over a millennium [@Inman2008; @CalzolariEtAl2018]. Playing with fresh endowments in each period is as if one could just undo carbon dioxide emissions at no cost.

A game designed by GÃ¤chter, Mengel, Tsakas & Vostroknutov [-@GMTV2017 hereafter, GMTV] as well as Stefan GroÃe (unpublished), shows that it is relatively simple to include this element of realism. They incorporate interdependencies into a _dynamic_ public goods game (dPGG) by defining endowments as the income of previous periods. Consequently, participants' actions in a given period affect their number of actions in subsequent periods: the more (less) they earn now, the more (less) they can contribute in the next period. Importantly, this modification qualitatively yields the same rational predictions as the static game (i.e. free-riding and the under-provision of the public good). It is thus, equally well suited to study dilemma situations.

As there is surprisingly little experimental research on interdependencies^[See e.g. @BattagliniEtAl2016, @Rockenbach2017 or @Moser2019.], I reproduced one of GMTVs' treatments to compare dynamics across (in)experienced samples to investigate its generalizability.
@GKLS2020 find that static public goods games do not generalize well to real-world climate action. They also find that generalizability is dependent on the structural resemblance of the public goods game with the context of climate change mitigation: Greater resemblance improves generalizability. Because GMTVs' dynamic setting has a more realistic propertyânamely, interdependenciesâone would expect it to be better suited to inform public policy. As means to test this intuition, I not only ran the experiment with different samples but also
<!--, that is, with participants who make their decisions in an environment which is more natural than the lab.-->
observed the participants' behavior in voluntary climate actions (VCA).^[The VCA is a (charitable) dictator game where each participant is a dictator dividing her budget between herself and some organization linked to the reduction of CO2 emissions. @EckelGrossman1996 were the first to implement a charitable dictator game observing contributions of 30% of the endowments. Like @GKLS2020, @CarpenterEtAl2008 report that students make lower contributions to charity than community members.] This yielded a setting similar to the one of @GKLS2020, which allows me to analyze how behavior in the abstract game translates into real-world action across samples. This research shows that the dynamic setting _does not_ add any advances to the generalizability of results.

@AGM2018 conducted static public goods games in the lab and on MTurk to draw (and report) lessons from online experimentation. This study extends this literature [see, e.g., @KrantzEtAl1997, @AmirEtAl2012; @KleinEtAl2014; @PaolacciChandler2014; @GoodmanPaolacci2017; @SnowbergYariv_2021; @GuptaRigottiWilson_2021; @BusoEtAl2021] by focusing on an _inconvenient_ sample (i.e. one that is a completely inexperienced sample [see also @BenndorfEtAl2017] that has not been exposed to interactive experiments before) playing a computationally more complex game during a time that was characterized by more inattentiveness in online samples [@ArecharRand2021; @PeytonEtAl2022]. With this background in mind, it required me to design robust (and thus, more complex) software to minimize attrition. I collected para data [@Parsons2022 p. 12]---which capture screen time and survey navigation, for instance---to assess the desired fluency and feasibility of the experiment. Like @AGM2018, this research is of practical relevance as it reports on the robust design that made the dynamic game feasible.

Taken together, this study makes three contributions. First, it reproduces parts of GMTVs' original experiment and highlights the importance of pure replications. Second, it shows that logistically complex online experiments are feasible for samples other than students or clickworkers. Third, this paper supports critics arguing that findings from abstract games do not generalize well---not even with a more representative sample. 

After commenting on transparent research practices and reporting the methods in @sec-transparency and @sec-methods, this paper is organized along these findings: The confirmatory reproduction is reported in @sec-replication, the online feasibility in @sec-feasibility, and the generalizability in @sec-generalizability. @sec-conclusion concludes. Because I observed some (qualitatively inconsequential) differences in reproducing GMTVs' analysis using their data, I report these in the [Appendix](@sec-appendix).

# Transparency {#sec-transparency}

Well before credibility crises comprised a variety of disciplines^[See e.g., @CamererEtAl2016; @BrodeurEtAl2016; @BrodeurEtAl2020; @ChristensenMiguel2018; @FerraroShukla2020; @PageEtAl2021 who focus on economics.], @bemwriting [p. 2] indirectly conceded discrepancies between fuzzy research processes and the polished article that results from it:

> _There are two possible articles you can write: (a) the article you planned to write when you designed your study or (b) the article that makes the most sense now that you have seen the results. They are rarely the same, and the correct answer is (b)._ 

Today, 20 years later, we have accustomed ourselves to mechanisms designed to unravel the fuzzy back-and-forth between exploratory and confirmatory research. Prominently, pre-registrations and pre-analysis plans (PAP) [see, e.g., @BrodeurEtAl2022] were established as a means to tie the researchers' hands with respect to p-hacking and ex-post theorizing when doing confirmatory research---reducing observable discrepancies between Daryl Bem's above-mentioned (a) and (b). 

While pre-registrations and PAPs could help to make empirical sciences more credible, they come at costs that are discussed by @Olken2015, @PageEtAl2021, as well as @Krishna2021, @SimmonsEtAl2021, and @PhamEtAl2021. One important concern is that strict pre-registrations may severely discount any additional analysis of the data inspired by surprise results. In practice, there are at least two ways to approach that problem: First, one could (and should) add unforeseen analyses to the article but be sure to label them as exploratory. Second, one could write a minimal pre-registration leaving enough degrees of freedom for the researcher to incorporate such surprises without further notice. The key difference between the two approaches is the deliberate omission of transparency.

For the field of neuropsychopharmacology @WaldronAllen2022 document that the depth of pre-registrations differs indeed: it is spanning from notes on hypotheses only to fully specified, detailed experimental protocols with accompanying power analyses and sampling plans. Treating all pre-registrations equally may thus lead to an adverse effect: If one can acquire the _'pre-registered'_ label with minimal effort and maximum flexibility, the label itself may run into danger by becomming worthless and to missing the target of making science more credible.

This erosion of credibility can be avoided by the means of transparency---not because transparency can tighten the ties but because it facilitates scrutiny, policing [see @AnkelPetersFialaNeubauer2023] and forensics [see, e.g., @SimonsohnEtAl2023]: transparent research practices make it easier for others to track and understand a researcher's reaction to surprises within the data, which, in turn, gives the researcher more flexibility to react to said surprises. As additional transparency is associated with additional effort, I propose version control as a natural and easy-to-implement solution augmenting pre-registrations.

Contemporary experimental research is, to a certain degree, a computational science: experiments often require complex software and the analysis thereof can result in complicated scripts. In addition, much research is carried out in collaborative efforts. For these reasons alone, it makes sense to implement version control systems such as GitHub and GitLab as well as OSF (albeit less powerful) that ensure that files are stored consistently and that changes are tracked. When these systems are implemented throughout the entire research process, one can archive different states of the software and visualize differences between different states. For instance, one can archive the analysis before and after the data was collected or the experimental software used in a pilot and the eventual experiment. Importantly, one can keep projects private and change their visibility to public only after the corresponding article is accepted for publication. Taken together, these features _automatically_ enhance transparency and facilitate scrutiny, policing, and forensics whilst making software development and collaboration more efficient. As a consequence, the implementation and eventual disclosure of version control come not only with meta-scientific but also with organizational benefits.

The experiment reported in this article qualifies for the _'pre-registered'_ label at first glance:
The analysis was pre-registered in the American Economic Association's RCT Registry [@preregistration]. Furthermore, I pre-registered the exact analyses I planned to run (PAP) when I designed the experiment on [GitHub](https://github.com/Howquez/GMTV/tree/July21Replication/analysis/R).^[bit.ly/3Pnak5H. To run the code, you need to execute the .Rmd files in this repository in the order that is indicated by file names. Detailed instructions can be found in the README file accompanying the analysis scripts.] 

At second glance, the analysis that made the most sense after having collected the data and that is reported here is a little different though: Because the more representative subject pool was exhausted earlier than anticipated, I recruited students which opened up a new research direction: assessing generalizability. Hence, this article combines both confirmatory as well as exploratory research. 

Accordingly, I edited many of the analysis scripts.^[The source code of this document contains the analysis and can be found [here](https://github.com/Howquez/GMTV/blob/main/analysis/article/paper.qmd): https://bit.ly/43LCyLw] However, because the originally planned analysis code is archived and reproducieable, these changes are transparent. Importantly, I followed a literate programming approach [@Knuth_1984; @AkhtarYe_2023]. Hence, all documents needed to analyze the data and to write the report stem from the same source. This establishes consistence between the commands one tells the statistical software to do and the explanation one tells human beings one told the statistical software to do. As such, (a), (b), and the transition from (a) to (b) are not only transparent but also comprehensible.

<!--
I archived detailed analyses scripts and created a [branch](https://GitHub.com/Howquez/coopUncertainty/tree/July21Replication) a few days _before_ we collected data and pre-registered the analysis via the American Economic Association's RCT Registry [@preregistration].
-->

# Methodology {#sec-methods}

In the terminology of @Hamermesh2007, I ran both a _pure_ as well as a _scientific_ reproduction^[In fact, @Hamermesh2007 uses the term _replication_ instead of _reproduction_. The reason I deviate is that I do not compare different treatments with each other to _replicate_ a treatment effect. Instead, I _reproduce_ the results of one specific treatment arm.] of one treatment of GMTVs' dynamic public goods game. The pure reproduction re-analyzes the original data. [Appendix A](#A:-Pure-Replication) documents [the errors I identified]() within the original paper. The scientific reproduction, where I utilize a different sample drawn from a different population in a different situation, is described in the following sections.

## Experimental Design {#sec-design}

The design builds on the workhorse model @Zelmer2003 [p.301] describes in her meta-analysis, where 

> _subjects are divided into groups and play the same game for a finite number of periods. Each period, every subject is endowed with an income [...] The subject must then divide this income between a contribution to a private account [...] that yields a constant return to themselves only and a contribution to a public account [...] where consumption benefits accrue to all group members. At the end of each period, subjects typically learn the aggregate contribution to the public good by all members of their group and their earnings for the period._

Considering the design, the 'only' differences between such a _static_ game (with partner matching) and GMTVs' design are _dynamics_: Instead of receiving fresh endowments every period, participants receive one endowment only at the beginning of the first period. A participant's endowment in the second period is the wealth she accumulated in the first period. A participant's endowment in the third period is the wealth she accumulated in the first two periods. And so on. Hence, a decision in one period has consequences on future endowments and, ultimately, growth paths. For this reason, the game is described as a _dynamic_ public goods game. 

 As in the NOPUNISH 10 Period treatment of GMTV, I ran sessions with groups of four ($i \in I=\{1,2,3,4\}$), an initial endowment of $N_i^1 = 20$ tokens^[A token was worth `r EXCHANGE_RATE` Euros.], $T=10$ periods, a private account with a return of $1$ and a group account with a return of $1.5$ ($\Rightarrow$ MPCR$\equiv \frac{1.5}{4}$). With $i$'s contribution in period $t$ being $c_i^t$, the model looks as follows:

$$
N_i^{t+1}=N_i^t - c_i^t + \frac{1.5}{4}\sum_{j=1}^4 c_j^t
$$


## Voluntary Climate Action (VCA)

In the same stroke as GMTV I employ a real giving task after the abstract game. Specifically, I employed a (charitable) dictator game where each participant is a dictator dividing her budget between herself and some organization. Whereas GMTV chose _Doctors without Borders_ as a receiver, I chose another organization in another context: like @GKLS2020, I employed a VCA, where participants could donate any amount of their earnings to offset carbon dioxide (that is, retire emission permits from the EU ETS).^[Importantly, @GKLS2020 made the VCA decision with a fresh endowment _before_ they played the abstract game. I deviate from their procedure to match GMTVs' procedure.]
To ensure that each participant had the same basic level of information about the impact of their decision, I provided some basic information about the mechanism. The information also highlighted that the mitigation came into operation on a European level.
Finally, I informed the participants that the documentation of individual and aggregate contributions was to be posted immediately after the conclusion of the sessions online. To avoid privacy or social image concerns, participants learned their unique and random IDs, which they needed to identify their individual contributions.
The document certified that their contributions have been used to offset [1.82 tons](https://www.compensators.org/compensatelist/?searchterm=stefan+traub) of carbon dioxide emissions.


## Recruitment and Sample Characteristics {#sec-sample}

I recruited the participants from the so-called _[HamburgPanel](https://www.wiso.uni-hamburg.de/forschung/forschungslabor/umfragelabor/aktuelle-umfragen/hamburgpanel.html)_ using HROOT [@hroot]. The panel is provided by the University of Hamburg's Research Laboratory, which used a randomized last digits approach to build the panel while drawing from the population of citizens of Hamburg, Germany. Because the sample was exhausted at one point, I also recruited students from the University of Hamburg.

At the time I conducted the experiment, the more representative sample was not familiar with interactive experiments. In fact, I ran the first interactive group experiment with this sample. The students, in contrast, were used to single-player experiments. Recruiting them, I excluded those who have participated in interactive experiments such as public goods games before, to keep the two distinct subject pools comparable in terms of their experience. As a consequence, nonnaivetÃ© is unlikely to affect the validity of the experiment [@GoodmanPaolacci2017 p. 204].

Throughout this paper, I will compare the results of my experiment with the results of GMTVs' NOPUNISH 10 Period treatments. I am thus, referring to three different samples utilized at two different points in time: the University of Nottingham's students (in late 2012), Hamburg's citizens, and the University of Hamburg's students (both in July 2021). @tbl-sample-properties gives an overview.

```{r sampleDifferences1, results = "asis", warning = FALSE, eval=FALSE}

covs[, sample := "Nottingham Students"]
covs[treatment == "replication" & student == TRUE, sample := "Hamburg Students"]
covs[treatment == "replication" & student == FALSE, sample := "Hamburg Citizens"]
covs[, sample := as.factor(sample)]
# covs[, sample := ordered(sample, levels = c("Nottingham Students", "Hamburg Students", "Hamburg Citizens"))]


m1 <- lm(formula = gender ~ sample, data = covs)
m2 <- lm(formula = age ~ sample, data = covs)
m3 <- lm(formula = switching_row ~ sample, data = covs)
m4 <- lm(formula = donation ~ sample, data = covs)
m5 <- lm(formula = pq11 ~ sample, data = covs)
m6 <- lm(formula = pq12 ~ sample, data = covs)
m7 <- lm(formula = pq13 ~ sample, data = covs)
m8 <- lm(formula = pq14 ~ sample, data = covs)

stargazer(m1, m2, m5, m6, m7, m8,


          column.labels = c("female", "age",
                            "trust", "meritocracy", "government", "equality") %>% str_to_title(),
          model.numbers = FALSE,
          dep.var.labels.include = FALSE,
          header=FALSE,
          intercept.bottom = FALSE,
          covariate.labels = c("Hamburg Citizens", "Hamburg Students", "Nottingham Students"),

          type = "latex", digits = 2, omit.stat = c("adj.rsq", "f"), df = FALSE
          )
```

```{r sampleProperties}
#| results: 'asis'
#| label: tbl-sample-properties
#| tbl-cap: Sample Properties

covs[, sample := "Nottingham Students"]
covs[treatment == "replication" & student == TRUE, sample := "Hamburg Students"]
covs[treatment == "replication" & student == FALSE, sample := "Hamburg Citizens"]
covs[, sample := as.factor(sample)]

vtable::sumtable(data = covs, 
                 vars = c('age', 'gender', 'switching_row'),
                 group = 'sample',
                 group.test = TRUE,
                 digits = 2,
                 labels = c('Age', 'Female', 'Risk Aversion (1-12)'),
                 # note = "Switching Row describes choices in a multiple price list to elicit risk preferences. The higher the value, the higher the certainty equivalent. Government Responsible is self-reported based on a question adapted from GMTV ('The government should take responsibility that people are better provided for.')",
                 out = 'latex')
```


```{r}
# covs[, sample := ordered(sample, levels = c("Nottingham Students", "Hamburg Students", "Hamburg Citizens"))]
```

Overall, I recruited `r covs[treatment == 'replication', .N]` participants for the experiment. The three samples differ significantly with respect to their age. The non-student sample is more diverse compared to the two student samples. 





## Software {#sec-software}

The experiment was logistically complex for several reasons. First, the sample was inexperienced. Second, the experiment was interactive and synchronous. Third, the underlying game was dynamic and interdependent. This makes dropouts not only more likely but also more expensive, which is why attrition  was a major concern in implementing the experiment.^[see @ZhouFischbach2016 for a nice illustration of how (selective) attrition affects identification.]

I chose oTree [@oTree] to implement the experiment because it is open-source, well-documented, and very flexible. Its [Bootstrap](https://getbootstrap.com/) (a powerful frontend toolkit) integration allowed me to make the graphical user interface interactive, appealing, and easy to navigate. The [Highcharts library](https://www.highcharts.com/) additionally made it easy to visualize results and communicate dynamics. Insofar, oTree served as a good tool to enhance the participants' user experience and thus, to make dropouts less likely.

Which features were required to handle dropouts? First, participants had to be matched to form a group _after_ comprehension questions were answered successfully. Importantly, participants were grouped according to the order they answered these questions to reduce waiting times. While waiting for other players to form the group, the participants saw a wait-page informing them that they are waiting for other participants to arrive and that they do not have to wait for longer than 10 minutes. The screen also informed them that they would receive a _patience bonus_ of one Euro after the expiration of that time (or what was left of it). Second, participants only had 10 minutes to make the first contribution and 4 minutes for the remaining contributions. After this time expired, participants were replaced by bots that made random contributions. In this case, the remaining group members were informed about the replacement. Both features were implemented to limit wait times and boredom for other participants. @sec-feasibility shows that the first feature became effective in some cases, whereas the second feature did not.


## Procedure {#sec-procedure}

Participants entered the experiment at appointed times remotely from home. They first saw a welcome screen. After agreeing to the privacy policy, they could proceed to the instructions individually. Having read these instructions, each participant has also seen a demo-screen explaining the user interface. Before proceeding, they had to answer six comprehension questions correctly to avoid confusion in later stages [@FerraroVossler2010]. Subsequently, they saw a waiting screen until they could be matched with three other participants, who have answered the comprehension questions correctly. Once matched, they were exposed to the decision screen over ten periods. At the end of the last period, participants saw the results of all periods. Subsequently, they made their VCA decision, before I elicited risk preferences [@HoltLaury2002] and finished with GMTVs' questionnaire.

Overall, I stuck to GMTVs' protocol as close as possible, however, I deviated in a few aspects. First, the instructions were German and also covered topics inherent to the online setting (dropouts and bots, for instance). Second, I used another software (oTree instead of zTree) which also affected the graphical user interface participants were exposed to. Third, GMTV gave participants the opportunity to donate to _Doctors without Borders_ whereas I offered carbon dioxide offsets.

```{r totalTimeSpent}
tmp <- duration[page_index == 1 | page_index == 76, 
                .(page_index, participant_code, page_submission)]
tmp[order(participant_code, page_index), 
    total := page_submission - data.table::shift(x = page_submission,
                                                 n = 1,
                                                 type = "lag"),
    by = participant_code]

total_completion_time <- tmp[!is.na(total), mean(total/ 60) %>% round()]
```

The experiment lasted on average for around `r total_completion_time` minutes. The earnings  averaged `r earningsMean %>% round(digits = 2)` Euros (sd = `r earningsSD  %>% round(digits = 2)`).^[This value include earnings from the incentivized risk elicitation task that is not part of the analysis.] 

# Results {#sec-results}

## Reproducibility {#sec-replication}

```{r}
N_r <- R1[treatment == "replication", .N]
N_o <- R1[treatment == "noPunish10", .N]
```

Throughout this section, I pool data from both the citizens as well as the students from Hamburg and compare it with the students GMTV recruited in Nottingham. I refer to the two samples as the _reproduction sample_ ($N_r$ = `r N_r`) and the _original sample_ ($N_o$ = `r N_o`), respectively.

### Contribution Behavior {#sec-contributions}

```{r prepFirstSummary}
replication <- R1[treatment == "replication", ownContribution]
GMTV    <- R1[treatment == "noPunish10", ownContribution]


rows <- sapply(X = list(replication, GMTV), FUN = NROW) %>% max()
temp <- data.frame(replication = c(replication, rep(NA, rows - NROW(replication))),
                   GMTV = c(GMTV, rep(NA, rows - NROW(GMTV))))

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 3) %>% 
  formatC(format = "f", 
          digits = 3)
```

First, I ask whether the samples differ with respect to their initial contributions to the public good. Is the reproduction sample (consisting of both students and non-students) more pro-social than the original sample? A two-sided rank sum test reveals that it is not (p=`r rs1`). Both samples contributed 10 tokens, that is, 50% of their endowments on average (median and mean). Moreover, both samples' initial contributions resemble the initial contributions participants usually make in the static game with partner matching.^[See Figure 3B in @fehrgaechter2000 [p.989], for instance.] However, in the dynamic game presented here, I am particularly interested in the subsequent periods because differences add up exponentially. Do the two groups remain similar over the course of time?


```{r firstRoundViz}
#| eval: false
#| fig-cap: Individual contributions to the dynamic public good in the first period
#| label: fig-first-round

ggplot(data = R1[treatment != "noPunish15"],
       mapping = aes(x = ownContribution, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 20),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 0.1),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Reproduction Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = R1[treatment == "replication", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 2) +
  geom_vline(xintercept = R1[treatment == "noPunish10", 
                             mean(ownContribution)],
             col = "#FFFFFF",
             lty = 1) +
    labs(title = "", 
       y = "", x = "Initially Contributed Tokens",
       caption = "White lines indicate means (dashed line = reproduction sample).") +
  layout +
  theme(legend.position = "top")
```


```{r firstRoundSummary}
#| results: asis
#| warning: false
#| fig.cap: "test"
#| eval: false

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

stargazer(temp,
          summary.stat = c("mean", "median","sd", "max", "min", "n"),
          type = type, 
          flip = TRUE, 
          header=FALSE)
```



```{r}
r <- 5
a <- main[round == r & treatment == 'replication', contribution]
b <- main[round == r & treatment == 'noPunish10', contribution]

rs2 <- wilcox.test(a, 
            b,
            exact = FALSE)$p.value %>% 
  round(digits = 4) %>% 
  formatC(format = "f", 
          digits = 4)
```


In particular, do the two samples' contributions follow the same path over the 10 periods they played? The answer is both _yes and no_. @fig-share-of-contributions and @tbl-contribution-periods illustrate that the samples make similar contributions at the beginning and the end of the game but behave differently in between. More precisely, the left panel--depicting the average contributions in absolute terms--shows that the original sample contributed more than the reproduction sample _in all but the first and last period_.^[In period five, this difference is significant: a two-sided rank sum test yields p=`r rs2`. ] For this reason, the original sample's behavior differs from the reproduction sample's behavior in two aspects: it contributes more and exhibits a considerable drop in the last period (whereas the reproduction sample's contributions flatten). 

Note that increasing contributions over time imply increasing endowments over time. Hence, absolute contributions do not say much about the willingness to cooperate. For this reason, the right panel in @fig-share-of-contributions shows the average _share of endowments contributed_ over time. Both samples exhibit a similar pattern: Their share of endowments contributed declined and did not stabilize. However, both samples also differ with respect to one aspect: the reproduction sample's share of contributions declines faster. This is mirrored by a two-sided rank sum test which is only significant at a five percent level in periods three and four.


```{r plotContributions}

SUM <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "contribution"]

SUM[,
    sum := round(contribution)]

SUM[,
    contribution := contribution/4]

upperLimit <- SUM$contribution %>% max() %>% round() + 5

p1 <- ggplot(data = SUM, 
             aes(x = round, y = contribution, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="top") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Average Amount of Tokens contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) + 
  guides(fill = "none") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))

rm(list = c("SUM"))
```


```{r plotShareOfContributions}
#| fig-cap: "The average amount of tokens contributed over time across samples."
#| label: fig-share-of-contributions

SHARE <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "share"]

# SHARE <- main[,
#             .(share = sum(contribution)/sum(endowment)),
#             by = c("round", "treatment")]

upperLimit <- 0.75

p2 <- ggplot(data = SHARE, 
             aes(x = round, y = share, fill = treatment, color = treatment, lty = treatment)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE) +
  geom_point() +
  scale_x_continuous(name="",  breaks = 1:15) +
  scale_y_continuous(limits = c(0, upperLimit), expand = c(0, 0)) +
  labs(y = "Share of Current Endowment contributed") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  guides(fill = "none")

p1 + p2 + plot_layout(guides = "collect") & 
  theme(legend.position = "top")

# rm(list = c("p1", "p2"))
```


Again, both samples' behavior resembles the contributions participants usually make in the static game with partner matching: contributions equal approximately half of the endowments in the very first period and decrease to around ten percent of endowments by the last period.^[The right panel is thus, comparable to the visualizations _and results_ in the static game. See, for instance, Figure 1B in @fehrgaechter2000 [p.986].] In the dynamic game presented here, however, different paths lead to different levels of wealth -- even if they share the same start- and endpoints. I am thus, more interested in the contributions' implications for wealth generation and growth. 

### Wealth Creation {#sec-wealth}

```{r summaryWealth}
#| results: asis
#| warning: false
#| fig.cap: "test"

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, stock]
GMTV    <- main[treatment == "noPunish10" & round == 10, stock]

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 3) %>% 
  formatC(format = "f", 
          digits = 3)

```

```{r wealthVariables}
originalWealth    <- main[treatment == "noPunish10" & round == 10, mean(stock)]
replicationWealth <- main[treatment == "replication" & round == 10, mean(stock)]
```

How do the different contribution paths translate into wealth?^[To measure wealth and growth, I define a variable called _stock_ which sums the endowments of all participants in a given group at the end of the round (that is, after the contributions have been made, multiplied, and redistributed).] Given that the original sample contributed more, one would expect the respective groups to be  more wealthy. A mere mean comparison indicates just that: An average group in the original sample accumulated about `r round(originalWealth)` tokens. In contrast, the average group in the reproduction sample accumulated about `r round(replicationWealth)` tokens. This difference is insignificant at conventional levels though: A two-sided rank sum test (comparing differences between samples) yields p=`r rs1` for the mean stock in the last period of the game.

```{r stockDistributionViz}
#| eval: false
#| fig-cap: Groups' income at the end of the game
#| label: fig-stock-distribution

ggplot(data = main,
       mapping = aes(x = stock, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Reproduction Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = replicationWealth,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = originalWealth,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Stock",
       caption = "Grey lines indicate means (dashed line = reproduction sample).") +
  layout +
  theme(legend.position = "top")
```

```{r overallGinis}
rG <- main[round == 10 & treatment == "replication",
           .(stock = sum(stock)),
           by = groupID][, Gini(stock)] %>% round(digits = 2)

oG <- main[round == 10 & treatment == "noPunish10",
           .(stock = sum(stock)),
           by = groupID][, Gini(stock)] %>% round(digits = 2)

rSD <- main[round == 10 & treatment == "replication",
            .(stock = sum(stock)),
            by = groupID][, sd(stock)] %>% round(digits = 2)

oSD <- main[round == 10 & treatment == "noPunish10",
            .(stock = sum(stock)),
            by = groupID][, sd(stock)] %>% round(digits = 2)
```

Although there clearly is growth, groups do not realize the maximal potential efficiency: under full cooperation, a group can accumulate a maximum of `r (80*1.5^10) %>% trunc()` tokens or EUR `r (80*1.5^10 /20) %>% trunc()`. This is depicted in the left panel of @fig-growth-heterogeneity, where one can see the average wealth over time by sample. The panel illustrates for both samples that growth was continuous and surprisingly linear, given the exponential character of the game's design. Despite somewhat differing contribution behavior between samples, neither the eventual wealth nor the corresponding growth paths differed. Differences in contribution behavior did, thus, not translate to significantly different wealth outcomes.

Why? Perhaps because the heterogeneity within samples and across groups has been too large to _detect_ a significant difference. The right panel of @fig-growth-heterogeneity depicts heterogeneity: In the reproduction sample, the richest group earned `r a <- main[treatment == "replication" & round == 10, max(stock)]; a` tokens (which is about `r round(a/80, digits = 2)*100`% of the initial endowment) whereas the poorest group ends up with `r b <- main[treatment == "replication" & round == 10, min(stock)]; b` tokens (`r round(b/80, digits = 2)*100`%). More broadly, the reproduction sample is characterized by inequality between groups ($SD_{Replication} =$ `r rSD`). The same holds true for the original sample ($SD_{Original} =$ `r oSD`). Hence, the heterogeneity across groups does not differ between samples, which is remarkable because the reproduction sample was drawn from a more heterogeneous (non-convenience sample). Does it differ within groups?

<!-- doch lieber Gini? statt SD reporten? -->

```{r plotStock .column-page}

# data
STOCK <- main[,
              lapply(.SD, mean, na.rm = TRUE),
              by = c("round", "treatment"),
              .SDcols = "stock"]

STOCKr <- main[rich == TRUE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]

STOCKp <- main[rich == FALSE,
               lapply(.SD, mean, na.rm = TRUE),
               by = c("round", "treatment"),
               .SDcols = "stock"]


# annotation
maxPath <- data.table(x = 1:10)
maxPath[, y := 80*1.5^x]
maxPath[, treatment := "replication"]
maxPath[, groupID := 42]

# plot
p1 <- ggplot(data = STOCK, 
             aes(x = round, y = stock, fill = treatment, color = treatment, lty = treatment)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 4700), expand = c(0, 0)) +
  labs(y = "Wealth", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  geom_point(mapping = aes(x = 10, y = 4613), col = "#b5b5b5") +
  annotate("text", x = 9, y = 4630, size = 3,
           label = "max", col = "#b5b5b5") +
  geom_line(data = maxPath, mapping = aes(x = x, y =y), col = "#b5b5b5", lty = 2) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

```{r growthHeterogeneityViz}
#| fig-cap: Average wealth over time across samples.
#| label: fig-growth-heterogeneity

# plot
p2 <- ggplot(data = main,
       mapping = aes(x = round, y = stock, color = treatment, by = groupID)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0, 2000),
                       expand = c(0, NA)) +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  labs(y = "", x = "Period") +
  layout +
  guides(col = "none")


# assembling
p1 + p2 + plot_layout(guides = "collect") & theme(legend.position = "top")
```

### Inequality {#sec-inequality}

```{r summaryGini}
#| results: asis
#| warning: false
#| fig.cap: "test"

if(knitr::is_html_output()){
  type <- "html"
} else {
  type = "latex"
}

replication <- main[treatment == "replication" & round == 10, gini]
GMTV    <- main[treatment == "noPunish10" & round == 10, gini]

rs1 <- wilcox.test(replication, 
                   GMTV,
                   exact = FALSE)$p.value %>% 
  round(digits = 3) %>% 
  formatC(format = "f", 
          digits = 3)

```

```{r giniVariables}
originalGini    <- main[treatment == "noPunish10" & round == 10, mean(gini)] %>% round(digits = 2)
replicationGini <- main[treatment == "replication" & round == 10, mean(gini)] %>% round(digits = 2)
```

Given the different samples and the possibility of endogenous growth---which essentially is the main feature of the game---I ask whether and how the inequality grows _within_ groups. @fig-gini-time-series illustrates that inequality did grow: at the end of the game, the original and the replication groups exhibit an average Gini coefficient of `r originalGini` and `r replicationGini`, respectively.^[The two-sided rank sum test (comparing differences between samples) yields p=`r rs1` for the mean Gini coefficient in the last round of the game.] Because every participant started with the same initial endowment (in _period 0_, so to speak), every group started equally--with a Gini coefficient equaling zero.

@fig-gini-time-series also shows that this initial state of equality ended with the first period already: both samples exhibit a stark incline in inequality before the second period started. From then on, the respective Gini coefficients grew slowly but continuously -- for both samples.^[In each and every period, the two-sided rank sum test comparing gini coefficients between both sample yields p-values way over ten percent.]

```{r giniDistributionViz}
#| eval: false
#| fig-cap: Groups' Gini coefficients (within groups) at the end of the game
#| label: fig-gini-distribution

ggplot(data = main,
       mapping = aes(x = gini, fill = treatment, lty = treatment)) +
  geom_density(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1),
                       expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
  scale_fill_manual(values = c(cPrimary, cSecondary),
                    labels = c("Original Sample", "Reproduction Sample")) +
  guides(lty = "none") +
  geom_vline(xintercept = replicationGini,
             col = "#000000", alpha = 0.5,
             lty = 2) +
  geom_vline(xintercept = originalGini,
             col = "#000000", alpha = 0.5,
             lty = 1) +
    labs(title = "", 
       y = "", x = "Gini Coefficient",
       caption = "Grey lines indicate means (dashed line = reproduction sample).") +
  layout +
  theme(legend.position = "top")
```

```{r giniOverTime}
#| fig-cap: Average Gini coefficient (within groups) over time across samples
#| label: fig-gini-time-series

tmp <- main[,
            lapply(.SD, mean, na.rm = TRUE),
            by = c("round", "treatment"),
            .SDcols = "gini"]

GINI <- rbind(tmp, list(0, "replication", 0), list(0, "noPunish10", 0))

ggplot(data = GINI, 
             aes(x = round, y = gini, fill = treatment, color = treatment, lty = treatment)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 0:10) +
  scale_y_continuous(limits = c(0, 0.25), expand = c(0, 0)) +
  labs(y = "Gini coefficient", x = "Period") +
  scale_color_manual(values = c(cPrimary, cSecondary),
                     labels = c("Original Sample", "Reproduction Sample")) +
  guides(fill = "none") +
  layout +
  theme(legend.position = "top") +
  guides(lty = "none")
```

**Result 1.** _The `NOPUNISH 10` treatment of GMTV can be replicated because the replication data resemble the original data with respect to initial and final contributions, wealth, and growth as well as inequality._

This finding is remarkable given the different sample and language, the different software and user interface as well as the online setting during the COVID-19 pandemic. The result suggests that by and large, the sum of these factors did not affect peopleâs preferences towards cooperation.

## Online Feasibility {#sec-feasibility}

```{r}
N <- duration[, participant_code %>% unique() %>% length()]
```

Throughout this section, I do not consider GMTVs' data and pool data I gathered from the citizens as well as the students from Hamburg (N = `r N`).

```{r reviewInformation}
#| eval: false

# to see how many people clicked on the popups to review instructions: 
full[dPGG.1.player.review_instructions !=0, .N]
full[dPGG.2.player.review_instructions !=0, .N]
#...

# or for contact information
full[dPGG.1.player.review_contact !=0, .N]
full[dPGG.2.player.review_contact !=0, .N]
# ...

```


How did the participants, who have never participated in an online group experiment before, cope with the situation? Moreover, did participants understand the unfamiliar setting they found themselves in? While the answer to the former question requires more thought, the answer to the latter simply is _yes_: `r 67` out of `r 44 + 67 + 5` answered with _"yes"_ when I asked them. Another `r 44` answered with _"rather yes"_ while nobody indicated that he or she did not understand the situation at all. Some behavioral data are supporting this finding: The user interface offered a popup to review instructions or contact information. I tracked both and find that none of the participants ever opened these popups even though they were clearly visible in the decision screens' header and introduced in the instructions. To further analyze how participants coped with the situation, I consider three additional metrics: selection into the experiment, attrition as well as the time spent on each page.

I first comment on the selection into the experiment: It was difficult to recruit the sample. The panel counted 1.209 non-students of which I was able to recruit `r main[student == FALSE, .N]` participants who finished the experiment---even though I varied the weekdays and timing of the sessions (which were conducted during a nationwide lockdown under mandatory home office regime). For this reason, I also recruited students in the last session which explains the relatively large number of showups in @tbl-meta. Although I intended to refrain from the recruitment of students initially, this particular sub-sample enabled me to investigate the generalizability of my results as I will discuss in  @sec-generalizability.

```{r showMeta}
#| label: tbl-meta
#| tbl-cap: The Experimental Sessions' Meta Data

meta %>% kable(col.names = names(meta) %>% 
                 str_replace_all(pattern = "\\.",
                                 replacement = " ") %>% 
                 str_to_title()) #, caption = 'A table of the first 10 rows of the mtcars data.')
```


```{r plotTimeData}
N <- duration[, participant_code %>% unique() %>% length()]
plotDT <- duration[app_name == "dPGG" & page_name == "dPGG_Decision",
                   .(time_spent = time_spent %>% sum()),
                   by = c("session_code", "participant_code", "page_index", "page_name")]

plotDT[, round := seq(from = 1, to = 10), by = c("participant_code")]

upperLimit <- plotDT[, time_spent %>% mean(), by = c("round")] %>% max()

tmp <- summarySE(data = plotDT,
                 measurevar = "time_spent",
                 groupvars=c("round"),
                 na.rm = FALSE,
                 conf.interval = 0.95,
                 .drop = TRUE) %>% 
  data.table()

```

Turning to the time spent on each page, I focus on the decision times in the dynamic public goods game as @Anderhub2001 did. How many seconds did the participants need to decide in each period of the game? Not too many. @fig-time-spent illustrates an plausible pattern: The first decision took about `r tmp[round == 1, round(time_spent)]` seconds. The second decision---where participants first learned about the other group members' previous decisions---took longer (about `r tmp[round == 2, round(time_spent)]` seconds). Subsequently, decision times first declined and stabilized at `r tmp[round > 3, round(mean(time_spent))]` seconds. Importantly, decision times were so short that crosstalk, that is, communication through private channels---a common concern^[See, for instance, the discussion section in @AGM2018 [p. 119].] in online experiments---was unlikely, especially because it would require the identification of other group members.^[There were only `r plotDT[page_index == 12 & time_spent > 60, .N]` participants (from all four sessions) who needed more than 60 seconds to make the second decision.]

```{r plotTime}
#| fig-cap: Average Time Spent for Each Contribution per Period
#| label: fig-time-spent

ggplot(data = tmp,
       mapping = aes(x = round, y = time_spent)) +
  layout +
  theme(legend.position="bottom") +
  geom_line(show.legend=FALSE, color = cInfo, lty=2) +
  geom_errorbar(aes(ymin=time_spent-ci, ymax=time_spent+ci), width=.25, alpha = 0.5, color = cInfo) +
  geom_point(color = cInfo) +
  scale_x_continuous(name="",  breaks = 1:10) +
  scale_y_continuous(limits = c(0, upperLimit + 10), expand = c(0, 0)) +
  labs(y = "Time Passed in Seconds", caption = "Bars indicate 95% confidence intervals.") +
  theme(plot.margin = margin(0.25,1,0.25,0.25, "cm"))
```

Considering attrition, I find that it did not affect the interactive experiment at all. To elaborate, I differentiate between dropouts and residuals:  Participants who could not be matched to other group members are called residuals. Participants who intentionally left the experiment are called dropouts. Residuals did not participate in the experiment _by design_. Dropouts did not participate in the experiment _by choice_. Out of `r meta[, sum(showups)]` people who showed up, I count `r meta[, sum(residuals)]` residuals and `r meta[, sum(dropouts)]` dropouts. All of the residuals waited to be matched to a group unsuccessfully before they got paid one Euro for their patience. In contrast, all of the dropouts left while reading the instructions and before being matched to other group members. Moreover, they received no payment at all. Hence, attrition was no concern considering the dynamic public goods game or the expenses.

**Result 2.** _Given the decision times and the fluent procedure, attrition was as negligible as it is in physical laboratories---where (a) not every invited person shows up and (b) several participants divisible by the group size are required as well._


## Generalizability {#sec-generalizability}

```{r generalizability_prep}
R1covs <- covs[, .(participant.code,
                   donation = donation * 20 ,
                   donationShare = (donation * 20) / payoff * 100,
                   age,
                   gender,
                   switching_row,
                   payoff)]
temp <- R1[R1covs, on = .(participant.code = participant.code)][treatment == "replication"]
temp[, contributionShare := ownContribution/20 * 100]
temp[, consistency := 1 - (contributionShare - donationShare)]

N_s <- temp[student == TRUE, .N]
N_g <- temp[student == FALSE, .N]
```


As before, I do not consider GMTVs' data in this section. Instead, I will differentiate between data from the citizens and the students from Hamburg. I refer to the two samples as the _students_ ($N_s$ = `r N_s`) and the _general population sample_ ($N_g$ = `r N_g`), respectively.


@GKLS2020 asked how much can we learn about voluntary climate action from the behavior in public goods games. Using a similar strategy, I answer the question for a _dynamic_ public goods game: _Not too much_. Overall, there seems to be no association between choices in the voluntary climate action and the first period in the dynamic public goods game. 


```{r vca_differences}
rs1 <- wilcox.test(temp[student == FALSE, donationShare], 
                   temp[student == TRUE, donationShare],
                   exact = FALSE)$p.value %>% 
  round(digits = 3) %>% 
  formatC(format = "f", 
          digits = 3)
```

@fig-kernel-generalizability shows distributions of contributions across both choices for both samples. The top panels illustrate the behavior of the general population sample. The bottom panels illustrate the behavior of the student sample. The left panels show the behavior in the VCA. The right panels show the behavior in the first period of the game. A visual inspection shows that (a) mean contributions are positive in both tasks for both samples.^[The same holds true for median contributions.] (b) Furthermore, average contributions are lower in the VCA. (c) In contrast to the observation of @GKLS2020, I do not observe a difference between samples in the abstract game's contribution behavior. (d) However, the student's share of income contributed to the VCA is significantly lower than the general population sample's contributions (two-sided rank sum test, p=`r rs1`). Taken together, these aggregate results indicate, that the consistency between tasks is higher for the general population than it is for students. Or, to put it differently, the general population's behavior in the abstract game better predicts their behavior in a real-world mitigation context.


```{r vizGeneralizability}
#| fig-cap: Kernel distributions of contributions across tasks and subject pools.
#| label: fig-kernel-generalizability
#| fig.height: 6

# viz wrapper
plotShares <- function(stud = FALSE,
                       var  = "donationShare",
                       col  = cPrimary,
                       xlab = "",
                       ylab = ""){
  
  dt <- temp[student == stud, .(x = get(var))]
  
  model <- lm(x ~ 1, dt)
  ci1 <- confint(model, level=0.95)[1]
  ci2 <- confint(model, level=0.95)[2]
  
  ggplot(data = dt,
         mapping = aes(x = x)) +
    annotate("rect", 
             xmin = ci1, xmax = ci2, 
             ymin  =0, ymax = Inf, 
             alpha = 0.33) +
    geom_density(alpha = 0.75,
                 fill = col) +
    scale_x_continuous(limits = c(0, 100),
                       expand = c(0, NA)) +
    scale_y_continuous(limits = c(0, NA),
                       expand = c(0, NA)) +
    geom_vline(xintercept = dt[,mean(x, na.rm = TRUE)],
               lty = 2) +
    labs(x = xlab,
         y = ylab) +
  layout
}

p1 <- plotShares(stud = FALSE, 
                 col = cSecondary,   
                 var = "donationShare",
                 xlab = "Share of income the general pop. contributed", 
                 ylab = "Density in VCA")

p2 <- plotShares(stud = FALSE, 
                 col = cSecondary,   
                 var = "contributionShare", 
                 xlab = "Share of endowment the general pop. contributed", 
                 ylab = "Density in dPGG Experiment (period 1)")

p3 <- plotShares(stud = TRUE,  col = cPrimary, var = "donationShare",
                 xlab = "Share of income the students contributed",
                 ylab = "Density in VCA")

p4 <- plotShares(stud = TRUE,  col = cPrimary, var = "contributionShare", 
                 xlab = "Share of endowment the students contributed",
                 ylab = "Density in dPGG Experiment (period 1)")

patchwork <- (p1 / p3) | (p2 / p4)
patchwork + plot_annotation(caption = "Dashed vertical lines indicate the respective means.\nShaded areas indicate 95% confidence intervals.")



rm(list = c("R1covs", "p1", "p2", "p3", "p4", "patchwork"))
```

@tbl-tobit-generalizability shows that this is not the case. It reports tobit regression results cautioning against transferability from dPGG results to real-world mitigation behavior: The share of endowment contributed in the first period (displayed in the first row) does not predict the share of earnings donated as a VCA. The student status negatively affects VCA donations in column two but disappears if one controls for age in column three. Importantly, the interaction between student status and first-period contributions is not significant. This suggests that the general population sample's transferability is just as bad as the student sample's. I thus, find a similar result as @GKLS2020 [p.6].

```{r generalizability_tobit}
#| results: 'asis'
#| label: tbl-tobit-generalizability
#| tbl-cap: Correlations between first-round-dPGG and VCA behavior

to1 <- censReg(formula = donationShare ~ contributionShare, 
                   data = temp, left = 0, right = 100)
to2 <- censReg(formula = donationShare ~ contributionShare + student + student * contributionShare, 
                   data = temp, left = 0, right = 100)
reg <- lm(formula = donationShare ~ contributionShare + student + student * contributionShare, 
                   data = temp,)
to3 <- censReg(formula = donationShare ~ contributionShare + student + student * contributionShare + age + gender, 
                   data = temp, left = 0, right = 100)
to4 <- censReg(formula = donationShare ~ contributionShare + student + student * contributionShare + age + gender + switching_row, 
                   data = temp, left = 0, right = 100)


stargazer(to1, to2, # reg, 
          to3, # to4,
          
          model.numbers = TRUE,
          dep.var.caption  = "Tobit Regression censoring below 0 and above 100",
          dep.var.labels = "VCA Donation as Share of Earnings",
          header=FALSE,
          intercept.bottom = TRUE,
          order = c(1, 2, 5, 3, 4),
          covariate.labels = c("First-period contribution in percent", "Student (1 = yes)", "First-period contr. x Student", "Age", "Female (1 = yes)"),
          type = "latex", digits = 2, omit.stat = c("adj.rsq", "f"), df = FALSE
          )
```

```{r}
#| fig-cap: Scatter plot of average contributions in the dPGG and real giving task.
#| label: fig-scatter-generalizability
#| fig.height: 6
#| eval: false

p1 <- ggplot(data = temp[, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cInfo, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cInfo) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "Percentage of endowment contributed in period 1") +
  layout

p2 <- ggplot(data = temp[student == FALSE, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cSecondary, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cSecondary) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "General pop.: Percentage of endowment contributed in period 1") +
  layout

p3 <- ggplot(data = temp[student == TRUE, 
                   .(vca = donation / payoff * 100,
                     dpgg = ownContribution / 20 * 100)],
       mapping = aes(y = vca, x = dpgg)) +
  annotate("segment", x = 0, xend = 100,
                   y = 0,
                   yend = 100,
                   colour = "black", lty = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, col = cPrimary, size = 2) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE,
              col = cPrimary) +
  scale_x_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  scale_y_continuous(limits = c(0, 105),
                     expand = c(0, NA)) +
  labs(y = "VCA: Percentage of endowment donated",
       x = "Students: Percentage of endowment contributed in period 1") +
  layout

p1 / (p2 + p3)
```

<!--
@fig-kernel-generalizability also indicates that the general population's contribution behavior is more consistent than the students' behavior. Hence, one may conclude that their behavior in the abstract game translates comparably well to the real world. This is not the case. If anything, the _students'_ behavior in the abstract game is more predictive for their VCA. @fig-scatter-generalizability visualizes the within-subjects relationship of both contribution tasks. More precisely, it shows a scatter plot of realized choices, with the percentage of endowment spent by each participant in the first period of the game on the x-axis and that spent in the VCA on the y-axis. In addition, the figures contains a fitted line of a linear model. The upper panel illustrates overall behavior and exhibits a slope which is indistinguishable from zero. No matter how much the participants contributed in the first round, they spent, on average, about `r temp[, round(mean(donationShare))]`% of their income on the VCA. The two bottom panels differentiate between both samples with slopes that differ qualitatively. However, none of the slopes is significant. 
-->

**Result 3.** _There is no significant correlation between average contributions in the abstract public goods game and contributions to the real public good of climate change mitigation---for none of the samples._



# Conclusion {#sec-conclusion}

The initial goal of the experiment was to reproduce specific experiments of GMTV in an online setting using a general population sample. 
<!--The results suggest that it is important to reproduce experiments---both purely and scientifically [@Hamermesh2007 p. 716]---before drawing conclusions about generalizability.
-->
The three most important findings are as follows: First, the contribution behavior in my experiment is statistically similar to the behavior reported in the original study. Consequently, the outcomes growth and inequality are reproducible in both meanings: they are _purely_ (despite minor differences one can re-analyze the original data and reach the same conclusions) and _scientifically_ (one can gather new data drawn from a different population in a different situation and find similar patterns) reproducible. Second, the online experiment proceeded fluently such that dropouts were no concern. Third, contribution behavior in the dynamic abstract setting is not linked to behavior in the real world---neither for students nor a more representative sample.

The significance of the first result is that similar procedures led to replicable findings under different circumstances across two different samples. The second result is of methodological importance: It highlights that even logistically complex experiments can be conducted online---not only with clickworkers but also with a true general population sample. The third result questions whether recruiting from more representative samples is worth the effort because it did not affect the transferability of abstract results to the real world.

Taken together the results answer the questions whether GMTVs' dynamic paradigm is reproducible  across samples (yes) and whether it is feasible to employ it online (also yes). The question that is _not_ answered is, whether GMTVs' treatment effects replicate well because I did not focus on treatments and treatment effects. It thus, remains an open question, whether the causal effects of GMTVs' treatments replicate well. Further, we do not know whether their treatments affect real-world behavior. So, how generalizable are treatment effects in public goods games in general? Albeit this study does not address these questions, it provides methods suited to investigate them---even with inconvenient samples.

# Acknowledgements

I gratefully acknowledge support by the German Research Foundation (DFG) under Germanyâs Excellence Strategy, cluster EXC 2037 âClimate, Climatic Change, and Societyâ (project 390683824). I thank Stefan Traub, Alexandrea Swanson, and the participants of the CLICCS B5 group for helpful discussions.

{{< pagebreak >}}


# A: Pure Replication {.appendix}

This section comments on two errors as well as a misconception I found in the original data.^[The data can be found in the supplementary materials they provide in their [online appendix](https://www.sciencedirect.com/science/article/pii/S0047272717300361#s0115).] Before I proceed to explain this in more detail I would like to say that the results of the original paper still hold after the error is fixed and that the authors responded kindly and quickly, showing an interest in solving the issue. In fact, some explanations in this section stem from input provided by the authors.

### Error 1: The Gini coefficient 

The Gini coefficient is wrongly computed in some periods for some group members. The authors found that this happened whenever two group members had exactly the same endowment because the program failed to rank these group members for further calculations.

```{r readGMTVagain}
GMTV <- read_dta(file="../../data/gaechteretal/GMTV-data.dta") %>% data.table()
noPunish10 <- GMTV[exp_num == 5 | exp_num == 8 | exp_num == 9]
# noPunish10 <- GMTV[longgame == 0 & punish == 0 & exp_num <= 10]
```

@tbl-gini-error illustrates this problem. It shows group 101 in period 5 and documents that the Gini coefficient differs among group members. According to the authors, the Gini coefficient should equal `GINI=``r GINI <- GMTV[exp_num == 1 & gr_id == 101 & per == 5, tokens %>% Gini() %>% round(digits = 3)]; GINI` for all subjects in the group. Instead, participants `112` and `113` who have an equal endowment deviate from that value. Importantly, the `DescTools::Gini()` function in the statistical software `R` does not yield this error, which is why I use that function for my calculations using both my as well as the original data.

```{r}
#| label: tbl-gini-error
#| tbl-cap: Subset of Data illustrating the Gini Coefficient's Error

GMTV[exp_num == 1 & gr_id == 101 & per == 5,
     .(exp_num,
       gr_id,
       per,
       subj_id,
       tokens,
       other1,
       other2,
       other3,
       gini = round(gini, digits = 3),
       GINI = GINI
       )] %>% 
  knitr::kable()
```

### Error 2: The share of endowments contributed

The original data provides a wrong measure of the share of endowments contributed (`mean`) because it relies on a lagged endowment (`gdp`). More precisely, the authors used the following STATA code for their calculations:

```
*tsset subj_id per
*gen mean=sum/l.gdp
```

@tbl-mean-error reports participant 111 in group 101 in experiment 1 over three periods. Both the `gdp` (that is, the sum of the groupâs endowments at the beginning of the period) as well as the `sum` (that is, the sum of the groupâs contributions) are group-level variables. 

```{r}
#| label: tbl-mean-error
#| tbl-cap: Subset of Data illustrating the Means's Error

GMTV[, MEAN := sum/gdp]

GMTV[exp_num == 1 & gr_id == 101 & (per == 4 | per == 5 | per == 6) & subj_id == 111,
     .(exp_num,
       gr_id,
       per,
       subj_id,
       gdp,
       sum,
       mean = round(mean, digits = 3),
       MEAN = round(MEAN, digits = 3)
       )] %>% 
  knitr::kable()
```

Calculating the share as `MEAN=sum/gdp` solves the problem and yields $\frac{18}{126}=0.143$ in period 5. I thus, used this proposed definition for all my calculations using both my as well as the original data.

### The misconception: Timing

The authors wrote a note stating that the Gini coefficient as well as the wealth in the paper always refer to the situation at the start of a period and that they clarify this because the paper (last paragraph at the bottom of page 5), says that wealth is defined as the endowment at the beginning of the following period. Furthermore, they write that this error came about as they switched between these two definitions during the course of revising the paper.

I argue that it makes more sense to calculate the variables as they state in the paper. More precisely, I think that the wealth at the _beginning_ of a period is less interesting than the wealth at the _end_ of a period for two reasons: First, there is no need for such a variable because it already exists (the endowment). Second, this definition yields a value that is determined by the design of the game but misses an important outcome at the end of the game. To illustrate this, note that the wealth would be defined as four times the initial endowment in period 1. Also note that the very last value would equal the wealth at the beginning of the last period and says nothing about the outcome of that period. Because the contributions often drop in the last period, this outcome is of particular interest (yet, not represented in the data).
Moreover, this definition of wealth yields more informative values to calculate the Gini coefficient for the same reasons: We know that the Gini coefficient is zero _before_ the participants made any decision by design. We do no know the inequality at the very end of the game---and the current definition does not tell us.

For these reasons, I define wealth and inequality measures as the outcomes of a period for all of my calculations using both my as well as the original data.^[Accordingly, the definition of `GINI` I provide in @tbl-gini-error is not the definition I used to calculate the current period's Gini coefficient but the previous period's Gini coefficient.]

{{< pagebreak >}}

# B: Tables

```{r}
#| label: tbl-contribution-periods
#| tbl-cap: The average amount of tokens contributed over time across samples.

p <- data.table(round = 1:10)
for(r in 1:10){
  a <- main[round == r & treatment == 'replication', contribution]
  b <- main[round == r & treatment == 'noPunish10', contribution]
  
  
  p[round == r, `p-value` := wilcox.test(a, b, exact = FALSE)$p.value %>% 
      round(digits = 3) %>% 
      formatC(format = "f", digits = 3)]
}

tmp <- main[, 
            .(Mean = mean(contribution, na.rm = TRUE) %>% formatC(format = "f", digits = 3),
              CI = t.test(contribution, conf.level = 0.95)$conf.int[c(1,2)] %>% formatC(format = "f", digits = 3) %>% paste(collapse = " - ")),
            by = c('treatment', 'round')] %>%
  data.table::dcast(formula = round ~ treatment,
                    value.var = c('Mean', 'CI'))

tmp <- tmp[, c(1,2,4,3,5)]

tmp[p, on = .(round = round)] %>%
  kable(col.names = c('Period', 
                      'Mean: Original', '95%-CI: Original',
                      'Mean: Reproduction', '95%-CI: Reproduction',
                      'p-Value'))

```

```{r}
#| label: tbl-share-periods
#| tbl-cap: The average share of current endowments contributed over time across samples.

p <- data.table(round = 1:10)
for(r in 1:10){
  a <- main[round == r & treatment == 'replication', share]
  b <- main[round == r & treatment == 'noPunish10', share]
  
  
  p[round == r, `p-value` := wilcox.test(a, b, exact = FALSE)$p.value %>% 
      round(digits = 3) %>% 
      formatC(format = "f", digits = 3)]
}

tmp <- main[, 
            .(Mean = mean(share, na.rm = TRUE) %>% formatC(format = "f", digits = 3),
              CI = t.test(share, conf.level = 0.95)$conf.int[c(1,2)] %>% formatC(format = "f", digits = 3) %>% paste(collapse = " - ")),
            by = c('treatment', 'round')] %>%
  data.table::dcast(formula = round ~ treatment,
                    value.var = c('Mean', 'CI'))

tmp <- tmp[, c(1,2,4,3,5)]

tmp[p, on = .(round = round)] %>%
  kable(col.names = c('Period', 
                      'Mean: Original', '95%-CI: Original',
                      'Mean: Reproduction', '95%-CI: Reproduction',
                      'p-Value'))

```

```{r}
#| label: tbl-wealth-periods
#| tbl-cap: The average wealth over time across samples.

p <- data.table(round = 1:10)
for(r in 1:10){
  a <- main[round == r & treatment == 'replication', stock]
  b <- main[round == r & treatment == 'noPunish10', stock]
  
  
  p[round == r, `p-value` := wilcox.test(a, b, exact = FALSE)$p.value %>% 
      round(digits = 3) %>% 
      formatC(format = "f", digits = 3)]
}

tmp <- main[, 
            .(Mean = mean(stock, na.rm = TRUE) %>% formatC(format = "f", digits = 3),
              CI = t.test(stock, conf.level = 0.95)$conf.int[c(1,2)] %>% formatC(format = "f", digits = 3) %>% paste(collapse = " - ")),
            by = c('treatment', 'round')] %>%
  data.table::dcast(formula = round ~ treatment,
                    value.var = c('Mean', 'CI'))

tmp <- tmp[, c(1,2,4,3,5)]

tmp[p, on = .(round = round)] %>%
  kable(col.names = c('Period', 
                      'Mean: Original', '95%-CI: Original',
                      'Mean: Reproduction', '95%-CI: Reproduction',
                      'p-Value'))

```

```{r}
#| label: tbl-gini-periods
#| tbl-cap: The average Gini coefficient over time across samples.

p <- data.table(round = 1:10)
for(r in 1:10){
  a <- main[round == r & treatment == 'replication', gini]
  b <- main[round == r & treatment == 'noPunish10', gini]
  
  
  p[round == r, `p-value` := wilcox.test(a, b, exact = FALSE)$p.value %>% 
      round(digits = 3) %>% 
      formatC(format = "f", digits = 3)]
}

tmp <- main[, 
            .(Mean = mean(gini, na.rm = TRUE) %>% formatC(format = "f", digits = 3),
              CI = t.test(gini, conf.level = 0.95)$conf.int[c(1,2)] %>% formatC(format = "f", digits = 3) %>% paste(collapse = " - ")),
            by = c('treatment', 'round')] %>%
  data.table::dcast(formula = round ~ treatment,
                    value.var = c('Mean', 'CI'))

tmp <- tmp[, c(1,2,4,3,5)]

tmp[p, on = .(round = round)] %>%
  kable(col.names = c('Period', 
                      'Mean: Original', '95%-CI: Original',
                      'Mean: Reproduction', '95%-CI: Reproduction',
                      'p-Value'))

```

{{< pagebreak >}}